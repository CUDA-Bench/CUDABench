{"id": 16, "task_name": "ReLU_Activation_Fuction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU_Activation_Fuction\n\nTask Description:\nImplement the ReLU activation function kernel. The kernel should take a one-dimensional float32 input tensor of size 1048576 and produce a float32 output tensor of the same size. For each element in the input, if the value is greater than zero, output the same value; otherwise, output zero. The computation must be performed element-wise independently.\n\nInput:\nrelu_input: float32, shape = (1048576,)\n\nOutput:\nrelu_out: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 19, "task_name": "ReLU_Activation_Fuction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU_Activation_Fuction\n\nTask Description:\nImplement the ReLU_Activation_Fuction kernel. The kernel applies the ReLU activation function element-wise to an input tensor. The input tensor is a 1-dimensional array of 67,108,864 float32 values named 'relu_input'. For each element, if the value is greater than 0, output the original value; otherwise, output 0. The output tensor 'relu_out' must be a float32 array of the same shape. The kernel must process all 67,108,864 elements without modifying the input.\n\nInput:\nrelu_input: float32, shape = (67108864,)\n\nOutput:\nrelu_out: float32, shape = (67108864,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 20, "task_name": "ReLU_Activation_Fuction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU_Activation_Fuction\n\nTask Description:\nImplement a CUDA kernel for the ReLU activation function. The input is a 1D tensor of float32 values with 268435456 elements. The output must be a 1D tensor of the same shape and data type. For each element, if the input value is greater than zero, output the same value; otherwise, output zero. The kernel must process all elements independently.\n\nInput:\nrelu_input: float32, shape = (268435456,)\n\nOutput:\nrelu_out: float32, shape = (268435456,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 17, "task_name": "ReLU_Activation_Fuction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU_Activation_Fuction\n\nTask Description:\nImplement the ReLU Activation Function kernel. The kernel should process a one-dimensional input tensor of 4,194,304 single-precision floating-point numbers. For each element in the input tensor, the kernel must output the element's value if positive, or zero otherwise. The output tensor must have identical shape and data type as the input. The kernel must preserve the element-wise correspondence between input and output tensors.\n\nInput:\nrelu_input: float32, shape = (4194304,)\n\nOutput:\nrelu_out: float32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 36, "task_name": "Reduction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReduction\n\nTask Description:\nTask: Reduction. Compute the sum of all elements in a 1D input tensor containing 1024 float32 values. The kernel must process each element exactly once and produce a single float32 output value representing the total sum. The implementation must be numerically stable and handle all 1024 elements.\n\nInput:\nreduce_input: float32, shape = (1024,)\n\nOutput:\nreduce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 30, "task_name": "Sorting", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSorting\n\nTask Description:\nImplement a CUDA kernel to sort an array of 8192 floating-point numbers in ascending order. The input is a 1D tensor of float32 values with shape (8192,). The output must be a 1D tensor of float32 values with the same shape, containing the sorted elements in non-decreasing order. The kernel must produce results that exactly match a reference sorted array.\n\nInput:\nsort_input: float32, shape = (8192,)\n\nOutput:\nsort_out: float32, shape = (8192,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 6, "task_name": "Post_Process_GL", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPost_Process_GL\n\nTask Description:\nImplement the Post_Process_GL kernel that processes an input image. The input is a 128x128 RGBA image stored as uint8 values. The kernel should apply a disc-shaped convolution filter with radius r=4 to each pixel. For pixels within the disc neighborhood, if their luminance (average of RGB channels) exceeds threshold=0.8, multiply their RGB values by highlight=2.0 before averaging. The output should be a flattened 16384-element array of uint32 values, each representing an ABGR-packed pixel (alpha=255) with processed RGB channels. Boundary handling should use clamp addressing, and shared memory should be used for efficient neighborhood access.\n\nInput:\ninput_img: uint8, shape = (128, 128, 4)\n\nOutput:\noutput_img: uint32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 10, "task_name": "Post_Process_GL", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPost_Process_GL\n\nTask Description:\nImplement the Post_Process_GL kernel that processes a 2048x2048 RGBA input image (8-bit per channel) to produce a 2048x2048 output image (32-bit packed integers). For each output pixel, compute a disc-shaped convolution within radius 4 around the corresponding input pixel. For neighborhood pixels within the disc, brighten RGB components by factor 2.0 if their luminance (average of RGB values) exceeds 0.8, then compute the average RGB values across valid neighborhood pixels. Pack the resulting RGB values into a 32-bit integer in ABGR format (alpha=255) with clamped [0,255] values.\n\nInput:\ninput_img: uint8, shape = (2048, 2048, 4)\n\nOutput:\noutput_img: uint32, shape = (2048, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 29, "task_name": "Sorting", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSorting\n\nTask Description:\nImplement a CUDA kernel named 'Sorting' that sorts an input array of 4096 single-precision floating-point numbers in ascending order. The input tensor 'sort_input' is a 1D array of shape (4096,) with float32 data type. The output tensor 'sort_out' must be a sorted version of the input array with the same shape and data type, where each element is less than or equal to the next element. The kernel must produce results that exactly match a reference sorted array within a tolerance of 1e-5.\n\nInput:\nsort_input: float32, shape = (4096,)\n\nOutput:\nsort_out: float32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 12, "task_name": "Recursive_Gaussian", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRecursive_Gaussian\n\nTask Description:\nImplement a recursive Gaussian filter kernel for image processing. The input is a 1024x1024 image where each pixel is stored as a 32-bit unsigned integer representing RGBA values (8 bits per channel). The output should be a filtered image of the same dimensions and data type. The kernel must process each image column independently using a two-pass approach: a forward pass from top to bottom and a reverse pass from bottom to top. Boundary conditions must clamp to the edge pixels. Filter coefficients (a0, a1, a2, a3, b1, b2, coefp, coefn) are provided as parameters.\n\nInput:\ninput_img: uint32, shape = (1024, 1024)\n\nOutput:\noutput_img: uint32, shape = (1024, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 13, "task_name": "Recursive_Gaussian", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRecursive_Gaussian\n\nTask Description:\nImplement a CUDA kernel named Recursive_Gaussian that applies a recursive Gaussian filter to an input image. The input is a 2048x2048 2D tensor of uint32 values representing RGBA pixels (8 bits per channel). The output is a filtered image of the same shape and data type. The kernel must process each column independently with one thread per column, implement both forward and reverse passes for symmetry, clamp to edge at boundaries, convert RGBA values between uint32 and float4 for processing, and use precomputed filter coefficients.\n\nInput:\ninput_img: uint32, shape = (2048, 2048)\n\nOutput:\noutput_img: uint32, shape = (2048, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 7, "task_name": "Post_Process_GL", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPost_Process_GL\n\nTask Description:\nImplement the Post_Process_GL kernel for GPU processing. The input is a 256×256 RGBA image with 4 channels (shape 256,256,4) of uint8 data type. The output must be a 256×256 image of uint32 values where each pixel is packed in ABGR format (alpha=255). The kernel must apply a disc-shaped convolution with a 4-pixel radius, processing only pixels within Euclidean distance ≤4 from the center. For pixels with luminance (average of RGB channels normalized to [0,1]) above 0.8, multiply RGB values by 2.0 before averaging. Handle image boundaries with clamping, and ensure shared memory optimization with halo loading for neighborhood access.\n\nInput:\ninput_img: uint8, shape = (256, 256, 4)\n\nOutput:\noutput_img: uint32, shape = (256, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 45, "task_name": "Dot_Product", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDot_Product\n\nTask Description:\nImplement a CUDA kernel named 'Dot_Product' that computes the dot product of two input vectors. The inputs are two one-dimensional tensors named 'dot_input_a' and 'dot_input_b', each containing 1048576 elements of type float32. The output is a single-element tensor named 'dot_out' of type float32, holding the sum of element-wise multiplications between corresponding elements of the input vectors. The kernel must handle the entire vector length accurately using atomic operations or reduction techniques to accumulate partial sums.\n\nInput:\ndot_input_a: float32, shape = (1048576,)\ndot_input_b: float32, shape = (1048576,)\n\nOutput:\ndot_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 46, "task_name": "Prefix_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPrefix_Sum\n\nTask Description:\nImplement a CUDA kernel for the Prefix_Sum task. The kernel should compute the inclusive prefix sum (cumulative sum) of a 1D input tensor containing 1024 float32 values. The output tensor must be a 1D float32 array of 1024 elements where each element at index i is the sum of all input elements from index 0 to i inclusive. The kernel must produce numerically accurate results matching a sequential cumulative sum calculation.\n\nInput:\nprefix_input: float32, shape = (1024,)\n\nOutput:\nprefix_out: float32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 23, "task_name": "Top-K_Selection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTop-K_Selection\n\nTask Description:\nImplement the Top-K Selection kernel. Given an input tensor of 16384 float32 values, compute the top 128 largest values. The output must be a tensor of 128 float32 values sorted in descending order. The kernel must correctly identify the largest elements and maintain them in sorted order throughout processing.\n\nInput:\ntopk_input: float32, shape = (16384,)\n\nOutput:\ntopk_out: float32, shape = (128,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 44, "task_name": "Dot_Product", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDot_Product\n\nTask Description:\nTask: Dot_Product. Compute the dot product of two input vectors. The inputs are two one-dimensional tensors named dot_input_a and dot_input_b, each containing 524288 elements of type float32. The output is a single scalar value stored in a tensor named dot_out with shape (1,) and dtype float32. The kernel must calculate the sum of element-wise products of corresponding elements from both input vectors.\n\nInput:\ndot_input_a: float32, shape = (524288,)\ndot_input_b: float32, shape = (524288,)\n\nOutput:\ndot_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 18, "task_name": "ReLU_Activation_Fuction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU_Activation_Fuction\n\nTask Description:\nReLU Activation Function: Compute the Rectified Linear Unit (ReLU) activation function. The input is a 1-dimensional tensor with 16,777,216 float32 elements. The output must be a tensor of the same shape and data type. For each element, if the input value is positive, output the same value; otherwise, output zero. The operation must be performed element-wise without altering adjacent elements.\n\nInput:\nrelu_input: float32, shape = (16777216,)\n\nOutput:\nrelu_out: float32, shape = (16777216,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 42, "task_name": "Dot_Product", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDot_Product\n\nTask Description:\nTask: Dot_Product. Compute the dot product of two input vectors. The inputs are two vectors of 131072 elements each, with data type float32. The output is a single scalar value of data type float32. The kernel must compute the sum of the products of corresponding elements from the two vectors.\n\nInput:\ndot_input_a: float32, shape = (131072,)\ndot_input_b: float32, shape = (131072,)\n\nOutput:\ndot_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 11, "task_name": "Recursive_Gaussian", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRecursive_Gaussian\n\nTask Description:\nImplement a Recursive Gaussian filter kernel. The input is a 512x512 image where each pixel is a 32-bit unsigned integer representing RGBA channels. The output must be a 512x512 image with the same data type. The kernel must process each image column independently, applying a recursive filter with forward and reverse passes. Boundary conditions clamp to edge pixels. The filter uses precomputed coefficients (a0, a1, a2, a3, b1, b2, coefp, coefn) derived from Gaussian sigma. Final pixel values combine both passes.\n\nInput:\ninput_img: uint32, shape = (512, 512)\n\nOutput:\noutput_img: uint32, shape = (512, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 47, "task_name": "Prefix_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPrefix_Sum\n\nTask Description:\nImplement a CUDA kernel for the Prefix_Sum task. The kernel must compute the cumulative sum (prefix sum) of a one-dimensional input tensor containing 4096 float32 elements. The output tensor should be the same shape and data type, where each element at position i is the sum of all input elements from index 0 to i. The kernel must produce results within a tolerance of 0.01 compared to a reference cumulative sum.\n\nInput:\nprefix_input: float32, shape = (4096,)\n\nOutput:\nprefix_out: float32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 25, "task_name": "Top-K_Selection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTop-K_Selection\n\nTask Description:\nTask: Top-K Selection. Compute the top-k elements from a one-dimensional input tensor of 262144 float32 values. The kernel must output a one-dimensional tensor of 512 float32 values containing the largest elements from the input, sorted in descending order (largest first). The kernel should process the entire input array and maintain a sorted list of the current top-k elements, updating it whenever a larger element is encountered. Constraints include fixed input size (262144) and output size (512), with output values sorted descending.\n\nInput:\ntopk_input: float32, shape = (262144,)\n\nOutput:\ntopk_out: float32, shape = (512,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 15, "task_name": "Recursive_Gaussian", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRecursive_Gaussian\n\nTask Description:\nTask name: Recursive_Gaussian. Implement a CUDA kernel that applies a recursive Gaussian filter to an input image. The input is a tensor named 'input_img' with data type uint32 and shape (8192, 8192), where each element represents a packed RGBA value (red, green, blue, alpha channels in 8 bits each). The output is a tensor named 'output_img' with the same data type and a flattened shape of (67108864,), equivalent to the input size. The kernel must process each column independently, performing a forward pass from top to bottom and a reverse pass from bottom to top for symmetry, summing the results. Boundary conditions should clamp to the edge pixels, and the filter uses precomputed coefficients (a0, a1, a2, a3, b1, b2, coefp, coefn) derived from a sigma value.\n\nInput:\ninput_img: uint32, shape = (8192, 8192)\n\nOutput:\noutput_img: uint32, shape = (67108864,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 27, "task_name": "Sorting", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSorting\n\nTask Description:\nTask: Sorting. Sort an array of 1024 floating-point numbers in ascending order. The input is a 1D tensor of 1024 float32 values. The output must be a 1D tensor of 1024 float32 values containing the same elements as the input but sorted in non-decreasing order. The algorithm must correctly sort any input array of the specified size.\n\nInput:\nsort_input: float32, shape = (1024,)\n\nOutput:\nsort_out: float32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 14, "task_name": "Recursive_Gaussian", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRecursive_Gaussian\n\nTask Description:\nImplement a Recursive_Gaussian filter for image processing. The input is a 4096x4096 image where each pixel is stored as a uint32 representing RGBA channels (8 bits per channel). The output should be a flattened 16777216-element uint32 array of filtered pixel data. The kernel must independently process each image column using a two-pass approach: a forward pass (top to bottom) and reverse pass (bottom to top) with clamp-to-edge boundary conditions. Filter coefficients (a0-a3, b1-b2, coefp, coefn) are precomputed parameters. Pixel data must be converted between uint32 and floating-point representations during processing, and the final output combines results from both passes.\n\nInput:\ninput_img: uint32, shape = (4096, 4096)\n\nOutput:\noutput_img: uint32, shape = (16777216,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 28, "task_name": "Sorting", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSorting\n\nTask Description:\nTask: Sorting\nDescription: Sort an input array of 2048 floating point numbers in ascending order.\nInput: A 1D tensor named 'sort_input' with shape (2048,) and data type float32.\nOutput: A 1D tensor named 'sort_out' with shape (2048,) and data type float32, containing the sorted elements of the input array in ascending order.\nConstraints: The output must be the sorted version of the input array and must match the reference sorted array within a tolerance of 1e-5.\n\nInput:\nsort_input: float32, shape = (2048,)\n\nOutput:\nsort_out: float32, shape = (2048,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 39, "task_name": "Reduction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReduction\n\nTask Description:\nTask: Reduction. Compute the sum of all elements in a one-dimensional input tensor of size 65,536 with float32 data type. The output is a single float32 value representing the total sum. The kernel must ensure that all input elements are processed correctly, and the output accurately reflects the sum without omitting any elements, respecting floating-point precision constraints.\n\nInput:\nreduce_input: float32, shape = (65536,)\n\nOutput:\nreduce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 40, "task_name": "Reduction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReduction\n\nTask Description:\nImplement a Reduction kernel that computes the sum of all elements in a 1D input tensor of 262144 float32 values. The output must be a single float32 value representing the total sum. The kernel must handle concurrent updates correctly to avoid race conditions.\n\nInput:\nreduce_input: float32, shape = (262144,)\n\nOutput:\nreduce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 38, "task_name": "Reduction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReduction\n\nTask Description:\nTask: Reduction. Compute the sum of all elements in a one-dimensional input array containing 16384 float32 values. The output should be a single float32 value representing the total sum. The kernel must handle concurrent thread updates correctly to avoid race conditions and produce a result accurate within a tolerance of ±0.01 relative to the reference sum.\n\nInput:\nreduce_input: float32, shape = (16384,)\n\nOutput:\nreduce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 41, "task_name": "Dot_Product", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDot_Product\n\nTask Description:\nImplement a CUDA kernel to compute the dot product of two input vectors. The kernel takes two input tensors 'dot_input_a' and 'dot_input_b', each containing 65536 float32 elements. The output should be a single float32 value in tensor 'dot_out' representing the sum of element-wise products. The kernel must handle parallel reduction correctly using atomic operations to ensure accuracy.\n\nInput:\ndot_input_a: float32, shape = (65536,)\ndot_input_b: float32, shape = (65536,)\n\nOutput:\ndot_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 49, "task_name": "Prefix_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPrefix_Sum\n\nTask Description:\nImplement a CUDA kernel for computing the inclusive prefix sum (also known as an inclusive scan) of a 1D array. The input is a single tensor named 'prefix_input' containing 65536 elements of type float32. The output tensor 'prefix_out' must have the same shape and data type, where each element at index i represents the sum of all input elements from index 0 to index i. The kernel must be implemented for GPU execution and produce results with an absolute tolerance of 0.01 compared to a sequential reference implementation.\n\nInput:\nprefix_input: float32, shape = (65536,)\n\nOutput:\nprefix_out: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 26, "task_name": "Sorting", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSorting\n\nTask Description:\nImplement a CUDA kernel to sort an array of 512 single-precision floating-point numbers in ascending order. The input is a one-dimensional tensor of shape (512,) containing unsorted float32 values. The output must be a tensor of identical shape and data type with elements sorted from smallest to largest. The kernel must produce results that exactly match a reference sorted array when validated.\n\nInput:\nsort_input: float32, shape = (512,)\n\nOutput:\nsort_out: float32, shape = (512,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 22, "task_name": "Top-K_Selection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTop-K_Selection\n\nTask Description:\nTask: Top-K_Selection. Given an input tensor named 'topk_input' of shape (4096,) and data type float32, the kernel must compute an output tensor named 'topk_out' of shape (64,) and data type float32. The output must contain the top 64 largest values from the input, sorted in descending order (from largest to smallest). The kernel must correctly handle the entire input array of 4096 elements to produce the output.\n\nInput:\ntopk_input: float32, shape = (4096,)\n\nOutput:\ntopk_out: float32, shape = (64,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 31, "task_name": "Matrix_Copy", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Copy\n\nTask Description:\nImplement a CUDA kernel named Matrix_Copy that copies a 1024x1024 matrix of single-precision floating-point numbers. The input tensor 'matrix_input' has shape (1024, 1024) and dtype float32. The output tensor 'matrix_out' must have identical shape and dtype. The kernel must copy each element from the input to the output at the same position. The implementation must handle exactly 1024x1024 elements.\n\nInput:\nmatrix_input: float32, shape = (1024, 1024)\n\nOutput:\nmatrix_out: float32, shape = (1024, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 51, "task_name": "Categorical_Cross-Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCategorical_Cross-Entropy_Loss\n\nTask Description:\nImplement a CUDA kernel to compute the categorical cross-entropy loss. The kernel should take two inputs: a 2D float32 tensor of logits with shape (16384, 10) and a 1D int32 tensor of labels with shape (16384). The output is a single float32 scalar representing the average loss across all samples. For each sample, first find the maximum logit value for numerical stability, then compute the exponential of each logit minus this maximum, sum these exponentials, take the logarithm of this sum, and calculate the loss as the logarithm plus the maximum minus the logit corresponding to the true label. Finally, average these per-sample losses across all 16384 samples.\n\nInput:\nce_logits: float32, shape = (16384, 10)\nce_labels: int32, shape = (16384,)\n\nOutput:\nce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 32, "task_name": "Matrix_Copy", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Copy\n\nTask Description:\nImplement a CUDA kernel named Matrix_Copy that copies a 2048x2048 input matrix of single-precision floating-point numbers to an output matrix. The input and output matrices must have identical dimensions and data types. Each element in the input matrix must be exactly replicated in the corresponding position of the output matrix with no modifications or transformations.\n\nInput:\nmatrix_input: float32, shape = (2048, 2048)\n\nOutput:\nmatrix_out: float32, shape = (2048, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 34, "task_name": "Matrix_Copy", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Copy\n\nTask Description:\nImplement a CUDA kernel named Matrix_Copy that copies a 8192x8192 matrix of float32 elements to a flattened output vector. The input matrix is stored in row-major order, and the output must be a 67108864-element float32 vector containing an exact copy of the input data. The kernel must only process valid indices within the matrix dimensions and preserve the original data layout.\n\nInput:\nmatrix_input: float32, shape = (8192, 8192)\n\nOutput:\nmatrix_out: float32, shape = (67108864,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 48, "task_name": "Prefix_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPrefix_Sum\n\nTask Description:\nTask: Prefix_Sum. Implement a CUDA kernel that computes the prefix sum (cumulative sum) of a 1D input array. The input is a tensor named 'prefix_input' with shape (16384,) and dtype float32. The output tensor 'prefix_out' must have the same shape and dtype, where each element at position i is the sum of all input elements from index 0 to i. The kernel must maintain sequential accumulation order and produce results within a numerical tolerance of 1e-2 relative to the reference implementation.\n\nInput:\nprefix_input: float32, shape = (16384,)\n\nOutput:\nprefix_out: float32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 33, "task_name": "Matrix_Copy", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Copy\n\nTask Description:\nImplement a CUDA kernel named Matrix_Copy that copies a 4096x4096 matrix of single-precision floating-point numbers from an input tensor to an output tensor. The input tensor is named 'matrix_input' and the output tensor is named 'matrix_out', both with shape (4096, 4096) and data type float32. The kernel must copy each element exactly without modification.\n\nInput:\nmatrix_input: float32, shape = (4096, 4096)\n\nOutput:\nmatrix_out: float32, shape = (4096, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 57, "task_name": "Monte_Carlo_Integration", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMonte_Carlo_Integration\n\nTask Description:\nImplement a CUDA kernel for Monte Carlo integration. The kernel should approximate the definite integral of a function over the interval [0.0, 0.5] using 65536 randomly sampled points. The input is a one-dimensional float32 tensor 'mc_y' of shape (65536,) containing function evaluations at random points in the interval. The output should be a single float32 value 'mc_out' of shape (1,) representing the integral estimate. The kernel must compute the average of all input values, scale it by the interval length (0.5 - 0.0), and use atomic operations for thread-safe accumulation.\n\nInput:\nmc_y: float32, shape = (65536,)\n\nOutput:\nmc_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 9, "task_name": "Post_Process_GL", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPost_Process_GL\n\nTask Description:\nTask: Post_Process_GL. Given an input image of shape (1024, 1024, 4) with uint8 data type, compute an output image of shape (1024, 1024) with uint32 data type. For each pixel, apply a disc-shaped convolution with radius 4. For each neighboring pixel within the disc, if its luminance (average of RGB values) exceeds 0.8, multiply its RGB values by 2.0. Compute the average RGB values of all valid neighboring pixels within the disc and pack the result into a 32-bit ABGR format (alpha=255). Handle boundary pixels using clamping behavior. Ensure the output pixel values are clamped to [0,255].\n\nInput:\ninput_img: uint8, shape = (1024, 1024, 4)\n\nOutput:\noutput_img: uint32, shape = (1024, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 43, "task_name": "Dot_Product", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDot_Product\n\nTask Description:\nTask: Dot_Product. Compute the dot product of two vectors. Inputs are two vectors dot_input_a and dot_input_b, each containing 262144 elements of type float32. Output is a single float32 value stored in a tensor of shape (1,). The kernel must multiply corresponding elements from both vectors and accumulate their sum into a single result. The kernel must handle the entire vector length of 262144.\n\nInput:\ndot_input_a: float32, shape = (262144,)\ndot_input_b: float32, shape = (262144,)\n\nOutput:\ndot_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 35, "task_name": "Matrix_Copy", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Copy\n\nTask Description:\nImplement a CUDA kernel named Matrix_Copy that copies a 16384x16384 matrix of single-precision floating-point numbers from an input array to an output array. The input tensor is named 'matrix_input' and the output tensor is named 'matrix_out'. The kernel must be designed to handle the entire matrix and only operate on valid indices within the matrix dimensions.\n\nInput:\nmatrix_input: float32, shape = (16384, 16384)\n\nOutput:\nmatrix_out: float32, shape = (16384, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 37, "task_name": "Reduction", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReduction\n\nTask Description:\nTask: Reduction. Compute the sum of all elements in a one-dimensional input tensor. The input tensor is of shape (4096,) and data type float32. The output is a single float32 value in a tensor of shape (1,) which is the sum. The computed sum must be accurate within a tolerance of 1e-2 relative to a reference sum.\n\nInput:\nreduce_input: float32, shape = (4096,)\n\nOutput:\nreduce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 52, "task_name": "Categorical_Cross-Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCategorical_Cross-Entropy_Loss\n\nTask Description:\nTask: Categorical_Cross-Entropy_Loss. Compute the average cross-entropy loss for a batch of 65536 samples across 10 classes. Inputs: 'ce_logits' is a float32 tensor of shape (65536, 10) containing unnormalized log probabilities, and 'ce_labels' is an int32 tensor of shape (65536,) containing true class indices in [0,9]. Output: 'ce_out' is a float32 scalar representing the batch-averaged loss. For each sample, find the maximum logit in its row, compute the log-sum-exp of shifted logits, and calculate loss as log-sum-exp plus max logit minus the true class logit. Accumulate each sample's loss contribution (divided by 65536) atomically into the output scalar to avoid race conditions.\n\nInput:\nce_logits: float32, shape = (65536, 10)\nce_labels: int32, shape = (65536,)\n\nOutput:\nce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 21, "task_name": "Top-K_Selection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTop-K_Selection\n\nTask Description:\nTask: Top-K_Selection. Given an input tensor 'topk_input' of 1024 floating-point numbers, the kernel must compute an output tensor 'topk_out' of 32 floating-point numbers. The output must contain the 32 largest values from the input, sorted in descending order (from largest to smallest). The kernel must respect the constraint that the output is exactly the top 32 values and in the correct order.\n\nInput:\ntopk_input: float32, shape = (1024,)\n\nOutput:\ntopk_out: float32, shape = (32,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 61, "task_name": "Histogramming", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHistogramming\n\nTask Description:\nImplement a Histogramming kernel that counts the frequency of integer values in an input array. The input is a 1D tensor of 65536 integers (int32). The output should be a 1D tensor of 256 integers (int32) representing counts for values 0 through 255. Only values between 0 and 255 inclusive should be counted. The kernel must safely handle concurrent updates to histogram bins when multiple input values map to the same bin.\n\nInput:\nhist_input: int32, shape = (65536,)\n\nOutput:\nhist_out: int32, shape = (256,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 58, "task_name": "Monte_Carlo_Integration", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMonte_Carlo_Integration\n\nTask Description:\nImplement a Monte Carlo integration kernel to approximate the definite integral of sin(2πx) over the interval [0, 0.5]. The input is a 1D tensor 'mc_y' containing 262144 float32 values representing function evaluations at randomly sampled points. The kernel must compute the average of these values and store the result in a scalar output tensor 'mc_out' of float32 type. Use atomic operations to safely accumulate partial sums across threads. The final scaling by the interval length (0.5) occurs after kernel execution.\n\nInput:\nmc_y: float32, shape = (262144,)\n\nOutput:\nmc_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 62, "task_name": "Histogramming", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHistogramming\n\nTask Description:\nImplement a CUDA kernel for Histogramming. The kernel takes a 1D input tensor 'hist_input' with 262144 elements of type int32. It must compute a histogram output tensor 'hist_out' with 256 elements of type int32, where each element at index i contains the count of how many times the value i appears in the input. Values in the input must be between 0 and 255 inclusive; any values outside this range should be ignored. The implementation must use atomic operations to ensure thread safety when updating the histogram bins.\n\nInput:\nhist_input: int32, shape = (262144,)\n\nOutput:\nhist_out: int32, shape = (256,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 54, "task_name": "Categorical_Cross-Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCategorical_Cross-Entropy_Loss\n\nTask Description:\nImplement the Categorical Cross-Entropy Loss computation. The input consists of a 2D float32 tensor 'ce_logits' with shape (1048576, 10) representing logits for 1048576 samples across 10 classes, and a 1D int32 tensor 'ce_labels' with shape (1048576,) containing the true class indices. The output should be a scalar float32 tensor 'ce_out' representing the average cross-entropy loss. For each sample, compute the loss by: 1) finding the maximum logit value, 2) calculating the sum of exponentials of (logit - max_logit) for all classes, 3) taking the logarithm of this sum, and 4) subtracting the true class's logit from (log_sum + max_logit). Accumulate each sample's loss divided by 1048576 into the output scalar. Numeric stability must be maintained through max subtraction.\n\nInput:\nce_logits: float32, shape = (1048576, 10)\nce_labels: int32, shape = (1048576,)\n\nOutput:\nce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 67, "task_name": "Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSigmoid\n\nTask Description:\nImplement a CUDA kernel for the Sigmoid task. The kernel must compute the sigmoid function for each element in a 16x4096 input tensor of 32-bit floats. The sigmoid function is defined as 1/(1 + exp(-x)). The output tensor must be the same shape and data type as the input. All elements must be processed independently, and the output values must lie within the range (0,1).\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 78, "task_name": "Batched_Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatched_Matrix_Multiplication\n\nTask Description:\nTask: Batched_Matrix_Multiplication. Compute matrix multiplications for a batch of matrices. Inputs: 'batchA' (16 matrices, each 512x256, float32) and 'batchB' (16 matrices, each 256x128, float32). Output: 'batchC_out' (16 matrices, each 512x128, float32). Each output matrix must be computed as the matrix multiplication of the corresponding input matrices from batchA and batchB. The kernel must respect matrix dimensions and batch independence, ensuring each batch element is processed separately.\n\nInput:\nbatchA: float32, shape = (16, 512, 256)\nbatchB: float32, shape = (16, 256, 128)\n\nOutput:\nbatchC_out: float32, shape = (16, 512, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 63, "task_name": "Histogramming", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHistogramming\n\nTask Description:\nTask: Histogramming. Compute a histogram from an input array of integers. The input is a 1D tensor named 'hist_input' with 1,048,576 elements of type int32. The output is a 1D tensor named 'hist_out' with 256 elements of type int32, where each element at index i represents the count of value i in the input. Input values must be in the range [0, 255]; values outside this range should be ignored. The histogram must be computed by incrementing counts using thread-safe operations.\n\nInput:\nhist_input: int32, shape = (1048576,)\n\nOutput:\nhist_out: int32, shape = (256,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 50, "task_name": "Prefix_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPrefix_Sum\n\nTask Description:\nImplement a CUDA kernel named 'Prefix_Sum' that computes the cumulative sum (prefix sum) of a one-dimensional input tensor. The input tensor is named 'prefix_input', has a shape of (262144,), and uses float32 data type. The output tensor 'prefix_out' must have the same shape and data type, where each element at index i is the sum of all input elements from index 0 to i. The kernel must maintain sequential dependency where each output element depends on the previous sum.\n\nInput:\nprefix_input: float32, shape = (262144,)\n\nOutput:\nprefix_out: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 68, "task_name": "Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSigmoid\n\nTask Description:\nSigmoid task: Implement a CUDA kernel to compute the sigmoid function for each element in a 2D input tensor. The input is a float32 tensor of shape (16, 16384), and the output must be a float32 tensor of identical shape. The sigmoid function is defined as 1 / (1 + exp(-x)) for each element x. The kernel must process all 262,144 elements independently with numerical precision within 1e-5 tolerance compared to a reference implementation.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 55, "task_name": "Categorical_Cross-Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCategorical_Cross-Entropy_Loss\n\nTask Description:\nThe task is to compute the categorical cross-entropy loss. Inputs are a 2D float32 tensor 'ce_logits' with shape (4194304, 10) representing logits for each sample and class, and a 1D int32 tensor 'ce_labels' with shape (4194304) containing the true class indices. The output is a single float32 scalar 'ce_out' representing the average loss. For each sample, determine the maximum logit value across classes to ensure numerical stability. Compute the sum of exponentials of (each logit minus the maximum logit). Calculate the loss as the logarithm of this sum plus the maximum logit, minus the logit corresponding to the true label. Accumulate these sample losses and compute their average across all samples. The kernel must handle atomic accumulation to avoid race conditions and ensure numerical stability.\n\nInput:\nce_logits: float32, shape = (4194304, 10)\nce_labels: int32, shape = (4194304,)\n\nOutput:\nce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 72, "task_name": "Ordinary_Least_Squares_Regression", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nOrdinary_Least_Squares_Regression\n\nTask Description:\nTask: Ordinary Least Squares Regression. Implement a CUDA kernel to compute linear regression coefficients. Inputs: a feature matrix 'ols_X' of shape (65536, 10) with float32 data type, and a target vector 'ols_y' of shape (65536,) with float32 data type. Output: a coefficient vector 'ols_out' of shape (10,) with float32 data type. The kernel must compute X^T X (10x10 matrix) and X^T y (10-element vector) on GPU, then solve the linear system (X^T X)β = X^T y on CPU to obtain coefficients. The solution must match reference within tolerance.\n\nInput:\nols_X: float32, shape = (65536, 10)\nols_y: float32, shape = (65536,)\n\nOutput:\nols_out: float32, shape = (10,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 66, "task_name": "Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSigmoid\n\nTask Description:\nTask: Sigmoid. Implement a CUDA kernel that computes the sigmoid function for each element in a 2D input tensor of shape (16, 1024) with float32 data type. The output tensor must have the same shape and data type. The sigmoid function is defined as output = 1.0 / (1.0 + exp(-input)) for each element. The kernel must handle all elements independently and respect the input tensor's dimensions.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 70, "task_name": "Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSigmoid\n\nTask Description:\nImplement a CUDA kernel named 'Sigmoid' that computes the sigmoid function element-wise on a 2D input tensor of shape (16, 262144) with float32 data type. The input tensor is named 'input', and the output tensor must have the same shape and data type, named 'output'. For each element x in the input, the output must be computed as 1.0 / (1.0 + exp(-x)). The kernel must handle all elements independently and preserve input-output dimensionality.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 24, "task_name": "Top-K_Selection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTop-K_Selection\n\nTask Description:\nTask: Top-K_Selection. The kernel must select the top 256 largest values from an input array of 65536 float32 numbers and output them in descending order (largest first). The input is a 1D tensor of shape (65536,) with dtype float32. The output must be a 1D tensor of shape (256,) with dtype float32, containing exactly the 256 largest values sorted in descending order. The kernel must maintain this sorted order throughout insertion and ensure no extraneous values are included.\n\nInput:\ntopk_input: float32, shape = (65536,)\n\nOutput:\ntopk_out: float32, shape = (256,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 65, "task_name": "Histogramming", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHistogramming\n\nTask Description:\nTask: Histogramming. Compute a 256-bin histogram for an input array of 16,777,216 integers. The input array contains values between 0 and 255 inclusive. The output is a 256-element array where each element at index i represents the count of occurrences of value i in the input. The kernel must ignore input values outside the [0,255] range and initialize the output array to zeros before accumulation.\n\nInput:\nhist_input: int32, shape = (16777216,)\n\nOutput:\nhist_out: int32, shape = (256,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 64, "task_name": "Histogramming", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHistogramming\n\nTask Description:\nTask: Histogramming. Compute a histogram of the input array, where each element represents an integer value. The input is a 1D tensor with 4,194,304 elements of type int32, and each value must be in the range [0, 255]. The output is a 1D tensor with 256 elements of type int32, where each element at index i indicates the count of occurrences of the value i in the input. Values outside the [0, 255] range should be ignored, and the output histogram must be initialized to zero before accumulation.\n\nInput:\nhist_input: int32, shape = (4194304,)\n\nOutput:\nhist_out: int32, shape = (256,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 75, "task_name": "Ordinary_Least_Squares_Regression", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nOrdinary_Least_Squares_Regression\n\nTask Description:\nOrdinary Least Squares Regression: Compute the regression coefficients for a linear model by solving the normal equations. Inputs include a feature matrix 'ols_X' of shape (4194304, 10) and a target vector 'ols_y' of shape (4194304), both float32. Output is a coefficient vector 'ols_out' of shape (10) as float32. The solution must compute the matrix product X^T·X and vector product X^T·y, then solve the linear system (X^T·X)·β = X^T·y. The computed coefficients must match reference solutions within a tolerance of 1e-2.\n\nInput:\nols_X: float32, shape = (4194304, 10)\nols_y: float32, shape = (4194304,)\n\nOutput:\nols_out: float32, shape = (10,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 79, "task_name": "Batched_Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatched_Matrix_Multiplication\n\nTask Description:\nImplement a CUDA kernel for Batched_Matrix_Multiplication. The kernel must multiply 32 pairs of matrices independently: each matrix A of shape (1024, 512) from input batchA with a corresponding matrix B of shape (512, 256) from input batchB, producing an output matrix C of shape (1024, 256) in batchC_out. All tensors are float32. The computation must follow standard matrix multiplication rules where each element in C is the dot product of corresponding rows from A and columns from B. The kernel must process all 32 batches in parallel without inter-batch dependencies.\n\nInput:\nbatchA: float32, shape = (32, 1024, 512)\nbatchB: float32, shape = (32, 512, 256)\n\nOutput:\nbatchC_out: float32, shape = (32, 1024, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 59, "task_name": "Monte_Carlo_Integration", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMonte_Carlo_Integration\n\nTask Description:\nTask: Monte Carlo Integration. Approximate the integral of a function over the interval [0.0, 0.5] using Monte Carlo integration with 1048576 samples. The input tensor 'mc_y' (shape: (1048576,), dtype: float32) contains function values at randomly sampled points. The output tensor 'mc_out' (shape: (1,), dtype: float32) must be computed as the average of all values in 'mc_y' multiplied by the interval length (0.5). The kernel must correctly accumulate contributions from all samples without race conditions.\n\nInput:\nmc_y: float32, shape = (1048576,)\n\nOutput:\nmc_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 56, "task_name": "Monte_Carlo_Integration", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMonte_Carlo_Integration\n\nTask Description:\nImplement the Monte_Carlo_Integration kernel. The kernel must approximate the integral of a function over the interval [0.0, 0.5] using the Monte Carlo method. The input is a 1D tensor named 'mc_y' with 16384 float32 values, representing function evaluations at random points in the interval. The output is a single float32 value in a tensor named 'mc_out', which is the estimated integral scaled by the interval length (0.5). The kernel must compute the average of the input values (by summing each value divided by 16384) and use atomic operations to accumulate the result. The kernel must process all 16384 elements.\n\nInput:\nmc_y: float32, shape = (16384,)\n\nOutput:\nmc_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 8, "task_name": "Post_Process_GL", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPost_Process_GL\n\nTask Description:\nTask: Post_Process_GL. Implement a CUDA kernel that processes a 512x512 input image with 4 channels (RGBA) of type uint8. The kernel must apply a disc-shaped convolution with radius 4, where pixels within Euclidean distance ≤4 are included. For pixels with luminance (R+G+B)/(3×255) > 0.8, multiply RGB values by 2.0 before averaging. Compute the average RGB for valid pixels, clamp results to [0,255], and pack into uint32 output in ABGR format (alpha=255). Boundary handling must use clamping. Output is a flattened 262144-element uint32 tensor.\n\nInput:\ninput_img: uint8, shape = (512, 512, 4)\n\nOutput:\noutput_img: uint32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 76, "task_name": "Batched_Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatched_Matrix_Multiplication\n\nTask Description:\nImplement the Batched_Matrix_Multiplication kernel. Given two input tensors: 'batchA' of shape (4, 128, 256) and 'batchB' of shape (4, 256, 128), both with float32 data type, compute a batch of matrix multiplications. For each of the 4 matrices in the batch, multiply the 128x256 matrix from 'batchA' with the corresponding 256x128 matrix from 'batchB'. The result should be a single output tensor 'batchC_out' of shape (4, 128, 128) with float32 data type, where each 128x128 matrix is the product of the corresponding input matrices. The kernel must handle all 4 batches simultaneously and compute each element as the dot product of the appropriate row and column vectors.\n\nInput:\nbatchA: float32, shape = (4, 128, 256)\nbatchB: float32, shape = (4, 256, 128)\n\nOutput:\nbatchC_out: float32, shape = (4, 128, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 69, "task_name": "Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSigmoid\n\nTask Description:\nImplement the Sigmoid kernel on GPU. The kernel should compute the sigmoid function element-wise for a 16x65536 input tensor of float32 values, producing a flattened output tensor of 1048576 float32 elements. The sigmoid function is defined as output = 1.0 / (1.0 + exp(-input)) for each element. The kernel must produce results within 1e-5 tolerance relative to a reference implementation.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 84, "task_name": "Gaussian_Blur", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGaussian_Blur\n\nTask Description:\nImplement a CUDA kernel named Gaussian_Blur that applies a Gaussian blur filter to an input image. The input is a 1024x2048 float32 image tensor and a 9x9 float32 kernel tensor. The output should be a 1024x2048 float32 tensor where each pixel is computed as the weighted sum of its neighbors using the kernel weights. The kernel must handle boundary conditions by only including pixels within image bounds and maintain the original image dimensions.\n\nInput:\nimage: float32, shape = (1024, 2048)\nkernel: float32, shape = (9, 9)\n\nOutput:\noutput: float32, shape = (1024, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 80, "task_name": "Batched_Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatched_Matrix_Multiplication\n\nTask Description:\nTask: Batched_Matrix_Multiplication. Compute matrix multiplication for a batch of matrices. Input batchA is a float32 tensor of shape (4, 2048, 1024) containing 4 matrices of size 2048x1024. Input batchB is a float32 tensor of shape (4, 1024, 512) containing 4 matrices of size 1024x512. Output batchC_out must be a float32 tensor of shape (4, 2048, 512) containing 4 matrices of size 2048x512. Each output matrix must be the product of corresponding matrices from batchA and batchB, computed independently per batch. The kernel must compute the standard matrix multiplication where each element in the output is the dot product of a row from the first matrix and a column from the second matrix.\n\nInput:\nbatchA: float32, shape = (4, 2048, 1024)\nbatchB: float32, shape = (4, 1024, 512)\n\nOutput:\nbatchC_out: float32, shape = (4, 2048, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 89, "task_name": "Matrix_Power", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Power\n\nTask Description:\nMatrix_Power: Compute the square (second power) of a given 256x256 matrix. The input is a single-precision floating-point tensor of shape (256, 256), and the output must be a single-precision floating-point tensor of the same shape representing the matrix squared. The kernel must handle square matrices and compute the exact mathematical matrix power operation for exponent 2.\n\nInput:\nmatA: float32, shape = (256, 256)\n\nOutput:\nmatC_out: float32, shape = (256, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 101, "task_name": "CeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCeLU\n\nTask Description:\nImplement the CeLU kernel. The CeLU function is defined as: for each element x in the input tensor, if x > 0, output = x; else output = alpha * (exp(x / alpha) - 1), where alpha is 1.0. The input is a 2D tensor of shape (16, 1024) with float32 data type. The output must have the same shape and data type as the input. The kernel must apply this transformation element-wise to all values in the input tensor.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 73, "task_name": "Ordinary_Least_Squares_Regression", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nOrdinary_Least_Squares_Regression\n\nTask Description:\nTask: Ordinary Least Squares Regression. Description: Implement a CUDA kernel to compute linear regression coefficients using ordinary least squares. The input consists of a feature matrix 'ols_X' with dimensions (262144, 10) and float32 data type, and a target vector 'ols_y' with length 262144 and float32 data type. The output must be a vector 'ols_out' of length 10 with float32 data type, representing the regression coefficients. The kernel must compute the solution to the normal equations (X^T X)β = X^T y, where X is the feature matrix and y is the target vector. The computed coefficients must match reference values within an absolute tolerance of 1e-2.\n\nInput:\nols_X: float32, shape = (262144, 10)\nols_y: float32, shape = (262144,)\n\nOutput:\nols_out: float32, shape = (10,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 83, "task_name": "Gaussian_Blur", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGaussian_Blur\n\nTask Description:\nImplement a Gaussian blur kernel for image processing. The input consists of a 1024x1024 float32 image and a 7x7 float32 kernel. The kernel should convolve the image with the Gaussian kernel, computing each output pixel as the weighted sum of surrounding pixels within the kernel window. Boundary handling must ignore out-of-bound pixels. Output is a 1048576-element float32 tensor (flattened 1024x1024 blurred image).\n\nInput:\nimage: float32, shape = (1024, 1024)\nkernel: float32, shape = (7, 7)\n\nOutput:\noutput: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 96, "task_name": "Binary_Cross_Entropy_With_Logits_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_With_Logits_Loss\n\nTask Description:\nImplement the Binary Cross Entropy With Logits Loss kernel. Compute the loss for each element between prediction and target vectors, both float32 tensors of shape (1024,). Accumulate individual losses into a single scalar output float32 tensor of shape (1,). Ensure the kernel handles element-wise computation correctly and accumulates results without race conditions. The final output must represent the mean loss across all elements.\n\nInput:\npred: float32, shape = (1024,)\ntarget: float32, shape = (1024,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 82, "task_name": "Gaussian_Blur", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGaussian_Blur\n\nTask Description:\nImplement a Gaussian blur filter for an image on a GPU. The input is a 512x512 float32 image and a 5x5 float32 kernel matrix. The output should be a 512x512 float32 blurred image. For each output pixel, compute a weighted sum of surrounding pixels using the kernel weights, handling image boundaries by skipping out-of-range pixels. The kernel center must be aligned with each pixel location during computation.\n\nInput:\nimage: float32, shape = (512, 512)\nkernel: float32, shape = (5, 5)\n\nOutput:\noutput: float32, shape = (512, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 53, "task_name": "Categorical_Cross-Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCategorical_Cross-Entropy_Loss\n\nTask Description:\nTask: Categorical_Cross-Entropy_Loss. Compute the average cross-entropy loss for a batch of 262144 samples across 10 classes. Inputs: 'ce_logits' is a 2D float32 tensor of shape (262144, 10) representing unnormalized logits, and 'ce_labels' is a 1D int32 tensor of shape (262144,) containing true class indices (0-9). Output: 'ce_out' is a scalar float32 tensor (shape (1,)) holding the mean loss. For each sample, first find the maximum logit value for numerical stability. Then compute the exponential sum of logits adjusted by this max value, take its logarithm, and derive the loss as log_sum_exp plus max_logit minus the true class logit. Accumulate losses atomically across all samples and divide by 262144 to compute the final average.\n\nInput:\nce_logits: float32, shape = (262144, 10)\nce_labels: int32, shape = (262144,)\n\nOutput:\nce_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 103, "task_name": "CeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCeLU\n\nTask Description:\nImplement the CeLU (Continuously Exponential Linear Unit) activation function on GPU. The input is a 2D float32 tensor of shape (16, 16384). For each element x in the input, the output should be x if x > 0, and alpha * (exp(x/alpha) - 1) if x <= 0, where alpha is fixed at 1.0. The output must be a float32 tensor with the same shape as the input. Each element must be processed independently.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 60, "task_name": "Monte_Carlo_Integration", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMonte_Carlo_Integration\n\nTask Description:\nImplement a GPU kernel for Monte Carlo integration. The input is a 1D tensor named `mc_y` containing 4194304 float32 values, which are function evaluations of sin(2*pi*x) at random points uniformly sampled in the interval [0.0, 0.5]. The kernel must compute the average of these function evaluations by having each thread process one element, divide it by 4194304, and atomically add the result to a global accumulator. The output is a single float32 scalar representing this average. Note that the integral estimate is obtained by multiplying this average by the interval length (0.5) in the host code.\n\nInput:\nmc_y: float32, shape = (4194304,)\n\nOutput:\nmc_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 87, "task_name": "Matrix_Power", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Power\n\nTask Description:\nTask: Matrix_Power. Compute the 5th power of a 128x128 square matrix of 32-bit floating point numbers. The input is a matrix of shape (128, 128) and data type float32. The output must be a matrix of the same shape and type. The exponent is fixed at 5. The computation must be performed by multiplying the input matrix by itself four times (using matrix multiplication) to achieve the 5th power.\n\nInput:\nmatA: float32, shape = (128, 128)\n\nOutput:\nmatC_out: float32, shape = (128, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 86, "task_name": "Matrix_Power", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Power\n\nTask Description:\nTask: Matrix_Power. Compute the 4th power of a 64x64 matrix. The input is a single matrix 'matA' of shape (64, 64) and data type float32. The output is a matrix 'matC_out' of the same shape and type, which is the input matrix multiplied by itself three times (i.e., matA^4). The exponent is fixed at 4 and the matrix size is fixed at 64x64. The computation must be performed by chaining three matrix multiplications.\n\nInput:\nmatA: float32, shape = (64, 64)\n\nOutput:\nmatC_out: float32, shape = (64, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 81, "task_name": "Gaussian_Blur", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGaussian_Blur\n\nTask Description:\nImplement a Gaussian blur filter on a 256x256 image using CUDA. The input consists of a 256x256 float32 image tensor and a 3x3 float32 kernel tensor. The output should be a 256x256 float32 tensor where each pixel is computed by convolving the kernel with the surrounding pixels. The kernel must be centered on each pixel, and boundary conditions should be handled by ignoring out-of-bound pixels during convolution.\n\nInput:\nimage: float32, shape = (256, 256)\nkernel: float32, shape = (3, 3)\n\nOutput:\noutput: float32, shape = (256, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 104, "task_name": "CeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCeLU\n\nTask Description:\nImplement a GPU kernel for the CeLU activation function. The input is a 16x65536 tensor of float32 values. The output must be a tensor of the same shape and data type. For each element, if the value is positive, output the value unchanged; if negative, apply the transformation: alpha * (exp(value / alpha) - 1), where alpha is fixed at 1.0.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 93, "task_name": "Binary_Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_Loss\n\nTask Description:\nImplement a CUDA kernel to compute the binary cross entropy loss. The kernel should process two input tensors: 'pred' and 'target', both float32 tensors of shape (16384,). For each element, compute the loss as -[target * log(pred + 1e-12) + (1 - target) * log(1 - pred + 1e-12)]. Accumulate all element losses using atomic operations into a single float32 output tensor of shape (1,). The final mean loss is computed by dividing the accumulated loss by the batch size (16384) in host code.\n\nInput:\npred: float32, shape = (16384,)\ntarget: float32, shape = (16384,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 114, "task_name": "ELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nELU\n\nTask Description:\nImplement the ELU (Exponential Linear Unit) activation function kernel. The input is a 2D tensor of float32 values with shape (16, 65536). For each element in the input, if the value is greater than 0, output the same value. If the value is less than or equal to 0, output 1.0 multiplied by the exponential of the value minus 1. The output tensor must have the same shape and data type as the input. The operation must be applied element-wise independently.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 102, "task_name": "CeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCeLU\n\nTask Description:\nTask: CeLU. Given an input tensor of shape (16, 4096) with float32 values, compute the output tensor of the same shape and type. For each element in the input, if the element is greater than zero, the output element is the same as the input. Otherwise, the output element is 1.0 multiplied by the exponential of the input element minus 1.0. The computation must be performed element-wise.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 95, "task_name": "Binary_Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_Loss\n\nTask Description:\nImplement a CUDA kernel for Binary Cross Entropy Loss. The kernel takes two input vectors, 'pred' and 'target', each containing 262144 float32 values. For each element, compute the loss as: - [target * log(pred + 1e-12) + (1 - target) * log(1 - pred + 1e-12)]. The kernel must sum all these individual losses into a single output value (a scalar tensor of shape (1)). Use atomic operations to accumulate the sum. Ensure the kernel avoids out-of-bounds memory accesses by checking the index.\n\nInput:\npred: float32, shape = (262144,)\ntarget: float32, shape = (262144,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 116, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nImplement the GELU activation function on a GPU using CUDA. The input is a 16x1024 float32 tensor. The output must be a 16x1024 float32 tensor where each element is computed as 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3))). The kernel must process elements independently and preserve input shape.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 71, "task_name": "Ordinary_Least_Squares_Regression", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nOrdinary_Least_Squares_Regression\n\nTask Description:\nTask: Ordinary Least Squares Regression. Given a feature matrix X of shape (16384, 10) with float32 data type and a target vector y of shape (16384,) with float32 data type, compute the regression coefficients vector beta of shape (10,) with float32 data type. The solution must first compute the matrix X^T * X and the vector X^T * y, then solve the linear system (X^T * X) * beta = X^T * y for beta. The dimensions are fixed: 16384 samples and 10 features. The computation must respect numerical precision constraints for float32.\n\nInput:\nols_X: float32, shape = (16384, 10)\nols_y: float32, shape = (16384,)\n\nOutput:\nols_out: float32, shape = (10,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 110, "task_name": "Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCross_Entropy_Loss\n\nTask Description:\nImplement a CUDA kernel for computing the cross entropy loss. The kernel should process a batch of predictions and target labels. Inputs: 'pred' is a float32 tensor with shape (262144, 128) representing predicted logits for each sample and class. 'target' is an int64 tensor with shape (262144,) representing the true class index for each sample. Output: 'output' is a float32 tensor with shape (1,) containing the average cross entropy loss across all samples. The kernel must compute softmax values for each sample using numerically stable methods, calculate the negative log likelihood for the target class, accumulate losses atomically, and normalize by batch size.\n\nInput:\npred: float32, shape = (262144, 128)\ntarget: int64, shape = (262144,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 77, "task_name": "Batched_Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatched_Matrix_Multiplication\n\nTask Description:\nTask: Batched Matrix Multiplication. Compute matrix multiplication for a batch of 8 matrices. Inputs are two float32 tensors: batchA with shape (8, 256, 256) and batchB with shape (8, 256, 256). Output is a float32 tensor batchC_out with shape (8, 256, 256). For each batch index b from 0 to 7, multiply matrix batchA[b] (256x256) with matrix batchB[b] (256x256) to produce output matrix batchC_out[b] (256x256). Each element (i,j) in the output matrix must be the dot product of the i-th row of batchA[b] and the j-th column of batchB[b]. The kernel must handle all 8 batches and matrix dimensions exactly as specified.\n\nInput:\nbatchA: float32, shape = (8, 256, 256)\nbatchB: float32, shape = (8, 256, 256)\n\nOutput:\nbatchC_out: float32, shape = (8, 256, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 92, "task_name": "Binary_Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_Loss\n\nTask Description:\nImplement the Binary_Cross_Entropy_Loss CUDA kernel. The kernel takes two input vectors of float32, each of size 4096: one for predictions (pred) and one for targets (target). The kernel must compute the binary cross entropy loss for each element: for each index i, the loss term is - [ target[i] * log(pred[i] + 1e-12) + (1 - target[i]) * log(1 - pred[i] + 1e-12) ]. The kernel must accumulate the total loss by summing all these terms into a single output scalar (of shape (1,)). The accumulation must be done in a thread-safe manner to avoid race conditions. Note: the kernel does not compute the mean loss; the mean is computed by the host code by dividing the total loss by 4096. The addition of 1e-12 is to prevent numerical instability.\n\nInput:\npred: float32, shape = (4096,)\ntarget: float32, shape = (4096,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 88, "task_name": "Matrix_Power", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Power\n\nTask Description:\nTask name: Matrix_Power. Compute the cube (third power) of a 96x96 square matrix of 32-bit floating-point numbers. The input is a single matrix 'matA' of shape (96, 96) and dtype float32. The output must be a matrix of the same shape and dtype, which is the result of the matrix multiplication of the input matrix by itself twice. The exponent is fixed to 3 and the matrix dimensions are fixed at 96x96.\n\nInput:\nmatA: float32, shape = (96, 96)\n\nOutput:\n\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 113, "task_name": "ELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nELU\n\nTask Description:\nImplement the ELU activation function on a GPU using CUDA. The input is a 16x16384 tensor of 32-bit floats. For each element in the input tensor, if the value is greater than zero, output the same value; otherwise, output 1.0 multiplied by (exponential of the value minus 1). The output tensor must have the same shape and data type as the input. The kernel must be implemented for GPU execution.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 90, "task_name": "Matrix_Power", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Power\n\nTask Description:\nTask: Matrix_Power. Compute the matrix power of exponent 3 for a given 512x512 matrix. The input is a tensor named 'matA' of data type float32 and shape (512, 512). The output is a tensor named 'matC_out' of data type float32 and shape (512, 512). The exponent is fixed to 3 and the matrix dimension is fixed to 512.\n\nInput:\nmatA: float32, shape = (512, 512)\n\nOutput:\nmatC_out: float32, shape = (512, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 108, "task_name": "Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCross_Entropy_Loss\n\nTask Description:\nTask: Cross_Entropy_Loss. Compute the mean cross-entropy loss for a batch of predictions and targets. The inputs are a float32 tensor 'pred' with shape (16384, 128) representing predicted logits, and an int64 tensor 'target' with shape (16384,) representing the true class indices. The output is a float32 scalar 'output' representing the mean loss. For each sample, compute the softmax of the prediction vector, extract the log probability of the target class, negate it, sum these values across all samples, and divide by the batch size. The kernel must ensure numerical stability during softmax computation using an online approach and handle concurrent writes to the output with atomic operations.\n\nInput:\npred: float32, shape = (16384, 128)\ntarget: int64, shape = (16384,)\n\nOutput:\noutput: float32, shape = ()\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 97, "task_name": "Binary_Cross_Entropy_With_Logits_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_With_Logits_Loss\n\nTask Description:\nTask Name: Binary Cross Entropy With Logits Loss. Given two input tensors `pred` and `target`, both of shape (4096,) and dtype float32, compute the binary cross entropy loss with logits. The kernel must compute per-element losses using the formula: loss_i = -[target_i * (pred_i - log(1 + exp(pred_i))) + (1 - target_i) * (-log(1 + exp(pred_i)))]. The kernel must accumulate these losses into a single output scalar of shape (1,) and dtype float32 using atomic operations. Target values must be in the range [0,1], and the kernel must handle batch size 4096.\n\nInput:\npred: float32, shape = (4096,)\ntarget: float32, shape = (4096,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 106, "task_name": "Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCross_Entropy_Loss\n\nTask Description:\nTask: Cross_Entropy_Loss. Compute the mean cross entropy loss for a batch of predictions and target labels. Inputs include a float32 prediction tensor of shape (1024, 128) and an int64 target tensor of shape (1024,). The output is a single float32 value representing the averaged loss. The kernel must compute softmax values for each prediction row, calculate negative log probabilities for the target indices, sum these losses across all samples, and finally divide by batch size. Numerical stability must be maintained during softmax computation.\n\nInput:\npred: float32, shape = (1024, 128)\ntarget: int64, shape = (1024,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 118, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nImplement the Gaussian Error Linear Unit (GELU) activation function kernel on GPU. The input is a 2D tensor of shape (16, 16384) with float32 data type. The output must be a tensor of identical shape and data type. For each element x in the input tensor, compute GELU(x) = 0.5 * x * (1 + tanh(α * (x + β * x³))) where α = √(2/π) ≈ 0.7978845608 and β = 0.044715. The kernel must process all elements independently and maintain numerical precision within tolerance limits.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 91, "task_name": "Binary_Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_Loss\n\nTask Description:\nImplement a CUDA kernel for the Binary_Cross_Entropy_Loss task. The kernel should compute the binary cross entropy loss for a batch of 1024 predictions and corresponding binary targets. The inputs are two 1D tensors of float32: 'pred' of shape (1024,) containing predicted probabilities, and 'target' of shape (1024,) containing true binary labels (values between 0 and 1). The output is a scalar float32 tensor containing the mean loss over the batch. Each thread should compute the loss for one element: loss_i = - (target_i * log(pred_i + 1e-12) + (1 - target_i) * log(1 - pred_i + 1e-12)). The kernel must accumulate these individual losses into a global sum using atomic operations. The final mean is computed by dividing the accumulated sum by 1024 on the host side.\n\nInput:\npred: float32, shape = (1024,)\ntarget: float32, shape = (1024,)\n\nOutput:\noutput: float32, shape = ()\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 126, "task_name": "Hard_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Shrink\n\nTask Description:\nImplement the Hard_Shrink kernel for GPU computation. The kernel should apply an element-wise operation to an input tensor of shape (16, 1024) with float32 values. For each element, if its absolute value exceeds 0.5, preserve the original value; otherwise, set it to zero. The output tensor must match the input's shape and data type. The operation must be applied independently to each element without altering other data.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 85, "task_name": "Gaussian_Blur", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGaussian_Blur\n\nTask Description:\nTask: Gaussian_Blur. Perform a 2D convolution between a 2048x2048 image and an 11x11 Gaussian kernel to produce a blurred image. The input image is a 2D tensor of shape (2048, 2048) with float32 data type. The kernel is a 2D tensor of shape (11, 11) with float32 data type. The output is a flattened 1D tensor of 4194304 float32 elements representing the blurred image in row-major order. Handle boundary conditions by only including pixels within the image boundaries when the kernel window extends beyond edges.\n\nInput:\nimage: float32, shape = (2048, 2048)\nkernel: float32, shape = (11, 11)\n\nOutput:\noutput: float32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 112, "task_name": "ELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nELU\n\nTask Description:\nImplement the ELU activation function kernel. The input is a 2D tensor of shape (16, 4096) with float32 data type. The output must be a tensor of the same shape and data type. For each element in the input, if the element is positive, the output element is the same as the input. For elements that are less than or equal to zero, the output element is 1.0 multiplied by (the exponential of the input element minus 1). The kernel must perform this operation element-wise and independently for each element.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 94, "task_name": "Binary_Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_Loss\n\nTask Description:\nImplement a CUDA kernel for Binary Cross Entropy Loss. The kernel takes two input tensors: 'pred' (float32, shape [65536]) and 'target' (float32, shape [65536]). Compute the element-wise loss using the formula: -[target * log(pred + epsilon) + (1 - target) * log(1 - pred + epsilon)] where epsilon=1e-12. Atomically accumulate the sum of all losses into a single output scalar (float32, shape [1]). The host code will later divide this sum by 65536 to compute the average loss.\n\nInput:\npred: float32, shape = (65536,)\ntarget: float32, shape = (65536,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 120, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nImplement a CUDA kernel for the Gaussian Error Linear Unit (GELU) activation function. The kernel should compute the GELU function element-wise on a 2D input tensor of shape (16, 262144) with float32 data type. The output tensor must have the same shape and data type as the input. The GELU function is defined as: GELU(x) = 0.5 * x * (1 + tanh(α * (x + β * x³))), where α is sqrt(2/π) ≈ 0.7978845608028654 and β is 0.044715. The kernel must process each element independently and maintain numerical precision within an absolute tolerance of 1e-5 compared to a reference implementation.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 117, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nImplement the Gaussian Error Linear Unit (GELU) activation function as a CUDA kernel. The kernel should compute the GELU of each element in a 16x4096 input tensor of float32 values. The output must be a float32 tensor of identical dimensions. The GELU function is defined as: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3))), using the constants sqrt(2/pi) ≈ 0.7978845608028654 and 0.044715. The kernel must process all 65,536 elements independently.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 128, "task_name": "Hard_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Shrink\n\nTask Description:\nImplement the Hard_Shrink kernel for GPU computation. The kernel should process a 2D input tensor of shape (16, 16384) with float32 data type. For each element in the input tensor: if the absolute value of the element is greater than 0.5, output the element unchanged; otherwise, output zero. The output tensor must have identical shape and data type to the input. All elements must be processed independently with no data dependencies.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 111, "task_name": "ELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nELU\n\nTask Description:\nImplement the ELU activation function kernel. The kernel should take a 2D input tensor of shape (16, 1024) with float32 values and produce a 1D output tensor of 16384 float32 elements. For each element in the input, if the value is positive, output the value unchanged; if the value is non-positive, output alpha * (exp(value) - 1) where alpha is 1.0. The kernel must process all elements independently.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 98, "task_name": "Binary_Cross_Entropy_With_Logits_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_With_Logits_Loss\n\nTask Description:\nTask: Binary_Cross_Entropy_With_Logits_Loss. Compute the average binary cross-entropy loss with logits for a batch of predictions and targets. The inputs are two float32 tensors: 'pred' (shape 16384) containing predicted logits, and 'target' (shape 16384) containing ground truth labels between 0 and 1. The output is a single float32 scalar representing the average loss over all elements. The kernel must compute per-element losses using the formula -[y*(p - log(1+exp(p))) + (1-y)*(-log(1+exp(p)))] for each element, then safely accumulate and average them across all elements.\n\nInput:\npred: float32, shape = (16384,)\ntarget: float32, shape = (16384,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 130, "task_name": "Hard_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Shrink\n\nTask Description:\nTask: Hard_Shrink. Implement a CUDA kernel that applies the Hard Shrink function to each element of a 16x262144 float32 input tensor. The function outputs the element's value if its absolute value exceeds 0.5, otherwise outputs 0. The output tensor must have identical shape and data type to the input. Each element must be processed independently.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 125, "task_name": "Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Multiplication\n\nTask Description:\nImplement a matrix multiplication kernel for multiplying two matrices. The first input matrix, matA, is a float32 tensor with dimensions 262144 by 4096. The second input matrix, matB, is a float32 tensor with dimensions 4096 by 2048. The output matrix, matC_out, should be a float32 tensor with dimensions 262144 by 2048. Each element of the output matrix must be the dot product of the corresponding row from matA and column from matB. The kernel must handle boundary conditions to avoid out-of-bounds memory accesses and respect the specified matrix dimensions.\n\nInput:\nmatA: float32, shape = (262144, 4096)\nmatB: float32, shape = (4096, 2048)\n\nOutput:\nmatC_out: float32, shape = (262144, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 119, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nTask: GELU. Compute the Gaussian Error Linear Unit (GELU) activation function for each element of the input tensor. The input is a 2D tensor of shape (16, 65536) with float32 data type. The output is a tensor of the same shape and data type. The GELU function is defined as: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3))). The kernel must compute this function independently for every element.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 127, "task_name": "Hard_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Shrink\n\nTask Description:\nTask: Hard_Shrink. Implement a CUDA kernel that applies the Hard Shrink function to a 16 by 4096 float32 input tensor. The Hard Shrink function is defined as: for each element, if the absolute value is greater than 0.5, the output is the element itself; otherwise, the output is 0. The output tensor must have the same shape and data type as the input. The kernel must process each element independently.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 134, "task_name": "Hard_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Sigmoid\n\nTask Description:\nImplement the Hard_Sigmoid kernel for GPU computation using CUDA. The kernel must compute a piecewise function where for each element in the input tensor: if the value is less than or equal to -3.0, output 0.0; if the value is greater than or equal to 3.0, output 1.0; otherwise, output the linear transformation (value / 6.0) + 0.5. The input is a 2D tensor of shape (16, 65536) with float32 data type, and the output must be a tensor of identical shape and data type. The kernel must process all elements independently and maintain numerical precision.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 133, "task_name": "Hard_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Sigmoid\n\nTask Description:\nImplement the Hard_Sigmoid kernel. The kernel should compute the hard sigmoid function element-wise on a 2D input tensor of shape (16, 16384) with float32 values. For each element: output 0.0 if input ≤ -3.0, output 1.0 if input ≥ 3.0, and otherwise compute (input / 6.0) + 0.5. The output tensor must have identical shape and data type as the input. All elements must be processed independently.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 121, "task_name": "Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Multiplication\n\nTask Description:\nImplement a CUDA kernel for matrix multiplication. The kernel multiplies two matrices: matA (1024x4096, float32) and matB (4096x2048, float32), producing output matrix matC_out (1024x2048, float32). Each element of matC_out must be the dot product of the corresponding row from matA and column from matB. Ensure the kernel only processes valid indices within matrix dimensions.\n\nInput:\nmatA: float32, shape = (1024, 4096)\nmatB: float32, shape = (4096, 2048)\n\nOutput:\nmatC_out: float32, shape = (1024, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 105, "task_name": "CeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCeLU\n\nTask Description:\nImplement the CeLU activation function kernel. The input is a 2D tensor of shape (16, 262144) with float32 data type. For each element in the input, if the value is greater than 0, output the same value; otherwise, output 1.0 multiplied by (exponential of the value divided by 1.0 minus 1). The output should be a 1D tensor of shape (4194304,) with float32 data type, which is the flattened result of applying CeLU to all input elements. The constant alpha is fixed at 1.0.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 132, "task_name": "Hard_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Sigmoid\n\nTask Description:\nTask: Hard_Sigmoid. Implement a CUDA kernel that applies the Hard Sigmoid function element-wise to a 16x4096 float32 input tensor, producing an identically shaped output tensor. The function must follow these rules: for each element x, output 0.0 if x ≤ -3.0; output 1.0 if x ≥ 3.0; otherwise output x/6.0 + 0.5. The kernel must process all elements independently.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 99, "task_name": "Binary_Cross_Entropy_With_Logits_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_With_Logits_Loss\n\nTask Description:\nImplement a CUDA kernel for the Binary_Cross_Entropy_With_Logits_Loss task. The kernel takes two input tensors: 'pred' (float32, shape (65536)) containing logits, and 'target' (float32, shape (65536)) containing true labels (each between 0 and 1). The kernel must compute the binary cross entropy loss with logits for each element. The loss for an element is calculated as: loss_i = - [ target_i * (pred_i - log(1 + exp(pred_i))) + (1 - target_i) * (-log(1 + exp(pred_i))) ]. The kernel must accumulate the sum of all these element-wise losses into a single output value (a tensor of shape (1) and float32). The accumulation must be done in a thread-safe manner to avoid race conditions. Note: the division by the batch size (65536) to obtain the mean loss is performed by the host after the kernel execution.\n\nInput:\npred: float32, shape = (65536,)\ntarget: float32, shape = (65536,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 123, "task_name": "Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Multiplication\n\nTask Description:\nMatrix_Multiplication: Multiply two input matrices, matA (16384 rows by 4096 columns, float32) and matB (4096 rows by 2048 columns, float32), to produce an output matrix matC_out (16384 rows by 2048 columns, float32). Each element in the output matrix at row i and column j is computed as the dot product of the i-th row of matA and the j-th column of matB. The kernel must respect the bounds of the output matrix and only compute for valid indices.\n\nInput:\nmatA: float32, shape = (16384, 4096)\nmatB: float32, shape = (4096, 2048)\n\nOutput:\nmatC_out: float32, shape = (16384, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 122, "task_name": "Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Multiplication\n\nTask Description:\nImplement a CUDA kernel for matrix multiplication. The inputs are two matrices: matA of shape 4096x4096 and matB of shape 4096x2048, both with float32 elements. The output is a matrix matC_out of shape 4096x2048 with float32 elements. Each element (i, j) of matC_out must be computed as the dot product of the i-th row of matA and the j-th column of matB, summing over 4096 multiplications. The kernel must respect the matrix dimensions and only compute for valid indices (i in [0, 4095] and j in [0, 2047]).\n\nInput:\nmatA: float32, shape = (4096, 4096)\nmatB: float32, shape = (4096, 2048)\n\nOutput:\nmatC_out: float32, shape = (4096, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 107, "task_name": "Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCross_Entropy_Loss\n\nTask Description:\nTask: Cross_Entropy_Loss. Compute the average cross-entropy loss for a batch of 4096 samples across 128 classes. Inputs: 'pred' is a float32 tensor of shape (4096, 128) representing predicted logits, 'target' is an int64 tensor of shape (4096,) representing true class indices (each between 0 and 127). Output: 'output' is a float32 scalar tensor containing the average loss. The kernel must compute per-sample loss using numerically stable softmax (online max and exponent sum), accumulate negative log softmax values atomically for the target index, and leave averaging by batch size to host code.\n\nInput:\npred: float32, shape = (4096, 128)\ntarget: int64, shape = (4096,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 136, "task_name": "Hard_Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Swish\n\nTask Description:\nImplement the Hard_Swish kernel for a 2D input tensor of shape (16, 1024) with float32 data type. The kernel should compute the Hard Swish activation function element-wise: for each input value x, output 0 if x ≤ -3, output x if x ≥ 3, and output x*(x+3)/6 for values between -3 and 3. The output tensor must have identical shape and data type as the input.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 74, "task_name": "Ordinary_Least_Squares_Regression", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nOrdinary_Least_Squares_Regression\n\nTask Description:\nOrdinary Least Squares Regression: Implement a CUDA kernel to compute the linear regression coefficients for a given dataset. The inputs are a feature matrix 'ols_X' of shape (1048576, 10) with float32 values and a target vector 'ols_y' of shape (1048576,) with float32 values. The output is a coefficient vector 'ols_out' of shape (10,) with float32 values. The kernel must compute X^T X and X^T y, then solve the linear system (X^T X) * beta = X^T y to obtain the regression coefficients. The solution must be accurate within a tolerance of 1e-2 relative to the reference implementation.\n\nInput:\nols_X: float32, shape = (1048576, 10)\nols_y: float32, shape = (1048576,)\n\nOutput:\nols_out: float32, shape = (10,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 129, "task_name": "Hard_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Shrink\n\nTask Description:\nImplement the Hard_Shrink kernel to apply a thresholding function on a 16x65536 float32 input tensor. The kernel must output a float32 tensor of identical shape where each element equals the input value if its absolute value exceeds 0.5, otherwise output zero. The computation must be element-wise independent and preserve the input tensor's dimensions.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 137, "task_name": "Hard_Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Swish\n\nTask Description:\nImplement the Hard_Swish function as a CUDA kernel. The function computes an element-wise transformation where each output value depends on the input value: if the input value is less than or equal to -3, the output is 0; if the input value is greater than or equal to 3, the output equals the input value; otherwise, for input values between -3 and 3, the output is computed as the input multiplied by the sum of the input and 3, then divided by 6. The input is a 16x4096 tensor of float32 values, and the output must be a tensor of the same shape and data type. Each element must be processed independently.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 124, "task_name": "Matrix_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Multiplication\n\nTask Description:\nTask: Matrix_Multiplication. Multiply two matrices matA (shape: 65536x4096) and matB (shape: 4096x2048), both float32, to produce output matrix matC_out (shape: 65536x2048) where each element C[i][j] is the dot product of row i from matA and column j from matB. The kernel must compute all valid output elements where 0 ≤ i < 65536 and 0 ≤ j < 2048, with no out-of-bounds access.\n\nInput:\nmatA: float32, shape = (65536, 4096)\nmatB: float32, shape = (4096, 2048)\n\nOutput:\nmatC_out: float32, shape = (65536, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 138, "task_name": "Hard_Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Swish\n\nTask Description:\nImplement the Hard_Swish kernel to compute the Hard Swish activation function element-wise on an input tensor. The input is a 16x16384 tensor of float32 values. The output must be a tensor of the same shape and data type. For each element x, the function is defined as: if x <= -3, output is 0; if x >= 3, output is x; otherwise, output is x multiplied by (x + 3) divided by 6. The kernel must process all elements independently and handle the entire input size efficiently.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 140, "task_name": "Hard_Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Swish\n\nTask Description:\nTask: Hard_Swish. Implement a CUDA kernel that computes the Hard Swish activation function element-wise on a 2D input tensor of shape (16, 262144) with float32 data type. The output tensor must have the same shape and data type. The Hard Swish function is defined as: for each element x, if x <= -3.0, output 0.0; if x >= 3.0, output x; otherwise, output x * (x + 3.0) / 6.0.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 142, "task_name": "Hard_Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Tanh\n\nTask Description:\nImplement a CUDA kernel named 'Hard_Tanh' that applies the hard tanh activation function to each element of an input tensor. The input is a 2D tensor of shape (16, 4096) with float32 data type. The output must be a tensor of identical shape and data type. For each element, the kernel must clamp values below -1.0 to -1.0 and values above 1.0 to 1.0, leaving values between -1.0 and 1.0 unchanged. The kernel must process all elements independently and maintain numerical precision.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 115, "task_name": "ELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nELU\n\nTask Description:\nTask name: ELU. Apply the Exponential Linear Unit (ELU) activation function element-wise to each value in the input tensor. The input is a 2D tensor of shape (16, 262144) with float32 data type. The output must be a 1D tensor of 4194304 float32 elements. For each element x in the input, if x > 0, output x unchanged; if x ≤ 0, output 1.0 × (e^x - 1). The operation must be performed independently for each element, and the alpha parameter is fixed to 1.0.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 141, "task_name": "Hard_Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Tanh\n\nTask Description:\nImplement a CUDA kernel for the Hard_Tanh task. The kernel should apply a hard tanh activation function element-wise to an input tensor of shape (16, 1024) with float32 data type. For each element, if the value is less than -1, it should be set to -1; if greater than 1, it should be set to 1; otherwise, it should remain unchanged. The output tensor must have the same shape (16, 1024) and float32 data type as the input. The kernel must process all 16,384 elements independently and efficiently.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 145, "task_name": "Hard_Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Tanh\n\nTask Description:\nTask: Hard_Tanh. Implement a CUDA kernel that computes the HardTanh activation function element-wise on a 2D input tensor of shape (16, 262144) with float32 data. For each element, clamp values below -1.0 to -1.0 and values above 1.0 to 1.0, leaving other values unchanged. The output tensor must match the input shape exactly.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 153, "task_name": "KL_Div_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKL_Div_Loss\n\nTask Description:\nImplement a CUDA kernel named 'KL_Div_Loss' that computes the Kullback-Leibler divergence loss between two input vectors. The inputs are two float32 tensors named 'pred' and 'target', each of shape (16384,). The output should be a single float32 value representing the mean KL divergence loss. The kernel must compute the loss as the sum of target[i] * (log(target[i]) - pred[i]) for all elements i, followed by averaging over the batch size of 16384. Use atomic operations for safe accumulation.\n\nInput:\npred: float32, shape = (16384,)\ntarget: float32, shape = (16384,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 148, "task_name": "Huber_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHuber_Loss\n\nTask Description:\nTask: Huber_Loss. Compute the Huber loss between two input tensors and output a single scalar value representing the mean loss. Inputs are two float32 tensors named 'input' and 'target', both with shape (16, 16384). Output is a float32 tensor named 'output' with shape (1,). The Huber loss function uses a delta value of 2.0: for each element, if |input - target| < 2.0, compute 0.5 * (input - target)^2; else compute 2.0 * (|input - target| - 1.0). The kernel must accumulate all element-wise losses and then compute the mean by dividing the sum by the total number of elements (16 * 16384).\n\nInput:\ninput: float32, shape = (16, 16384)\ntarget: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 135, "task_name": "Hard_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Sigmoid\n\nTask Description:\nTask: Hard_Sigmoid. Given an input tensor of shape (16, 262144) with float32 data type, compute an output tensor of the same shape and data type by applying the Hard Sigmoid function element-wise. The Hard Sigmoid function is defined as: for each element x, if x <= -3.0, set output to 0.0; if x >= 3.0, set output to 1.0; otherwise, set output to x/6 + 0.5. The kernel must process each element independently.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 147, "task_name": "Huber_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHuber_Loss\n\nTask Description:\nTask: Huber_Loss. Compute the mean Huber loss between two input tensors. Inputs: 'input' and 'target', both float32 tensors of shape (16, 4096). Output: a single float32 scalar representing the mean loss. For each element, if |input - target| < 2.0, compute 0.5 * (input - target)^2; otherwise, compute 2.0 * (|input - target| - 1.0). Sum all element-wise losses and divide by total elements (16*4096) to get the mean.\n\nInput:\ninput: float32, shape = (16, 4096)\ntarget: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 155, "task_name": "KL_Div_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKL_Div_Loss\n\nTask Description:\nImplement a CUDA kernel to compute the Kullback-Leibler divergence loss (KL_Div_Loss). The kernel should take two input vectors 'pred' and 'target', each containing 262144 float32 values. The computation involves calculating the element-wise loss as target[i] * (log(target[i]) - pred[i]), then summing these values across all elements. The final output should be a single float32 value representing the mean loss (sum divided by 262144). The kernel must use atomic operations for safe summation and include bounds checking.\n\nInput:\npred: float32, shape = (262144,)\ntarget: float32, shape = (262144,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 149, "task_name": "Huber_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHuber_Loss\n\nTask Description:\nImplement a CUDA kernel for the Huber_Loss task. The kernel computes the Huber loss between two input tensors 'input' and 'target', both of shape (16, 65536) and dtype float32. The output is a single scalar value of dtype float32. For each element pair (x, y) in the input and target tensors, the loss is calculated as: if |x - y| < 2.0, use 0.5 * (x - y)^2; otherwise, use 2.0 * (|x - y| - 1.0). The kernel must atomically accumulate the loss values across all elements and output the sum. After kernel execution, the host code will compute the mean by dividing the sum by the total number of elements (16 * 65536).\n\nInput:\ninput: float32, shape = (16, 65536)\ntarget: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 159, "task_name": "L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nL1_Loss\n\nTask Description:\nImplement an L1_Loss kernel that computes the mean absolute error between two input tensors. The inputs are two float32 tensors named 'input' and 'target', both with shape (16, 65536). The kernel must compute the average of absolute differences between corresponding elements across all positions. The output is a single scalar float32 value representing the mean absolute error. The kernel must process all elements (16 * 65536 total elements) and ensure atomic accumulation for thread safety.\n\nInput:\ninput: float32, shape = (16, 65536)\ntarget: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = ()\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 156, "task_name": "L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nL1_Loss\n\nTask Description:\nL1_Loss: Compute the mean absolute error (L1 loss) between two input tensors. The inputs are two float32 tensors named 'input' and 'target', each with shape (16, 1024). The output is a single float32 scalar value representing the average absolute difference across all elements. The kernel must compute the sum of absolute differences for all corresponding elements in the tensors and then divide by the total number of elements (16 × 1024) to produce the final output.\n\nInput:\ninput: float32, shape = (16, 1024)\ntarget: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 160, "task_name": "L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nL1_Loss\n\nTask Description:\nImplement the L1_Loss kernel to compute the mean absolute error between two input tensors. The inputs are two float32 tensors named 'input' and 'target', both with shape (16, 262144). The output is a scalar float32 tensor. The kernel must compute the absolute difference for each corresponding element between input and target, sum these differences across all elements, then divide by the total number of elements (16 × 262144) to produce the mean L1 loss.\n\nInput:\ninput: float32, shape = (16, 262144)\ntarget: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = ()\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 164, "task_name": "Leaky_ReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLeaky_ReLU\n\nTask Description:\nImplement a CUDA kernel named 'Leaky_ReLU' that applies the Leaky ReLU activation function to a 2D input tensor of shape (16, 65536) containing float32 values. The output tensor must have identical shape and dtype. For each element x in the input, compute: output = x if x > 0, otherwise output = 0.01 * x. The kernel must process all elements independently and maintain the input-output shape correspondence.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 150, "task_name": "Huber_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHuber_Loss\n\nTask Description:\nImplement the Huber_Loss kernel to compute the mean Huber loss between two input tensors. The inputs are two float32 tensors named 'input' and 'target', each with shape (16, 262144). The output is a single float32 value in a tensor of shape (1,). For each element pair (x, y) in the input and target tensors: if |x - y| < 2.0, add 0.5 * (x - y)^2 to the total loss; otherwise, add 2.0 * (|x - y| - 1.0). Accumulate losses atomically across all elements, then divide the total loss by the number of elements (16 * 262144) to compute the mean.\n\nInput:\ninput: float32, shape = (16, 262144)\ntarget: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 152, "task_name": "KL_Div_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKL_Div_Loss\n\nTask Description:\nImplement a CUDA kernel named KL_Div_Loss to compute the Kullback-Leibler divergence loss between two input vectors. The kernel takes two float32 tensors as input: 'pred' with shape (4096,) and 'target' with shape (4096,). It outputs a single float32 scalar value. For each element in the vectors, compute the loss as target[i] multiplied by (log(target[i]) - pred[i]). Sum these values across all elements using atomic addition, then divide the final result by 4096 to obtain the mean loss. Ensure thread safety during accumulation.\n\nInput:\npred: float32, shape = (4096,)\ntarget: float32, shape = (4096,)\n\nOutput:\noutput: float32, shape = ()\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 172, "task_name": "Log_Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Softmax\n\nTask Description:\nImplement a CUDA kernel named 'Log_Softmax' that computes the logarithmic softmax function for a 2D input tensor of shape (4096, 128) with float32 data type. For each row in the input tensor, the kernel must independently compute the log softmax values. The computation must follow this formula for each element in a row: subtract the row's maximum value and then subtract the natural logarithm of the sum of exponentials of all elements in that row (after subtracting the row's maximum for numerical stability). The output tensor must have the same shape and data type as the input. The kernel must process all 4096 rows in parallel while ensuring numerical stability.\n\nInput:\ninput: float32, shape = (4096, 128)\n\nOutput:\noutput: float32, shape = (4096, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 139, "task_name": "Hard_Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Swish\n\nTask Description:\nImplement the Hard_Swish activation function for a 2D input tensor of shape (16, 65536) with float32 data type. The output tensor must have the same shape and data type. For each element in the input, apply the Hard Swish function: if the element is less than or equal to -3, output 0; if the element is greater than or equal to 3, output the element itself; for elements between -3 and 3, output the element multiplied by (element + 3) divided by 6. The computation must be performed independently for each element without altering the input.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 166, "task_name": "Log_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Sigmoid\n\nTask Description:\nImplement a CUDA kernel to compute the log-sigmoid function element-wise on a 2D input tensor. The input tensor has dimensions 16x1024 with float32 data type. The output tensor must have the same dimensions and data type. For each element x in the input, compute the log-sigmoid value as x - log(1 + exp(x)), ensuring numerical stability.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 167, "task_name": "Log_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Sigmoid\n\nTask Description:\nImplement the Log_Sigmoid kernel. The kernel takes a single input tensor of shape (16, 4096) with float32 elements and produces an output tensor of identical shape and data type. For each element in the input tensor, compute the logarithm of the sigmoid function: log(1/(1 + exp(-x))), which simplifies to x - log(1 + exp(x)). The computation must be performed element-wise and independently for each position in the tensor.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 161, "task_name": "Leaky_ReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLeaky_ReLU\n\nTask Description:\nImplement a CUDA kernel for the Leaky ReLU activation function. The input is a 16x1024 tensor of float32 values. For each element in the input tensor, if the value is positive, output the same value; if negative, output the value multiplied by 0.01. The output tensor must have the same shape (16x1024) and data type (float32) as the input. The kernel must process all 16384 elements using the fixed negative slope of 0.01.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 183, "task_name": "Mish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMish\n\nTask Description:\nImplement the Mish activation function for a 2D tensor on GPU. The input is a float32 tensor of shape (16, 16384), and the output should have the same shape and data type. For each element x, compute Mish(x) = x * tanh(softplus(x)), where softplus(x) = ln(1 + exp(x)). Optimize the softplus calculation: when x * beta > threshold (with beta=1.0 and threshold=20.0), use x directly instead of the full softplus calculation to prevent overflow. Each element must be processed independently.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 143, "task_name": "Hard_Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Tanh\n\nTask Description:\nImplement the Hard_Tanh function for a 2D input tensor of shape (16, 16384) with float32 data type. The Hard_Tanh function is defined as: for each element, if the value is less than -1.0, set it to -1.0; if the value is greater than 1.0, set it to 1.0; otherwise, leave the value unchanged. The output tensor must have the same shape and data type as the input.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 131, "task_name": "Hard_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Sigmoid\n\nTask Description:\nImplement the Hard_Sigmoid activation function on a GPU. The input is a 2D tensor of 16 rows and 1024 columns with float32 data type. The output is a tensor of the same shape and data type. For each element in the input tensor, compute the output element as follows: if the input element is less than or equal to -3.0, set the output to 0.0; if the input element is greater than or equal to 3.0, set the output to 1.0; otherwise, set the output to the input element divided by 6.0 plus 0.5. The computation must be performed independently for each element.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 175, "task_name": "Log_Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Softmax\n\nTask Description:\nImplement a CUDA kernel for the Log_Softmax operation. The kernel should process a 2D input tensor of float32 values with dimensions (262144, 128). For each row in the tensor, compute the log softmax value for every element in that row. The log softmax for an element x_i is calculated as x_i minus the maximum value in its row, minus the natural logarithm of the sum of exponentials of all elements in the row after subtracting the row maximum. The output tensor must have identical dimensions and data type as the input. The kernel must handle large batch sizes efficiently and maintain numerical stability.\n\nInput:\ninput: float32, shape = (262144, 128)\n\nOutput:\noutput: float32, shape = (262144, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 157, "task_name": "L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nL1_Loss\n\nTask Description:\nImplement the L1_Loss kernel. Given two input tensors 'input' and 'target', both of shape (16, 4096) and data type float32, compute the mean absolute error (L1 loss) between them. The output is a single scalar (shape (1,)) of type float32. The L1 loss is computed by taking the absolute difference of each element in the input and target, summing these absolute differences, and then dividing by the total number of elements (16 * 4096).\n\nInput:\ninput: float32, shape = (16, 4096)\ntarget: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 163, "task_name": "Leaky_ReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLeaky_ReLU\n\nTask Description:\nImplement a CUDA kernel for the Leaky ReLU activation function. The kernel should process a single input tensor of shape (16, 16384) with float32 data type and produce a single output tensor of shape (262144,) with float32 data type. For each element in the input tensor, apply the Leaky ReLU function defined as: output = max(0, x) + negative_slope * min(0, x), where negative_slope is fixed at 0.01. The kernel must handle all elements independently and preserve the flattened layout of the output.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 165, "task_name": "Leaky_ReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLeaky_ReLU\n\nTask Description:\nImplement a CUDA kernel for the Leaky_ReLU activation function. The kernel takes a 2D input tensor of shape (16, 262144) with float32 values. For each element in the input, compute the output as: if the element is greater than 0, keep it unchanged; otherwise, multiply it by 0.01. The output tensor must have the same shape and data type as the input. All elements must be processed independently.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 100, "task_name": "Binary_Cross_Entropy_With_Logits_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBinary_Cross_Entropy_With_Logits_Loss\n\nTask Description:\nImplement the Binary Cross Entropy With Logits Loss kernel. The kernel should compute the loss between prediction and target tensors. Inputs are two float32 tensors: 'pred' with shape (262144,) and 'target' with shape (262144,). The output is a single float32 value representing the mean loss. The loss for each element is calculated as: -[target * (pred - log(1 + exp(pred))) + (1 - target) * (-log(1 + exp(pred)))]. All individual losses must be summed atomically, and the final output should be divided by 262144 to compute the mean loss.\n\nInput:\npred: float32, shape = (262144,)\ntarget: float32, shape = (262144,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 181, "task_name": "Mish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMish\n\nTask Description:\nImplement the Mish activation function kernel. The input is a 16x1024 tensor of 32-bit floats. The output must be a 16x1024 tensor of 32-bit floats where each element is computed as Mish(x) = x * tanh(softplus(x)). For numerical stability, when x > 20.0, use softplus(x) ≈ x; otherwise, compute softplus(x) as (1/β) * ln(1 + exp(β*x)) with β=1.0. Each element must be processed independently.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 177, "task_name": "Max_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Pooling_3D\n\nTask Description:\nImplement a 3D max pooling operation on a GPU using CUDA. The input tensor 'pool_input' has a shape of (16, 32, 24, 24, 24) and data type float32. The output tensor 'pool_output' should have a shape of (16, 32, 10, 10, 10) and data type float32. The pooling operation uses a kernel size of 3, stride of 2, padding of 1, and dilation of 3. The kernel must slide over the input tensor's spatial dimensions (depth, height, width), compute the maximum value within each sliding window while respecting dilation and padding, and write the result to the corresponding output position. Boundary checks must ensure indices stay within input tensor limits.\n\nInput:\npool_input: float32, shape = (16, 32, 24, 24, 24)\n\nOutput:\npool_output: float32, shape = (16, 32, 10, 10, 10)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 169, "task_name": "Log_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Sigmoid\n\nTask Description:\nImplement the Log_Sigmoid kernel. The kernel should compute the log-sigmoid function for each element in the input tensor. The input is a 2D tensor of shape (16, 65536) with float32 data type. The output should be a 1D tensor of shape (1048576) with float32 data type, containing the flattened results. The log-sigmoid function for an element x is defined as: x - log(1 + exp(x)). The kernel must perform an element-wise operation, meaning each output element is computed solely from the corresponding input element.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 109, "task_name": "Cross_Entropy_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCross_Entropy_Loss\n\nTask Description:\nImplement a CUDA kernel to compute the cross entropy loss. The inputs are a 2D tensor 'pred' of float32 values with shape (65536, 128) representing predicted logits, and a 1D tensor 'target' of int64 values with shape (65536,) representing ground truth class indices. The output is a scalar float32 tensor of shape (1,) containing the average cross entropy loss. The kernel must compute the loss by applying a numerically stable softmax to each row of 'pred', taking the negative log probability of the target class, summing these values across all rows, and dividing by the batch size (65536).\n\nInput:\npred: float32, shape = (65536, 128)\ntarget: int64, shape = (65536,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 151, "task_name": "KL_Div_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKL_Div_Loss\n\nTask Description:\nCompute the KL divergence loss kernel. The inputs are two one-dimensional tensors of 1024 float32 values each: pred (predicted values) and target (target values). The output is a single float32 value. For each element, compute the loss as target[i] * (log(target[i]) - pred[i]). The kernel must accumulate these computed loss values into a single output using atomic operations to avoid race conditions. The kernel must handle all 1024 elements.\n\nInput:\npred: float32, shape = (1024,)\ntarget: float32, shape = (1024,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 154, "task_name": "KL_Div_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKL_Div_Loss\n\nTask Description:\nImplement a CUDA kernel for the KL divergence loss computation. The kernel takes two input vectors: 'pred' (65536 float32 elements) and 'target' (65536 float32 elements). The output is a single float32 scalar. For each element, compute target[i] × (log(target[i]) - pred[i]), then atomically add this value to the output sum. The kernel must check thread indices to avoid out-of-bounds access. The final output represents the sum of all element-wise losses, which will be averaged by host code.\n\nInput:\npred: float32, shape = (65536,)\ntarget: float32, shape = (65536,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 162, "task_name": "Leaky_ReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLeaky_ReLU\n\nTask Description:\nImplement the Leaky ReLU activation function for a 2D input tensor of shape (16, 4096) and dtype float32. The output should be a 1D tensor of 65536 elements with dtype float32. For each element in the input, if the element is positive, the output is the same as the input; if the element is negative or zero, the output is the input multiplied by 0.01. The computation must be performed independently for each element.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 168, "task_name": "Log_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Sigmoid\n\nTask Description:\nImplement the Log_Sigmoid kernel. Given a 2D input tensor of shape (16, 16384) with float32 values, compute an output tensor of identical shape and data type. For each element x in the input, compute the Log Sigmoid function: f(x) = x - log(1 + exp(x)). The kernel must perform this computation element-wise across the entire tensor without altering dimensions or data types.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 187, "task_name": "NLL_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNLL_Loss\n\nTask Description:\nImplement the NLL_Loss kernel to compute the negative log likelihood loss. The inputs are a 2D tensor 'pred' of float32 values with shape (4096, 128) and a 1D tensor 'target' of int64 values with shape (4096,). The output is a scalar float32 tensor with shape (1,) representing the average loss over the batch. For each sample, the loss is computed as the negative value of the predicted log probability at the target index. The kernel must accumulate these losses atomically and the host will later divide the sum by the batch size of 4096 to compute the mean loss.\n\nInput:\npred: float32, shape = (4096, 128)\ntarget: int64, shape = (4096,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 144, "task_name": "Hard_Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHard_Tanh\n\nTask Description:\nTask: Hard_Tanh. Apply the Hard Tanh activation function element-wise to an input tensor of shape (16, 65536) and dtype float32. The output is a tensor of shape (1048576,) and dtype float32. The function clamps each input value to the range [-1.0, 1.0]: if the value is less than -1.0, set it to -1.0; if greater than 1.0, set it to 1.0; otherwise, leave it unchanged. Each element is processed independently.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 174, "task_name": "Log_Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Softmax\n\nTask Description:\nImplement the Log_Softmax kernel for a 2D input tensor of shape (65536, 128) with float32 elements. The kernel must independently process each row (65536 rows) to compute Log_Softmax values: for each element x in a row, output = x - max_val - log(sum(exp(x_i - max_val))) where max_val is the row's maximum value. Use numerically stable computation by tracking max values during exponent summation. Output must match input shape and dtype.\n\nInput:\ninput: float32, shape = (65536, 128)\n\nOutput:\noutput: float32, shape = (65536, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 176, "task_name": "Max_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Pooling_3D\n\nTask Description:\nImplement the Max_Pooling_3D kernel for 5D input tensors with shape (batch, channels, depth, height, width). The input tensor has dimensions (16, 32, 16, 16, 16) and float32 data type. The output tensor must have dimensions (16, 32, 6, 6, 6) and float32 data type. The kernel must apply max pooling with a 3x3x3 kernel size, stride of 2, padding of 1, and dilation of 3 across all spatial dimensions. For each output position, compute the maximum value within the dilated kernel window in the corresponding input region, respecting boundary conditions where out-of-bounds positions should be ignored.\n\nInput:\npool_input: float32, shape = (16, 32, 16, 16, 16)\n\nOutput:\npool_output: float32, shape = (16, 32, 6, 6, 6)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 188, "task_name": "NLL_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNLL_Loss\n\nTask Description:\nTask: NLL_Loss. Compute the mean negative log likelihood loss for a batch of predictions and targets. The input 'pred' is a float32 tensor of shape (16384, 128) representing log probabilities for each sample and class. The input 'target' is an int64 tensor of shape (16384,) representing the true class indices for each sample. The output is a float32 tensor of shape (1,) containing the mean loss. Constraints: Target indices must be within [0, 127], and the kernel must accumulate losses atomically due to concurrent writes.\n\nInput:\npred: float32, shape = (16384, 128)\ntarget: int64, shape = (16384,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 173, "task_name": "Log_Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Softmax\n\nTask Description:\nTask: Log_Softmax. Implement a CUDA kernel that computes the log softmax of a 2D input tensor with shape (16384, 128) and float32 data type. The output tensor must have the same shape and data type. For each row in the input, compute the log softmax by: finding the row's maximum value, exponentiating each element after subtracting this maximum, summing these exponentiated values, taking the natural logarithm of this sum, and then subtracting both the maximum and this log-sum from each original element in the row. Ensure numerical stability and process each row independently.\n\nInput:\ninput: float32, shape = (16384, 128)\n\nOutput:\noutput: float32, shape = (16384, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 171, "task_name": "Log_Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Softmax\n\nTask Description:\nTask: Log_Softmax. The kernel must compute the Log Softmax function for each row of a 1024x128 input matrix of 32-bit floating point numbers. The Log Softmax for a row is computed as: for each element x_i in the row, output x_i minus the maximum value in the row minus the natural logarithm of the sum of exponentials of (each element in the row minus the maximum value). This ensures numerical stability. Each row is processed independently.\n\nInput:\ninput: float32, shape = (1024, 128)\n\nOutput:\noutput: float32, shape = (1024, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 179, "task_name": "Max_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Pooling_3D\n\nTask Description:\nTask: Max_Pooling_3D. Perform 3D max pooling on a 5D input tensor of shape (16, 32, 40, 40, 40) with float32 data type. The output tensor must have shape (16, 32, 18, 18, 18) and float32 data type. The pooling uses a kernel size of 3 in all spatial dimensions, stride of 2, padding of 1, and dilation of 3. For each output position, compute the maximum value from the corresponding dilated input window, skipping out-of-bound indices.\n\nInput:\npool_input: float32, shape = (16, 32, 40, 40, 40)\n\nOutput:\npool_output: float32, shape = (16, 32, 18, 18, 18)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 178, "task_name": "Max_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Pooling_3D\n\nTask Description:\nImplement a CUDA kernel for 3D max pooling. The input is a 5D tensor of shape (16, 32, 32, 32, 32) with float32 data type representing batches, channels, depth, height, and width. The output is a 5D tensor of shape (16, 32, 14, 14, 14) with float32 data type. The pooling uses a cubic kernel of size 3, stride 2, padding 1, and dilation 3. For each output position, compute the maximum value within the corresponding dilated input window. Boundary conditions must be handled correctly by ignoring out-of-bound window positions.\n\nInput:\npool_input: float32, shape = (16, 32, 32, 32, 32)\n\nOutput:\npool_output: float32, shape = (16, 32, 14, 14, 14)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 170, "task_name": "Log_Sigmoid", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLog_Sigmoid\n\nTask Description:\nImplement the Log_Sigmoid kernel. The kernel takes a 2D input tensor of shape (16, 262144) with float32 values. For each element in the input, compute the Log Sigmoid function: LogSigmoid(x) = x - log(1 + exp(x)). The output tensor must have the same shape (16, 262144) and data type (float32) as the input. Each element in the output is computed independently from the corresponding element in the input.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 180, "task_name": "Max_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Pooling_3D\n\nTask Description:\nTask: Max_Pooling_3D. Compute a 3D max pooling operation on a 5D input tensor of shape (16, 32, 48, 48, 48) and data type float32. The operation uses a kernel size of 3 in each spatial dimension, a stride of 2, padding of 1, and dilation of 3. The output tensor must have the shape (16, 32, 22, 22, 22) and data type float32. The kernel must respect the boundaries of the input tensor by skipping out-of-bound window positions. For each window, the maximum value of the elements (accounting for dilation) must be computed and placed in the corresponding output location.\n\nInput:\npool_input: float32, shape = (16, 32, 48, 48, 48)\n\nOutput:\npool_output: float32, shape = (16, 32, 22, 22, 22)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 193, "task_name": "Pixel_Shuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Shuffle\n\nTask Description:\nImplement the Pixel_Shuffle kernel for GPU using CUDA. The kernel should reorganize an input tensor with shape (8, 8, 128, 128) [batch, channels, height, width] into an output tensor with shape (8, 2, 256, 256). The input uses float32 data type. The kernel must rearrange elements such that the spatial dimensions (height and width) increase by a factor of 2, while reducing the channel dimension by a factor of 4. This requires mapping each output element to a specific input element based on channel and spatial position transformations.\n\nInput:\ninput: float32, shape = (8, 8, 128, 128)\n\nOutput:\noutput: float32, shape = (8, 2, 256, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 190, "task_name": "NLL_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNLL_Loss\n\nTask Description:\nImplement a CUDA kernel named NLL_Loss that computes the negative log likelihood loss. The kernel should take two inputs: a 2D tensor 'pred' of float32 values with shape (262144, 128) representing predicted log probabilities, and a 1D tensor 'target' of int64 values with shape (262144) representing target class indices. The output is a scalar tensor of float32 with shape (1) containing the mean loss. The kernel must compute the loss for each sample by selecting the predicted value at the target index, negate it, and accumulate these values atomically. After accumulation, the sum must be divided by the batch size (262144) to obtain the mean loss.\n\nInput:\npred: float32, shape = (262144, 128)\ntarget: int64, shape = (262144,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 205, "task_name": "ReLU6", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU6\n\nTask Description:\nImplement the ReLU6 kernel for a 2D input tensor of shape (16, 262144) with float32 data type. The kernel must apply the ReLU6 activation function element-wise: for each input value x, compute min(max(x, 0), 6). The output tensor must have the same shape and data type as the input. The kernel must handle all elements independently and preserve the input dimensions.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 194, "task_name": "Pixel_Shuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Shuffle\n\nTask Description:\nImplement a Pixel_Shuffle CUDA kernel that rearranges input tensor elements to increase spatial resolution while reducing channel depth. The input is a 4D float32 tensor of shape (8, 8, 256, 256). The output must be a float32 tensor of shape (8, 2, 512, 512). Each output element at (b, c, h, w) must map to input element (b, c*(r²) + (h%r)*r + (w%r), h/r, w/r) where r=2 is the upscale factor. Batch dimension remains unchanged.\n\nInput:\ninput: float32, shape = (8, 8, 256, 256)\n\nOutput:\noutput: float32, shape = (8, 2, 512, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 189, "task_name": "NLL_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNLL_Loss\n\nTask Description:\nImplement the NLL_Loss kernel to compute the average negative log likelihood loss for a batch of predictions and targets. The input 'pred' is a 2D float32 tensor of shape (65536, 128) representing predicted log probabilities for each class. The input 'target' is a 1D int64 tensor of shape (65536,) containing true class indices. The output is a scalar float32 tensor of shape (1,) holding the average loss. The kernel must compute the loss for each sample by taking the negative value of the predicted log probability at the target index, sum these losses across all samples, and divide by the batch size 65536 to obtain the final output. Atomic operations should be used for thread-safe accumulation.\n\nInput:\npred: float32, shape = (65536, 128)\ntarget: int64, shape = (65536,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 184, "task_name": "Mish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMish\n\nTask Description:\nImplement the Mish activation function on a GPU using CUDA. The Mish function is defined as Mish(x) = x * tanh(softplus(x)), where softplus(x) = ln(1 + exp(x)). The input is a 2D tensor of shape (16, 65536) with float32 data type. The output should be a 1D tensor of shape (1048576,) with float32 data type, containing the Mish activation applied element-wise to the input. For numerical stability, if x is greater than 20.0, approximate softplus(x) as x; otherwise, compute it as (1.0 / beta) * log1pf(exp(beta * x)) with beta=1.0.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 192, "task_name": "Pixel_Shuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Shuffle\n\nTask Description:\nImplement the Pixel_Shuffle kernel. The kernel should reorganize a 4D input tensor with shape (batch, channels, height, width) into a higher-resolution output tensor by rearranging channel data into spatial blocks. Input: float32 tensor with shape (8, 8, 64, 64). Output: float32 tensor with shape (8, 2, 128, 128). Constraints: The kernel must use an upscale factor of 2, meaning each 2x2 block in the input's channel dimension becomes spatial pixels in the output. The channel dimension must be divisible by 4 (upscale factor squared).\n\nInput:\ninput: float32, shape = (8, 8, 64, 64)\n\nOutput:\noutput: float32, shape = (8, 2, 128, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 199, "task_name": "Pixel_Unshuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Unshuffle\n\nTask Description:\nImplement the Pixel_Unshuffle CUDA kernel. The input is a 4D tensor of shape (8, 2, 512, 512) with float32 data type. The kernel should rearrange spatial elements into new channels, reducing spatial dimensions by a factor of 2. Output must be a float32 tensor of shape (8, 8, 256, 256). Each output element at (b, c, h, w) maps to input at (b, c_in, h*2 + block_idx//2, w*2 + block_idx%2), where block_idx = c % 4 and c_in = c // 4.\n\nInput:\ninput: float32, shape = (8, 2, 512, 512)\n\nOutput:\noutput: float32, shape = (8, 8, 256, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 195, "task_name": "Pixel_Shuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Shuffle\n\nTask Description:\nImplement a CUDA kernel for the Pixel_Shuffle operation. The input is a 4D tensor of shape (8, 8, 512, 512) with float32 data type. The output should be a 4D tensor of shape (8, 2, 1024, 1024) with float32 data type. The kernel must spatially rearrange elements from the input tensor to the output tensor using an upscale factor of 2. Specifically, each element in the output tensor must be mapped from the input tensor such that the channel dimension is reduced by a factor of 4 (since 2×2=4) while the height and width dimensions are each increased by a factor of 2. The kernel must preserve all input values without modification and ensure correct indexing for this transformation.\n\nInput:\ninput: float32, shape = (8, 8, 512, 512)\n\nOutput:\noutput: float32, shape = (8, 2, 1024, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 204, "task_name": "ReLU6", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU6\n\nTask Description:\nImplement a CUDA kernel for the ReLU6 operation. The kernel should process a 2D input tensor of shape (16, 65536) with float32 data type. For each element, apply the ReLU6 function: clamp the value between 0 and 6 (output = min(max(input, 0), 6)). The output tensor must match the input shape (16, 65536) with float32 data type. The computation must be performed element-wise without altering the tensor dimensions.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 191, "task_name": "Pixel_Shuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Shuffle\n\nTask Description:\nImplement the Pixel Shuffle operation. Given an input tensor of shape (8, 8, 32, 32) with float32 values, reorganize its elements to produce an output tensor of shape (8, 2, 64, 64). The operation must rearrange channel information into spatial dimensions using an upscaling factor of 2. Input channels must be divisible by 4 (upscale factor squared), and each output pixel must be mapped from a corresponding input channel group based on spatial position.\n\nInput:\ninput: float32, shape = (8, 8, 32, 32)\n\nOutput:\noutput: float32, shape = (8, 2, 64, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 196, "task_name": "Pixel_Unshuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Unshuffle\n\nTask Description:\nImplement a CUDA kernel for the Pixel Unshuffle operation. The input is a 4D tensor with shape (8, 2, 64, 64) and float32 data type. The output should be a 4D tensor with shape (8, 8, 32, 32) and float32 data type. The kernel must rearrange the input by dividing each spatial dimension into non-overlapping 2x2 blocks, then flattening and moving these blocks to the channel dimension. This increases the channel count by a factor of 4 (from 2 to 8) while reducing spatial dimensions from 64x64 to 32x32. The batch dimension remains unchanged.\n\nInput:\ninput: float32, shape = (8, 2, 64, 64)\n\nOutput:\noutput: float32, shape = (8, 8, 32, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 208, "task_name": "SeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSeLU\n\nTask Description:\nImplement the SeLU activation function for a GPU using CUDA. The task requires applying the SeLU function element-wise to an input tensor of shape (16, 16384) with float32 data type. The output tensor must match the input shape and data type. The SeLU function is defined as: scale * (max(0, x) + min(0, alpha * (exp(x) - 1))), where alpha is approximately 1.673263 and scale is approximately 1.050701. The kernel must process all elements independently and preserve tensor dimensions.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 214, "task_name": "SiLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSiLU\n\nTask Description:\nImplement the SiLU (Sigmoid-weighted Linear Unit) activation function for a GPU using CUDA. The kernel should compute the element-wise transformation defined as SiLU(x) = x / (1 + exp(-x)). The input is a 2D tensor of shape (16, 65536) with float32 values, and the output must be a tensor of identical shape and data type. Each element must be processed independently without dependencies on other elements.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 209, "task_name": "SeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSeLU\n\nTask Description:\nImplement the SeLU activation function kernel. The input is a 2D tensor of shape (16, 65536) with float32 elements. The output must be a tensor of identical shape and data type. The SeLU function is defined as: scale * (max(0, x) + min(0, alpha * (exp(x) - 1))) where alpha = 1.6732632423543772 and scale = 1.0507009873554805. The kernel must apply this operation element-wise to all input values.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 211, "task_name": "SiLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSiLU\n\nTask Description:\nImplement the SiLU activation function kernel. The input is a 2D tensor with shape (16, 1024) of float32 values. The output tensor must have the same shape and dtype as the input. For each element x in the input, compute the SiLU function: output = x / (1 + exp(-x)). The kernel must process all 16384 elements independently.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 185, "task_name": "Mish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMish\n\nTask Description:\nTask: Mish. Compute the Mish activation function for each element in a 2D float32 tensor of shape (16, 262144). The Mish function is defined as Mish(x) = x * tanh(softplus(x)), where softplus(x) is computed as: if x > 20.0 then softplus(x) = x, else softplus(x) = log(1 + exp(x)). The output tensor must have the same shape and data type as the input.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 217, "task_name": "Smooth_L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSmooth_L1_Loss\n\nTask Description:\nImplement the Smooth L1 Loss kernel. The kernel should compute the mean Smooth L1 Loss between two input tensors of shape (16, 4096) with float32 data types. The loss for each element is defined as: if the absolute difference between input and target is less than beta (1.0), use 0.5 * (difference)^2 / beta; otherwise, use |difference| - 0.5 * beta. Sum all element losses and divide by the total number of elements (16*4096) to produce a single float32 output. Use atomic operations for accumulation.\n\nInput:\ninput: float32, shape = (16, 4096)\ntarget: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 201, "task_name": "ReLU6", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU6\n\nTask Description:\nReLU6: Implement a CUDA kernel that applies the ReLU6 activation function to a 2D input tensor. The input is a float32 tensor of shape (16, 1024). For each element, compute min(max(x, 0), 6) where x is the input value. The output must be a float32 tensor of identical shape (16, 1024). The kernel must perform this operation element-wise with no cross-element dependencies.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 198, "task_name": "Pixel_Unshuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Unshuffle\n\nTask Description:\nImplement the Pixel_Unshuffle CUDA kernel. The operation reorganizes spatial data into channel data by downscaling resolution. Input is a 4D tensor with shape (8, 2, 256, 256) of float32 values representing batch, channel, height, and width dimensions. Output must be a 4D tensor with shape (8, 8, 128, 128) of float32 values. The downscale factor is 2, meaning input spatial dimensions (256x256) must be reduced by factor 2 in both height and width while increasing channel dimension by factor 4. Each output element must correspond to specific spatial positions in non-overlapping 2x2 input blocks.\n\nInput:\ninput: float32, shape = (8, 2, 256, 256)\n\nOutput:\noutput: float32, shape = (8, 8, 128, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 206, "task_name": "SeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSeLU\n\nTask Description:\nImplement a CUDA kernel named SeLU that computes the Scaled Exponential Linear Unit activation function. The input is a 2D tensor of shape (16, 1024) with float32 data type. The output must be a tensor of identical shape and data type. For each element, the kernel must compute: if the value is positive, multiply it by a constant scale (≈1.0507); if negative, apply alpha (≈1.6733) multiplied by (exponential of the value minus 1), then multiply by scale. The computation must be performed element-wise without altering the tensor dimensions.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 226, "task_name": "Soft_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Shrink\n\nTask Description:\nImplement the Soft_Shrink kernel. For each element in a 16x1024 float32 input tensor, apply the SoftShrink function with lambda=0.5: output = (input - 0.5) if input > 0.5, (input + 0.5) if input < -0.5, and 0 otherwise. Output must be a 16x1024 float32 tensor with results computed independently for each element.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 202, "task_name": "ReLU6", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU6\n\nTask Description:\nTask: ReLU6. Given an input tensor of shape (16, 4096) with float32 data type, compute an output tensor of the same shape and data type where each element is the ReLU6 activation of the corresponding input element. The ReLU6 function is defined as: for each element x, output = min(max(x, 0), 6). The kernel must process each element independently and handle the entire input tensor of 65536 elements.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 182, "task_name": "Mish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMish\n\nTask Description:\nTask: Mish. Compute the Mish activation function for each element in a 2D input tensor of shape (16, 4096) and data type float32. The Mish function is defined as Mish(x) = x * tanh(softplus(x)), where softplus(x) = ln(1 + exp(x)). For numerical stability, if x > 20.0, set softplus(x) = x to avoid overflow; otherwise, compute softplus(x) = log(1 + exp(x)). The output must have the same shape and data type as the input.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 146, "task_name": "Huber_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nHuber_Loss\n\nTask Description:\nImplement the Huber_Loss kernel. The kernel computes the Huber loss between two input tensors: 'input' and 'target', both of shape (16, 1024) and float32 data type. The output is a single float32 value representing the mean loss. The Huber loss for each element is defined as: if the absolute difference between input and target is less than 2.0, use 0.5 * (difference)^2; otherwise, use 2.0 * (absolute difference - 1.0). The kernel must compute the sum of losses across all elements and then divide by the total number of elements (16384) to obtain the mean loss.\n\nInput:\ninput: float32, shape = (16, 1024)\ntarget: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 158, "task_name": "L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nL1_Loss\n\nTask Description:\nImplement a CUDA kernel for the L1_Loss task. The kernel should compute the mean absolute error (L1 loss) between two input tensors. Both input tensors have dimensions 16x16384 and data type float32. The output should be a single scalar value of data type float32. The computation must calculate the absolute difference for each corresponding element pair across both tensors, sum all absolute differences, and then divide the total sum by the number of elements (16*16384) to produce the mean loss.\n\nInput:\ninput: float32, shape = (16, 16384)\ntarget: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 224, "task_name": "Soft_Plus", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Plus\n\nTask Description:\nImplement the Soft_Plus kernel for GPU computation. The kernel should compute the SoftPlus function for each element in a 16x65536 input tensor of 32-bit floating-point values. The SoftPlus function is defined as: when beta*x exceeds the threshold (20.0), output x; otherwise, output (1/beta) * log(1 + exp(beta*x)), where beta is 1.0. The output must be a tensor of identical shape and data type as the input.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 230, "task_name": "Soft_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Shrink\n\nTask Description:\nImplement the Soft_Shrink kernel for a 2D tensor of shape (16, 262144) with float32 data type. For each element in the input tensor, apply the Soft Shrink function: if the element value exceeds 0.5, subtract 0.5; if less than -0.5, add 0.5; otherwise output zero. The lambda parameter is fixed at 0.5, and each element must be processed independently.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 186, "task_name": "NLL_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNLL_Loss\n\nTask Description:\nImplement the NLL_Loss CUDA kernel. The kernel takes two inputs: a 2D float32 tensor 'pred' of shape (1024, 128) representing predicted log probabilities for each class, and a 1D int64 tensor 'target' of shape (1024,) representing the index of the true class for each sample. The kernel must compute the negative log likelihood loss for each sample: for each sample i, the loss is the negative of the predicted log probability at the class index specified by target[i]. The kernel must accumulate the sum of these losses into a single float32 output tensor of shape (1,). The kernel should be launched with enough threads to cover the entire batch (1024 samples), and atomic operations must be used to safely accumulate the loss sum due to concurrent writes to the single output value.\n\nInput:\npred: float32, shape = (1024, 128)\ntarget: int64, shape = (1024,)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 212, "task_name": "SiLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSiLU\n\nTask Description:\nImplement a CUDA kernel for the SiLU task that computes the SiLU activation function on a 16x4096 float32 input tensor. The SiLU function is defined as x * sigmoid(x), equivalent to x / (1 + exp(-x)), and must be applied element-wise. The output should be a float32 tensor of the same shape, where each element depends solely on its corresponding input element.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 234, "task_name": "LayerNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLayerNorm\n\nTask Description:\nImplement a LayerNorm kernel for GPU using CUDA. The kernel processes a 4D input tensor of shape (16, 4, 512, 512) with float32 values. For each spatial position (height and width) across all batches, independently compute the mean and variance across the feature dimension (size 4). Normalize each feature value by subtracting the mean and dividing by the square root of the variance plus a small epsilon (1e-5) to prevent division by zero. The output tensor must have identical shape and data type as the input. Ensure thread synchronization when sharing computed mean and variance values across threads processing the same spatial position.\n\nInput:\ninput: float32, shape = (16, 4, 512, 512)\n\nOutput:\noutput: float32, shape = (16, 4, 512, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 218, "task_name": "Smooth_L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSmooth_L1_Loss\n\nTask Description:\nImplement a CUDA kernel for the Smooth L1 Loss computation. The task involves calculating the mean Smooth L1 Loss between two input tensors: 'input' and 'target', both of shape (16, 16384) and float32 data type. The loss for each element is defined as: if the absolute difference between input and target is less than 1.0, compute 0.5 * (difference squared) / 1.0; otherwise, compute the absolute difference minus 0.5. The kernel must atomically accumulate these losses into a single output scalar of shape (1,) and float32 type, which will later be divided by the total number of elements (262144) to obtain the mean loss.\n\nInput:\ninput: float32, shape = (16, 16384)\ntarget: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 203, "task_name": "ReLU6", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReLU6\n\nTask Description:\nTask: ReLU6. Apply the ReLU6 activation function element-wise to a 2D input tensor of shape (16, 16384) and data type float32. The ReLU6 function is defined as: for each element x, output = min(max(x, 0), 6). The output tensor must have the same shape and data type as the input. The operation must be performed independently for each element.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 215, "task_name": "SiLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSiLU\n\nTask Description:\nTask: SiLU. Compute the SiLU function element-wise on an input tensor. The SiLU function is defined as SiLU(x) = x / (1 + exp(-x)). The input tensor has a shape of (16, 262144) and data type float32. The output tensor must have the same shape and data type, and the kernel should accurately compute the function for each element without altering the tensor structure.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 231, "task_name": "LayerNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLayerNorm\n\nTask Description:\nImplement a CUDA kernel for Layer Normalization. The kernel should normalize input tensor values across the feature dimension for each spatial location independently. Input is a 4D tensor of shape (16, 4, 64, 64) with float32 data type, representing batch size, features, height, and width dimensions. Output must have identical shape and data type. The kernel must compute mean and variance across the feature dimension for each spatial location, then normalize each value using (x - mean) / sqrt(variance + 1e-5). Shared memory should be used for mean and variance synchronization between threads.\n\nInput:\ninput: float32, shape = (16, 4, 64, 64)\n\nOutput:\noutput: float32, shape = (16, 4, 64, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 229, "task_name": "Soft_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Shrink\n\nTask Description:\nImplement the Soft_Shrink kernel for GPU computation. The kernel processes a 2D input tensor of shape (16, 65536) with float32 data type, applying the SoftShrink function element-wise. For each element x, if x > 0.5, output x - 0.5; if x < -0.5, output x + 0.5; otherwise output 0. The output must be a float32 tensor of identical shape (16, 65536), with each element transformed independently.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 200, "task_name": "Pixel_Unshuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Unshuffle\n\nTask Description:\nImplement the Pixel Unshuffle operation for a 4D tensor. The input tensor has a shape of (8, 2, 1024, 1024) and data type float32. The output tensor must have a shape of (8, 8, 512, 512) and data type float32. The operation must reorganize the input by taking non-overlapping 2x2 blocks in the spatial dimensions and moving each element of the block to consecutive channels in the output, thereby reducing the spatial dimensions by a factor of 2 and increasing the channel dimension by a factor of 4.\n\nInput:\ninput: float32, shape = (8, 2, 1024, 1024)\n\nOutput:\noutput: float32, shape = (8, 8, 512, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 213, "task_name": "SiLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSiLU\n\nTask Description:\nTask: SiLU. Apply the SiLU activation function element-wise to an input tensor of shape (16, 16384) and data type float32, producing an output tensor of shape (262144,) and data type float32. The SiLU function is defined as: output = x / (1 + exp(-x)) for each element x in the input. The kernel must process all 262144 elements without changing the order or dimensionality of the data.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 227, "task_name": "Soft_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Shrink\n\nTask Description:\nImplement the Soft_Shrink operation on a GPU using CUDA. The kernel should process a 2D input tensor of shape (16, 4096) with float32 data type. For each element, if the value is greater than 0.5, subtract 0.5 from it; if the value is less than -0.5, add 0.5 to it; otherwise, output 0. The output must be a tensor of identical shape and data type as the input. All operations must be element-wise and independent.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 235, "task_name": "LayerNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLayerNorm\n\nTask Description:\nImplement a LayerNorm CUDA kernel. The kernel should perform layer normalization across the feature dimension for a 4D input tensor with shape (16, 4, 1024, 1024) of float32 values. For each spatial position (height and width) in every batch element, compute the mean and variance across the 4 feature channels. Normalize each feature value using the formula (x - mean) / sqrt(variance + 1e-5). The output must have the same shape and dtype as the input. The kernel must handle batch and spatial dimensions independently.\n\nInput:\ninput: float32, shape = (16, 4, 1024, 1024)\n\nOutput:\noutput: float32, shape = (16, 4, 1024, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 232, "task_name": "LayerNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLayerNorm\n\nTask Description:\nImplement a CUDA kernel for Layer Normalization (LayerNorm). The kernel should normalize each spatial position across the feature dimension. The input is a 4D tensor with dimensions (batch_size, features, height, width) = (16, 4, 128, 128) and data type float32. The output must have the same shape and data type as the input. The kernel must compute the mean and variance across the feature dimension for each spatial position (height, width) and batch index, then apply normalization using the formula (x - mean) / sqrt(variance + epsilon), where epsilon is 1e-5. All operations must preserve the input's structure and maintain numerical stability.\n\nInput:\ninput: float32, shape = (16, 4, 128, 128)\n\nOutput:\noutput: float32, shape = (16, 4, 128, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 210, "task_name": "SeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSeLU\n\nTask Description:\nTask: SeLU. Compute the Scaled Exponential Linear Unit (SeLU) activation function on a 2D input tensor of shape (16, 262144) with float32 data type. For each element in the input tensor, if the element is positive, multiply it by a constant scale value; otherwise, apply the function scale * alpha * (exponential of the element minus 1). The constants alpha and scale are approximately 1.6732632423543772 and 1.0507009873554805, respectively. The output tensor must have the same shape and data type as the input, and each element must be processed independently.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 225, "task_name": "Soft_Plus", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Plus\n\nTask Description:\nTask: Soft_Plus. Implement a GPU kernel that computes the SoftPlus activation function element-wise on a 2D input tensor of shape (16, 262144) containing float32 values. The output must have identical shape and data type as the input. For each element x, if β*x exceeds 20.0 (with β=1.0), output x directly; otherwise compute (1/β)*log(1 + exp(β*x)). The kernel must handle all 4,194,304 elements independently.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 246, "task_name": "Nearest_Neighbor", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNearest_Neighbor\n\nTask Description:\nImplement the Nearest_Neighbor kernel. Given an input tensor `points` with shape (512, 3) containing float32 3D point coordinates, compute for each point the index of its nearest neighbor (excluding itself). The output tensor `indices_out` must have shape (512,) with int32 values representing neighbor indices. The kernel must find the closest point based on squared Euclidean distance (dx² + dy² + dz²) and ensure no point selects itself.\n\nInput:\npoints: float32, shape = (512, 3)\n\nOutput:\nindices_out: int32, shape = (512,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 207, "task_name": "SeLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSeLU\n\nTask Description:\nImplement the SeLU (Scaled Exponential Linear Unit) activation function for a 2D input tensor of shape (16, 4096) with float32 data type. The function is defined as: for each element x, if x is greater than 0, output = scale * x; otherwise, output = scale * alpha * (exp(x) - 1), where scale = 1.0507009873554805 and alpha = 1.6732632423543772. The output tensor must have the same shape as the input, and the computation must be applied element-wise to all 16*4096 elements independently.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 216, "task_name": "Smooth_L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSmooth_L1_Loss\n\nTask Description:\nImplement the Smooth L1 Loss kernel. The inputs are two 2D tensors named 'input' and 'target', both with shape (16, 1024) and data type float32. The output is a scalar tensor (shape (1,)) of float32. For each corresponding element in the input and target tensors, compute the absolute difference. If the absolute difference is less than 1.0, calculate the loss as 0.5 times the squared difference. Otherwise, calculate the loss as the absolute difference minus 0.5. The kernel must compute the mean of these losses over all elements, resulting in a single output value. The beta parameter is fixed at 1.0.\n\nInput:\ninput: float32, shape = (16, 1024)\ntarget: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 236, "task_name": "Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh\n\nTask Description:\nImplement a GPU kernel to compute the hyperbolic tangent function (tanh) for each element in a 2D input tensor. The input tensor has dimensions (16, 1024) with float32 data type. The output tensor must have identical dimensions (16, 1024) and float32 data type. Each output element should be the tanh of the corresponding input element, computed as tanh(x) = sinh(x)/cosh(x). The computation must be performed element-wise with no dependencies between different elements.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 243, "task_name": "Tanh_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh_Shrink\n\nTask Description:\nImplement a CUDA kernel for the Tanh_Shrink operation. The kernel should compute the Tanh Shrink function, defined as output = x - tanh(x), for each element in a 2D input tensor of shape (16, 16384) containing float32 values. The output tensor must have identical shape and data type. The kernel must process all 262144 elements independently without changing input dimensions.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 249, "task_name": "Nearest_Neighbor", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNearest_Neighbor\n\nTask Description:\nImplement a CUDA kernel named 'Nearest_Neighbor' that processes 4096 points in 3D space. Each point is represented by three float32 coordinates. For every point, find the index of its nearest neighbor (excluding itself) based on squared Euclidean distance. The output should be an int32 tensor of 4096 elements where each element represents the index of the nearest neighbor for the corresponding point. The kernel must skip self-comparisons and correctly handle floating-point comparisons.\n\nInput:\npoints: float32, shape = (4096, 3)\n\nOutput:\nindices_out: int32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 248, "task_name": "Nearest_Neighbor", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNearest_Neighbor\n\nTask Description:\nImplement the Nearest_Neighbor kernel to compute nearest neighbor indices for 3D points. The input is a 2048x3 float32 tensor representing point coordinates. The output should be a 2048-element int32 tensor where each element contains the index of the nearest neighbor for the corresponding point (excluding self). The kernel must compute squared Euclidean distances and select the closest neighbor index for each point.\n\nInput:\npoints: float32, shape = (2048, 3)\n\nOutput:\nindices_out: int32, shape = (2048,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 247, "task_name": "Nearest_Neighbor", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNearest_Neighbor\n\nTask Description:\nImplement the Nearest_Neighbor kernel. Given a 1024x3 float32 tensor representing 3D points, compute for each point the index of its closest neighbor (excluding itself) based on Euclidean distance squared. Output a 1024-length int32 tensor where each element is the index of the nearest neighbor to the corresponding point. The kernel must avoid self-comparisons and efficiently compute pairwise distances without square roots.\n\nInput:\npoints: float32, shape = (1024, 3)\n\nOutput:\nindices_out: int32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 252, "task_name": "Max_Subarray_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Subarray_Sum\n\nTask Description:\nImplement the Max_Subarray_Sum kernel to find the maximum sum of any contiguous subarray with a fixed window size of 128 elements in a 1D integer array. The input is a tensor of shape (4096,) with int32 values. The output should be a single int64 value representing the maximum sum found. Ensure the kernel handles array bounds correctly by skipping invalid window positions beyond index 4096 - 128. The computation must consider all possible contiguous windows of size 128 within the array.\n\nInput:\ninput: int32, shape = (4096,)\n\nOutput:\noutput: int64, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 221, "task_name": "Soft_Plus", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Plus\n\nTask Description:\nImplement the Soft_Plus kernel for CUDA. The kernel must apply the SoftPlus activation function to each element in a 16x1024 input tensor of float32 values. The output should be a float32 tensor of identical shape. The SoftPlus function is defined as SoftPlus(x) = (1/β) * log(1 + exp(β*x)), with β=1.0. For numerical stability, when β*x exceeds a threshold of 20.0, the output should default to x instead of computing the full formula.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 233, "task_name": "LayerNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLayerNorm\n\nTask Description:\nTask name: LayerNorm. Compute layer normalization for a 4D input tensor with shape (16, 4, 256, 256) and float32 data type. The kernel should normalize over the feature dimension (size 4) for each batch and each spatial position (height and width). Specifically, for each batch, height, and width index, compute the mean and variance of the features at that position, then apply the normalization formula: (input - mean) / sqrt(variance + epsilon), where epsilon is 1e-5 for numerical stability. The output must have the same shape (16, 4, 256, 256) and float32 data type as the input.\n\nInput:\ninput: float32, shape = (16, 4, 256, 256)\n\nOutput:\noutput: float32, shape = (16, 4, 256, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 244, "task_name": "Tanh_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh_Shrink\n\nTask Description:\nImplement the Tanh_Shrink CUDA kernel. The kernel should compute the Tanh Shrink function for each element in a 2D input tensor of shape (16, 65536) with float32 data type. The output tensor must have the same shape and data type, with each element calculated as the input value minus the hyperbolic tangent of the input value. The kernel must process all elements independently and efficiently on GPU.\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 237, "task_name": "Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh\n\nTask Description:\nImplement a CUDA kernel for the Tanh task. The kernel should compute the hyperbolic tangent function element-wise on a 2D input tensor of shape (16, 4096) with float32 data type. The output tensor must have identical shape and data type, where each element is tanh(input_element). All elements must be processed independently and maintain mathematical precision.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 241, "task_name": "Tanh_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh_Shrink\n\nTask Description:\nImplement a CUDA kernel for the Tanh_Shrink task. The kernel must compute the Tanh Shrink function element-wise for each value in a 2D input tensor of shape (16, 1024) and data type float32. The Tanh Shrink function is defined as output = input - tanh(input), where tanh is the hyperbolic tangent function applied to each input element. The output must be a tensor of the same shape (16, 1024) and float32 data type. Constraints include processing all elements of the input tensor without altering the shape, ensuring each output element depends only on the corresponding input element, and handling the data efficiently.\n\nInput:\ninput: float32, shape = (16, 1024)\n\nOutput:\noutput: float32, shape = (16, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 240, "task_name": "Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh\n\nTask Description:\nImplement a CUDA kernel to compute the hyperbolic tangent (Tanh) function. The kernel must process a 2D input tensor of shape (16, 262144) with float32 data type. The output tensor must have the same shape and data type as the input. Each element in the output tensor should be the Tanh of the corresponding element in the input tensor. The kernel must be thread-safe, with no dependencies between threads, and achieve numerical accuracy within a tolerance of 1e-5 compared to a reference implementation.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (16, 262144)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 222, "task_name": "Soft_Plus", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Plus\n\nTask Description:\nTask: Soft_Plus. Implement a CUDA kernel that computes the SoftPlus activation function element-wise on a 2D input tensor of shape (16, 4096) with float32 data type. The SoftPlus function is defined as: for each element x, if (beta * x) > threshold (with beta=1.0 and threshold=20.0), then output x; otherwise, output (1/beta) * log(1 + exp(beta * x)). The output must be a tensor of the same shape and type. The kernel must handle each element independently and use the threshold to avoid numerical overflow.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 223, "task_name": "Soft_Plus", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Plus\n\nTask Description:\nImplement a CUDA kernel for the Soft_Plus function. The input is a 2D tensor of float32 values with shape (16, 16384). The output should be a 1D tensor of float32 values with 262144 elements. For each element in the input, compute the SoftPlus activation: if the element multiplied by 1.0 exceeds 20.0, output the element directly; otherwise, apply the formula log(1 + exp(1.0 * element)). The kernel must process all elements independently.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 242, "task_name": "Tanh_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh_Shrink\n\nTask Description:\nImplement a CUDA kernel named Tanh_Shrink that computes the Tanh Shrink function for each element in a 2D input tensor. The input tensor has dimensions (16, 4096) and uses float32 data type. The output tensor must have identical shape (16, 4096) and data type. The Tanh Shrink function is defined as output = x - tanh(x), where x is an element from the input tensor. Each element must be processed independently with no data dependencies between computations.\n\nInput:\ninput: float32, shape = (16, 4096)\n\nOutput:\noutput: float32, shape = (16, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 228, "task_name": "Soft_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoft_Shrink\n\nTask Description:\nTask: Soft_Shrink. The kernel should apply the SoftShrink function element-wise to the input tensor. For each element x, if x is greater than 0.5, output x minus 0.5; if x is less than -0.5, output x plus 0.5; otherwise, output 0.0. The input is a tensor of shape (16, 16384) with float32 data type, and the output must have the same shape and data type. Constraints: the lambda parameter is fixed at 0.5, the computation must be independent for each element, and the output tensor must match the input dimensions exactly.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 255, "task_name": "Max_Subarray_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Subarray_Sum\n\nTask Description:\nMax_Subarray_Sum: Compute the maximum sum of any contiguous subarray of fixed length 1024 in an array of 32768 integers. Input is a single int32 tensor of shape (32768,). Output is a single int64 value representing the maximum subarray sum. The kernel must consider all possible contiguous windows of length 1024 within the array bounds.\n\nInput:\ninput: int32, shape = (32768,)\n\nOutput:\noutput: int64, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 239, "task_name": "Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh\n\nTask Description:\nTask: Tanh. Compute the element-wise hyperbolic tangent (tanh) function for a 2D input tensor of shape (16, 65536) containing 32-bit floating-point values. The output tensor must have the same shape and data type as the input, with each element transformed independently. The output values must be constrained to the range [-1, 1].\n\nInput:\ninput: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (16, 65536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 254, "task_name": "Max_Subarray_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Subarray_Sum\n\nTask Description:\nTask: Max_Subarray_Sum. Compute the maximum sum of any contiguous subarray of fixed length 512 from a one-dimensional input array of 16384 integers. The input array consists of int32 values. The output should be a single int64 value representing this maximum sum. The kernel must respect the window size of 512 and handle the array bounds correctly, ensuring that all possible window starting positions are considered. Parallel execution should use atomic operations to update the global maximum sum safely.\n\nInput:\ninput: int32, shape = (16384,)\n\nOutput:\noutput: int64, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 253, "task_name": "Max_Subarray_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Subarray_Sum\n\nTask Description:\nTask: Max_Subarray_Sum. Implement a CUDA kernel to compute the maximum sum of any contiguous subarray of length 256 in an input array of 8192 integers. The input tensor is a 1D array of int32 with shape (8192,). The output is a single int64 value representing the maximum sum. The kernel must handle all possible contiguous windows of length 256 within the array bounds.\n\nInput:\ninput: int32, shape = (8192,)\n\nOutput:\noutput: int64, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 219, "task_name": "Smooth_L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSmooth_L1_Loss\n\nTask Description:\nTask: Smooth_L1_Loss. Given two input tensors named 'input' and 'target', both of data type float32 and shape (16, 65536), compute a single output tensor of shape (1,) and data type float32 representing the mean smooth L1 loss over all elements. The smooth L1 loss for each element is defined with a beta parameter of 1.0. For each element, if the absolute difference between the input and target is less than beta, the loss is 0.5 times the square of the difference divided by beta. Otherwise, the loss is the absolute difference minus 0.5 times beta. The mean loss is computed by summing the losses of all elements and then dividing by the total number of elements (16 * 65536).\n\nInput:\ninput: float32, shape = (16, 65536)\ntarget: float32, shape = (16, 65536)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 262, "task_name": "Avg_Pooling_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_2D\n\nTask Description:\nTask name: Avg_Pooling_2D. Implement a CUDA kernel that computes 2D average pooling on a 4-dimensional input tensor of shape (16, 128, 128, 128) with float32 data type. The output tensor should have shape (16, 128, 64, 64) and float32 data type. The kernel must use a 2x2 pooling window with stride 2, sliding non-overlappingly across the height and width dimensions. For each window, compute the average of the four elements. Batch and channel dimensions should be preserved. The kernel must handle no-padding conditions and maintain data type consistency.\n\nInput:\navgpool2d_X: float32, shape = (16, 128, 128, 128)\n\nOutput:\navgpool2d_Y_out: float32, shape = (16, 128, 64, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 250, "task_name": "Nearest_Neighbor", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nNearest_Neighbor\n\nTask Description:\nTask: Nearest_Neighbor. Compute the nearest neighbor for each point in a 3D point cloud. Input is a float32 tensor of shape (8192, 3) representing 8192 points with XYZ coordinates. Output must be an int32 tensor of shape (8192,) where each element is the index of the nearest neighbor point for the corresponding input point, excluding itself. The nearest neighbor is determined by minimizing the squared Euclidean distance. Each point must have a distinct neighbor (no self-matches allowed).\n\nInput:\npoints: float32, shape = (8192, 3)\n\nOutput:\nindices_out: int32, shape = (8192,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 238, "task_name": "Tanh", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh\n\nTask Description:\nImplement a CUDA kernel for the Tanh function. The kernel should compute the hyperbolic tangent for each element of an input tensor with shape (16, 16384) and data type float32. The output tensor must have identical shape and data type. Each output element must be the tanh of the corresponding input element, satisfying the mathematical definition tanh(x) = (e^x - e^{-x}) / (e^x + e^{-x}). The kernel must process all 262,144 elements independently.\n\nInput:\ninput: float32, shape = (16, 16384)\n\nOutput:\noutput: float32, shape = (16, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 265, "task_name": "Avg_Pooling_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_2D\n\nTask Description:\nTask: Avg_Pooling_2D. Implement a CUDA kernel for 2D average pooling. The input is a 4D tensor named 'avgpool2d_X' with shape (64, 64, 128, 128) and float32 dtype. The output tensor 'avgpool2d_Y_out' must have shape (64, 64, 64, 64) and float32 dtype. The kernel should use a 2x2 pooling window with stride 2, computing the average of each non-overlapping window without padding. The input tensor dimensions represent batch, channels, height, and width respectively.\n\nInput:\navgpool2d_X: float32, shape = (64, 64, 128, 128)\n\nOutput:\navgpool2d_Y_out: float32, shape = (64, 64, 64, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 263, "task_name": "Avg_Pooling_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_2D\n\nTask Description:\nImplement the Avg_Pooling_2D kernel. Given a 4D input tensor of shape (32, 64, 128, 128) with float32 data type, compute average pooling using a 2x2 kernel with stride 2. For each non-overlapping 2x2 window in the spatial dimensions (height and width), calculate the average of the four elements. The output should be a 4D tensor of shape (32, 64, 64, 64) with float32 data type, where each element represents the average of its corresponding input window.\n\nInput:\navgpool2d_X: float32, shape = (32, 64, 128, 128)\n\nOutput:\navgpool2d_Y_out: float32, shape = (32, 64, 64, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 264, "task_name": "Avg_Pooling_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_2D\n\nTask Description:\nImplement the Avg_Pooling_2D CUDA kernel. The kernel should perform 2D average pooling on a 4D input tensor with shape (32, 128, 128, 128) of float32 values. The output tensor should have shape (32, 128, 64, 64) of float32 values. The pooling uses a 2x2 window with stride 2 in both height and width dimensions, covering non-overlapping regions. For each window, compute the average of all four elements and store it in the corresponding output position. The kernel must preserve batch and channel dimensions while reducing spatial dimensions by half.\n\nInput:\navgpool2d_X: float32, shape = (32, 128, 128, 128)\n\nOutput:\navgpool2d_Y_out: float32, shape = (32, 128, 64, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 261, "task_name": "Avg_Pooling_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_2D\n\nTask Description:\nTask: Avg_Pooling_2D. Perform 2D average pooling on an input tensor with shape (16, 64, 128, 128) and data type float32. The output should be a tensor of shape (16, 64, 64, 64) with float32 values. Use a fixed kernel size of 2x2 and stride of 2 in both height and width dimensions. The pooling must be applied independently for each batch and channel, computing the average of elements in each non-overlapping 2x2 window without any padding.\n\nInput:\navgpool2d_X: float32, shape = (16,64,128,128)\n\nOutput:\navgpool2d_Y_out: float32, shape = (16,64,64,64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 257, "task_name": "AdamW", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAdamW\n\nTask Description:\nImplement the AdamW kernel for updating parameters in a matrix using gradient, first moment, and second moment data. The input tensor matA is a float32 matrix of shape (1024, 3072) containing parameters and other values. The input tensor matB is a float32 matrix of shape (3072, 3072) storing gradients in its first third, first moments in the next third, and second moments in the final third. The output tensor matC_out is a float32 matrix of shape (1024, 3072) where only the first third of elements (in row-major order) are updated via AdamW. The kernel must use fixed hyperparameters: beta1=0.9, beta2=0.999, learning rate=0.0005, epsilon=1e-8, weight decay=0.01, and time step=5.0. Elements beyond the first third should remain unchanged.\n\nInput:\nmatA: float32, shape = (1024, 3072)\nmatB: float32, shape = (3072, 3072)\n\nOutput:\nmatC_out: float32, shape = (1024, 3072)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 251, "task_name": "Max_Subarray_Sum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMax_Subarray_Sum\n\nTask Description:\nTask name: Max_Subarray_Sum. Given an input array of 2048 integers (int32), compute the maximum sum of any contiguous subarray of length 64. The output is a single integer (int64) representing this maximum sum. The kernel must consider every possible starting index for the window of 64 consecutive elements and determine the maximum sum.\n\nInput:\ninput: int32, shape = (2048,)\n\nOutput:\noutput: int64, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 271, "task_name": "BatchNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatchNorm\n\nTask Description:\nTask: BatchNorm. Compute batch normalization on a 2D input tensor. The input tensor matA is a float32 matrix of shape (4096, 1024). The second input matB is a float32 matrix of shape (1024, 1024), but only the first 2048 elements (in row-major order) are used: the first 1024 elements are the gamma parameters (one per column of matA) and the next 1024 elements are the beta parameters (one per column of matA). The output is a float32 matrix of shape (4096, 1024). For each column of matA, compute the mean and variance of the elements in that column. Then, for each element in the column, compute: (element - mean) * rsqrt(variance + 1e-5) * gamma + beta, where gamma and beta are the parameters for that column. The kernel must use an epsilon value of 1e-5.\n\nInput:\nmatA: float32, shape = (4096, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (4096, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 258, "task_name": "AdamW", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAdamW\n\nTask Description:\nImplement the AdamW optimization algorithm for updating parameters stored in a flattened tensor. The input includes two matrices: matA (1536x4608, float32) containing parameters, and matB (4608x4608, float32) containing gradients, first moments, and second moments. The output is a flattened tensor matC_out (7077888 elements, float32) with updated parameters. Only update the first third of parameters using the AdamW algorithm with fixed hyperparameters: beta1=0.9, beta2=0.98, learning_rate=0.002, epsilon=1e-8, weight_decay=0.0, and timestep=20. The remaining two-thirds of parameters should be copied unchanged from matA.\n\nInput:\nmatA: float32, shape = (1536, 4608)\nmatB: float32, shape = (4608, 4608)\n\nOutput:\nmatC_out: float32, shape = (7077888,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 245, "task_name": "Tanh_Shrink", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nTanh_Shrink\n\nTask Description:\nImplement the Tanh_Shrink function as a CUDA kernel. The input is a 2D tensor of 16 rows and 262144 columns, with float32 data type. The output is a 1D tensor of 4194304 float32 elements. The Tanh_Shrink function is defined as f(x) = x - tanh(x). This operation must be applied element-wise, meaning each output element depends solely on the corresponding input element. The kernel must process all 4194304 elements independently.\n\nInput:\ninput: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 273, "task_name": "BatchNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatchNorm\n\nTask Description:\nTask: BatchNorm. Implement a kernel to perform batch normalization on a 2D input matrix. The input matrix matA has dimensions 4096x3072 (float32) representing a batch of 4096 samples with 3072 features each. The second input matB (3072x3072, float32) contains gamma parameters in its first 3072 elements and beta parameters in its next 3072 elements when flattened. For each feature column in matA, compute the mean and variance across all samples. Then normalize each element using: (value - mean) * gamma / sqrt(variance + 1e-5) + beta. The output matC_out must have the same dimensions 4096x3072 (float32) as matA. Ensure numerical stability by including the 1e-5 epsilon term in variance calculations.\n\nInput:\nmatA: float32, shape = (4096, 3072)\nmatB: float32, shape = (3072, 3072)\n\nOutput:\nmatC_out: float32, shape = (4096, 3072)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 259, "task_name": "AdamW", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAdamW\n\nTask Description:\nTask: AdamW. The kernel updates parameters using the AdamW optimization algorithm. Input matA (shape: 1792x5376, dtype: float32) contains parameters and other values. Input matB (shape: 5376x5376, dtype: float32) provides gradients, first moments, and second moments. Output matC_out (shape: 1792x5376, dtype: float32) stores updated parameters for the first third of elements (in row-major order) and unchanged values for the rest. Hyperparameters are fixed: beta1=0.8, beta2=0.99, learning_rate=0.001, epsilon=1e-8, weight_decay=0.05, time_step=7.0. The kernel must leave the last two-thirds of elements unmodified.\n\nInput:\nmatA: float32, shape = (1792, 5376)\nmatB: float32, shape = (5376, 5376)\n\nOutput:\nmatC_out: float32, shape = (1792, 5376)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 277, "task_name": "Conv_Transposed_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_2D\n\nTask Description:\nTask name: Conv_Transposed_2D. Compute a 2D transposed convolution operation. The input matrix `matA` has shape (131072, 64) and data type float32. The input matrix `matB` has shape (64, 256) and data type float32. The output matrix `matC_out` must have shape (131072, 256) and data type float32. The kernel must reshape `matA` to (2, 64, 256, 256) and use the first 8192 elements of `matB` (reshaped to (64, 64, 2, 2)) as convolution weights. The transposed convolution uses stride 2 with no padding. For each output position (n, oc, oh, ow), compute the sum over input channels (ic) and kernel positions (kh, kw) where valid input positions (ih, iw) are derived as ih = (oh - kh)/2 and iw = (ow - kw)/2, requiring (oh - kh) and (ow - kw) to be non-negative, even, and within input spatial bounds (0-255).\n\nInput:\nmatA: float32, shape = (131072, 64)\nmatB: float32, shape = (64, 256)\n\nOutput:\nmatC_out: float32, shape = (131072, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 256, "task_name": "AdamW", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAdamW\n\nTask Description:\nImplement the AdamW optimization kernel. The input matrix matA (1024x3072) contains parameters to be updated. The input matrix matB (3072x3072) stores the gradients, first moments, and second moments in its first 3145728 elements (flattened), divided into three equal segments. The output matrix matC_out (1024x3072) will contain the updated parameters. For each element in the output, if its linear index (row-major) is less than 1048576 (one-third of the total elements in matA), update it using the AdamW formula with fixed hyperparameters: beta1=0.9, beta2=0.999, learning_rate=0.001, weight_decay=0.01, epsilon=1e-8, and time step=10. Otherwise, copy the corresponding element from matA. The hyperparameters are constants and must not be changed.\n\nInput:\nmatA: float32, shape = (1024, 3072)\nmatB: float32, shape = (3072, 3072)\n\nOutput:\nmatC_out: float32, shape = (1024, 3072)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 266, "task_name": "Avg_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_3D\n\nTask Description:\nTask: Avg_Pooling_3D. The kernel must perform 3D average pooling. The input is a tensor 'matA' of shape (262144, 8) and data type float32, which represents a 5D tensor of shape (1, 8, 64, 64, 64) in row-major order. The second input 'matB' of shape (8, 1) and float32 is provided but not used. The output 'matC_out' is a vector of shape (262144,) and float32, which is the flattened version of the 5D output tensor of shape (1, 8, 32, 32, 32). The pooling operation uses a kernel size of 3x3x3, stride of 2x2x2, and padding of 1x1x1. For each output element, the kernel must average 27 input elements from the corresponding window in the input. When the window extends beyond the input boundaries (due to padding), the out-of-bound elements should be treated as zero. The kernel must be implemented for the GPU using CUDA.\n\nInput:\nmatA: float32, shape = (262144, 8)\nmatB: float32, shape = (8, 1)\n\nOutput:\nmatC_out: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 220, "task_name": "Smooth_L1_Loss", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSmooth_L1_Loss\n\nTask Description:\nImplement the Smooth_L1_Loss kernel for GPU using CUDA. The kernel computes the mean Smooth L1 Loss between two input float32 tensors of shape (16, 262144) and outputs a single float32 scalar. For each element, calculate the absolute difference between input and target. If the difference is less than 1.0, compute 0.5 * (difference)^2 / 1.0; otherwise compute (difference - 0.5). Accumulate all element losses atomically and divide the total by the number of elements (16 * 262144) to produce the mean loss.\n\nInput:\ninput: float32, shape = (16, 262144)\ntarget: float32, shape = (16, 262144)\n\nOutput:\noutput: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 267, "task_name": "Avg_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_3D\n\nTask Description:\nTask: Avg_Pooling_3D\nDescription: Implement a 3D average pooling operation on the input tensor A, which is provided as a 2D matrix of shape (393216, 8). This tensor must be interpreted as a 5D tensor of shape (2, 4, 64, 96, 64) in row-major order. Apply a 3x3x3 pooling kernel with stride 2 in each spatial dimension and padding 1. For each output position, compute the average of all valid elements within the 3x3x3 window (treating out-of-bound elements as zero) and divide the sum by 27.0. The result should be a 5D tensor of shape (2, 4, 32, 48, 32), flattened into the output matrix of shape (393216, 1). The input matrix B of shape (8, 1) is provided but not used in the computation.\n\nInput:\nmatA: float32, shape = (393216, 8)\nmatB: float32, shape = (8, 1)\n\nOutput:\nmatC_out: float32, shape = (393216, 1)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 260, "task_name": "AdamW", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAdamW\n\nTask Description:\nTask: AdamW. The kernel performs the AdamW optimization step on parameters. The input tensor matA (float32, shape 2048x6144) contains parameters and other data. The input tensor matB (float32, shape 6144x6144) provides gradients, first moments, and second moments in its flattened segments. The output tensor matC_out (float32, shape 12582912) contains updated parameters. Only the first third of parameters (by linear index) are updated using fixed hyperparameters: beta1=0.9, beta2=0.999, learning rate=0.0003, weight decay=0.1, time step=3.0, and epsilon=1e-8. Unmodified elements from matA are copied to matC_out for indices beyond the first third.\n\nInput:\nmatA: float32, shape = (2048, 6144)\nmatB: float32, shape = (6144, 6144)\n\nOutput:\nmatC_out: float32, shape = (12582912,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 287, "task_name": "2D_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\n2D_Convolution\n\nTask Description:\nTask: 2D_Convolution. Implement a CUDA kernel for 2D convolution without padding (valid convolution). The input is a 256x256 matrix stored as a flattened float32 array of length 65536. The kernel is a 24x24 matrix stored as a flattened float32 array of length 576. The output is a 233x233 matrix stored as a flattened float32 array of length 54289. For each output position (i,j), compute the sum of element-wise products between the input submatrix starting at (i,j) with dimensions 24x24 and the kernel matrix.\n\nInput:\nconv_input: float32, shape = (65536,)\nconv_kernel: float32, shape = (576,)\n\nOutput:\nconv_output: float32, shape = (54289,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 285, "task_name": "Conv_Transposed_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_3D\n\nTask Description:\nTask: Conv_Transposed_3D. Compute a 3D transposed convolution (deconvolution) operation. The input tensor matA has shape (131072, 8) and dtype float32, representing a flattened tensor of shape [batch=1, input_channels=8, depth=64, height=64, width=32]. The weight tensor matB has shape (8, 64) and dtype float32, representing a flattened kernel of shape [input_channels=8, output_channels=8, kernel_depth=2, kernel_height=2, kernel_width=2]. The output tensor matC_out has shape (131072, 64) and dtype float32, representing a flattened output of shape [1, 8, 128, 128, 64]. The convolution uses stride 2 in all spatial dimensions with no padding. For each output element, iterate over input channels and kernel dimensions, compute input indices by subtracting kernel offsets and dividing by stride (only when differences are even), and accumulate products of input elements and weights. Skip computations where derived input indices are out-of-bounds.\n\nInput:\nmatA: float32, shape = (131072, 8)\nmatB: float32, shape = (8, 64)\n\nOutput:\nmatC_out: float32, shape = (131072, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 274, "task_name": "BatchNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatchNorm\n\nTask Description:\nTask name: BatchNorm. The kernel should compute batch normalization on a 2D matrix. Input matA is a float32 matrix of shape (16384, 1024). Input matB is a float32 matrix of shape (1024, 1024), where the first 1024 elements represent gamma (scale) parameters and the next 1024 elements represent beta (shift) parameters. The output matC_out is a float32 matrix of shape (16384, 1024). For each element, compute the mean and variance of its entire column in matA, then normalize the element using these statistics: subtract the mean, divide by the square root of variance plus 1e-5, scale by gamma, and add beta. Each column's statistics must be computed independently.\n\nInput:\nmatA: float32, shape = (16384, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (16384, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 197, "task_name": "Pixel_Unshuffle", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPixel_Unshuffle\n\nTask Description:\nTask: Pixel_Unshuffle. Transform an input tensor of shape (8, 2, 128, 128) and dtype float32 into an output tensor of shape (8, 8, 64, 64) and dtype float32. The kernel must reorganize spatial data by downscaling height and width dimensions by a factor of 2 while increasing the channel dimension. Specifically, each 2x2 spatial block in the input should be flattened into channel dimensions, where elements are mapped such that spatial positions within each block determine their new channel index. The kernel must preserve batch dimensions and ensure all input elements are correctly mapped without exceeding output boundaries.\n\nInput:\ninput: float32, shape = (8, 2, 128, 128)\n\nOutput:\noutput: float32, shape = (8, 8, 64, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 275, "task_name": "BatchNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatchNorm\n\nTask Description:\nImplement a Batch Normalization kernel for a 2D matrix. The input matrix matA has dimensions (8192, 1536) and data type float32. The second input matB has dimensions (1536, 1536) and contains gamma parameters in its first 1536 elements and beta parameters in its next 1536 elements. The output matrix matC_out must have dimensions (8192, 1536) and data type float32. For each column, compute the mean and variance across all rows. Then normalize each element using: normalized_value = (element - column_mean) * rsqrtf(column_variance + 1e-5) * gamma_j + beta_j, where gamma_j and beta_j correspond to the column index.\n\nInput:\nmatA: float32, shape = (8192, 1536)\nmatB: float32, shape = (1536, 1536)\n\nOutput:\nmatC_out: float32, shape = (8192, 1536)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 278, "task_name": "Conv_Transposed_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_2D\n\nTask Description:\nPerform a 2D transposed convolution on an input tensor stored in matrix A (shape: [49152, 128]), which represents a 4D tensor of dimensions [1, 128, 256, 192]. The weights are stored in the first 65536 elements of matrix B (shape: [128, 512]) and represent a 4D tensor of dimensions [128, 128, 2, 2]. The convolution uses a stride of 2 in both height and width with no padding. The output tensor has dimensions [1, 128, 512, 384] and must be stored in matrix C (shape: [49152, 512]). For each output element at position (oc, oh, ow) (with batch index 0), compute the sum over input channels (ic) and kernel indices (kh, kw) of the product of the input element at (ic, ih, iw) and the weight at (ic, oc, kh, kw), where ih = (oh - kh) / 2 and iw = (ow - kw) / 2. Only include terms where (oh - kh) and (ow - kw) are even, and ih is in [0, 256) and iw is in [0, 192).\n\nInput:\nmatA: float32, shape = (49152, 128)\nmatB: float32, shape = (128, 512)\n\nOutput:\nmatC_out: float32, shape = (49152, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 282, "task_name": "Conv_Transposed_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_3D\n\nTask Description:\nTask: Conv_Transposed_3D. Compute a 3D transposed convolution operation. The input tensor 'matA' has data type float32 and shape (131072, 16), representing a 5D tensor with dimensions [1, 16, 32, 64, 64] when reshaped. The weight tensor 'matB' has data type float32 and shape (16, 128), but only the first 1024 elements are used, representing a 5D weight tensor with dimensions [16, 16, 2, 2, 2]. The output tensor 'matC_out' has data type float32 and shape (131072, 128), representing a 5D output tensor with dimensions [1, 16, 64, 128, 128] when reshaped. The kernel must compute each output element by iterating over input channels and kernel dimensions (depth=2, height=2, width=2). For each output position (n, output_channel, output_depth, output_height, output_width), contributions are gathered from input positions (n, input_channel, input_depth, input_height, input_width) where input_depth = (output_depth - kernel_depth) / 2 (and similarly for height/width), only when the subtraction yields an even result and the computed input position is within bounds. The kernel must avoid out-of-bounds accesses and correctly decompose linear indices into 5D coordinates.\n\nInput:\nmatA: float32, shape = (131072, 16)\nmatB: float32, shape = (16, 128)\n\nOutput:\nmatC_out: float32, shape = (131072, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 289, "task_name": "2D_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\n2D_Convolution\n\nTask Description:\nImplement a 2D convolution kernel that processes a 1024x1024 input matrix using a 24x24 kernel. The input tensors are conv_input (float32, 1024x1024) and conv_kernel (float32, 24x24). The output tensor conv_output must be float32 with dimensions 1001x1001. For each output element at position (i, j), compute the sum of element-wise products between the kernel and the corresponding input patch starting at (i, j), ensuring the kernel never exceeds input boundaries. The kernel must handle all valid positions where the kernel fully overlaps the input without padding.\n\nInput:\nconv_input: float32, shape = (1024, 1024)\nconv_kernel: float32, shape = (24, 24)\n\nOutput:\nconv_output: float32, shape = (1001, 1001)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 288, "task_name": "2D_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\n2D_Convolution\n\nTask Description:\nImplement a 2D convolution kernel where the input is a 512x512 matrix stored as a 262144-element float32 vector. The convolution kernel is a 24x24 matrix stored as a 576-element float32 vector. The output must be a float32 vector of size 239121, corresponding to the valid convolution result without padding. The kernel should compute each output element as the sum of element-wise products between the corresponding input patch and the kernel matrix. Input patches must be slid across the input matrix with a stride of 1 and no padding.\n\nInput:\nconv_input: float32, shape = (262144,)\nconv_kernel: float32, shape = (576,)\n\nOutput:\nconv_output: float32, shape = (239121,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 286, "task_name": "2D_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\n2D_Convolution\n\nTask Description:\nTask: 2D Convolution. Compute a 2D convolution of a 128x128 input matrix with a 24x24 kernel matrix, producing a 105x105 output matrix. The convolution is performed without padding (valid convolution). Inputs are provided as 1D arrays: conv_input (16384 float32 elements) represents the flattened 128x128 matrix, and conv_kernel (576 float32 elements) represents the flattened 24x24 kernel. Output conv_output must be a 1D array of 11025 float32 elements representing the flattened 105x105 result. Each output element is computed as the sum of element-wise products between the kernel and a corresponding 24x24 input patch.\n\nInput:\nconv_input: float32, shape = (16384,)\nconv_kernel: float32, shape = (576,)\n\nOutput:\nconv_output: float32, shape = (11025,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 303, "task_name": "RMSNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRMSNorm\n\nTask Description:\nCompute the RMSNorm kernel. Given two input matrices: matA (float32, shape 16384x2048) and matB (float32, shape 2048x2048). For each row in matA, compute the root mean square (RMS) across all 2048 elements. The RMS for row i is calculated as sqrt( sum(matA[i,j]²) / 2048 + 1e-6 ). Then, for each element (i,j) in the output matrix matC_out (float32, shape 16384x2048), compute (matA[i,j] / RMS_i) * gamma_j, where gamma_j is the j-th element from the first 2048 values of matB (treated as a vector). Ensure all computations respect tensor boundaries and numerical stability.\n\nInput:\nmatA: float32, shape = (16384, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (16384, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 269, "task_name": "Avg_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_3D\n\nTask Description:\nTask: Avg_Pooling_3D. Compute a 3D average pooling operation on the input tensor matA. Inputs: matA (float32, shape (884736, 8)) and matB (float32, shape (8, 1)). The kernel must interpret matA as a 5D tensor with dimensions (batch=2, channels=8, depth=96, height=48, width=96). Output: matC_out (float32, shape (884736,)), representing a flattened 5D tensor of shape (2, 8, 48, 24, 48). Use a 3x3x3 pooling window with stride 2x2x2 and padding 1x1x1. For each output element, average all valid elements in the corresponding input window, treating out-of-bound elements as zero. Divide the sum by 27 to compute the average. Constraints: The kernel must ignore matB, handle boundary conditions via padding, and preserve input-output mapping invariants.\n\nInput:\nmatA: float32, shape = (884736, 8)\nmatB: float32, shape = (8, 1)\n\nOutput:\nmatC_out: float32, shape = (884736,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 306, "task_name": "SGD", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSGD\n\nTask Description:\nImplement a CUDA kernel for the SGD task. The kernel should take two input matrices, matA and matB, both of shape 1024x1024 and data type float32. For each element at position (i, j), compute the output as matA[i][j] minus 0.1 times matB[i][j]. The output matrix matC_out must have the same shape and data type as the inputs. Ensure the kernel only processes valid indices within the 1024x1024 boundaries.\n\nInput:\nmatA: float32, shape = (1024, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (1024, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 290, "task_name": "2D_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\n2D_Convolution\n\nTask Description:\nPerform a 2D convolution operation on a 2048x2048 input matrix using a 24x24 kernel. The input matrix is provided as a flattened array of 4194304 float32 elements. The kernel is provided as a flattened array of 576 float32 elements. The output should be a 2025x2025 matrix (flattened to 4100625 float32 elements), computed by sliding the kernel over the input matrix without padding (valid convolution). At each position, the output element is the sum of the element-wise product between the kernel and the overlapping input patch.\n\nInput:\nconv_input: float32, shape = (4194304,)\nconv_kernel: float32, shape = (576,)\n\nOutput:\nconv_output: float32, shape = (4100625,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 310, "task_name": "SGD", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSGD\n\nTask Description:\nImplement an SGD kernel that performs an element-wise operation on two input matrices. The kernel should compute each element of the output matrix as: output[i][j] = matA[i][j] - (0.005 * matB[i][j]). Input matrices matA and matB are both float32 with dimensions 3072x3072. The output matrix matC_out must be float32 with dimensions 3072x3072. The kernel must process all elements in the matrices and maintain dimensional consistency.\n\nInput:\nmatA: float32, shape = (3072, 3072)\nmatB: float32, shape = (3072, 3072)\n\nOutput:\nmatC_out: float32, shape = (3072, 3072)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 296, "task_name": "PReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPReLU\n\nTask Description:\nTask: PReLU. Apply the Parametric Rectified Linear Unit (PReLU) activation function to each element of input matrix matA (shape: 8192x1024, dtype: float32). The alpha parameter for PReLU is stored as the first element of input matrix matB (shape: 1024x1024, dtype: float32). For each element in matA: if the element is greater than or equal to zero, output the element unchanged; otherwise, output the element multiplied by alpha. The output matrix matC_out must have shape 8192x1024 and dtype float32. Constraints: Only the first element of matB is used; all other elements in matB are ignored. The kernel must handle the fixed input dimensions.\n\nInput:\nmatA: float32, shape = (8192, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (8192, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 268, "task_name": "Avg_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_3D\n\nTask Description:\nTask name: Avg_Pooling_3D. Implement a CUDA kernel that performs 3D average pooling on a 5D tensor. The input tensor matA (float32, shape: 524288x8) must be interpreted as a reshaped 5D tensor with dimensions (1, 16, 128, 64, 32). The pooling uses a 3x3x3 kernel, stride 2x2x2, and padding 1x1x1. Output positions correspond to a flattened 5D tensor of shape (1, 16, 64, 32, 16) stored in matC_out (float32, shape: 524288). For each output element, average all valid elements in the 3x3x3 input window (out-of-bound elements count as zero), then divide by 27. The input matB (float32, shape: 8x1) is unused.\n\nInput:\nmatA: float32, shape = (524288, 8)\nmatB: float32, shape = (8, 1)\n\nOutput:\nmatC_out: float32, shape = (524288,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 295, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nImplement a CUDA kernel for the GELU activation function. The kernel must take two input matrices: matA of shape (32768, 2048) and matB of shape (2048, 2048), both of data type float32. The kernel should compute the GELU function element-wise on each element of matA, ignoring matB. The output matrix matC_out must have the same shape as matA (32768, 2048) and data type float32. The GELU function is defined by the formula: y = 0.5 * x * (1.0 + tanh(0.7978845608 * (x + 0.044715 * x * x * x))).\n\nInput:\nmatA: float32, shape = (32768, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (32768, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 293, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nImplement a CUDA kernel named GELU to compute the Gaussian Error Linear Unit activation function. The kernel takes two input matrices: matA (float32, 16384x2048) and matB (float32, 2048x2048). However, only matA is used in the computation, while matB is ignored. For each element x in matA, compute y = 0.5 * x * (1 + tanh(0.7978845608 * (x + 0.044715 * x^3))). The output matrix matC_out (float32, 16384x2048) must contain these computed values. The kernel should be grid-stride based and handle arbitrary matrix dimensions via thread indexing.\n\nInput:\nmatA: float32, shape = (16384, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (16384, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 294, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nTask Name: GELU. Compute the Gaussian Error Linear Unit (GELU) activation function for each element of input matrix A, ignoring input matrix B. Matrix A is a float32 tensor with dimensions 32768 rows by 1024 columns. Matrix B is a float32 tensor with dimensions 1024 rows by 1024 columns but is unused in the computation. The output must be a float32 tensor of shape (32768, 1024) containing the GELU activation result for each element of A. The GELU function is defined as: 0.5 * x * (1 + tanh(0.7978845608 * (x + 0.044715 * x^3))). The kernel must respect matrix boundaries to avoid out-of-bounds memory accesses.\n\nInput:\nmatA: float32, shape = (32768, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (32768, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 281, "task_name": "Conv_Transposed_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_3D\n\nTask Description:\nImplement a CUDA kernel for a 3D Transposed Convolution operation. The kernel takes two input tensors: matA of shape (32768, 8) and matB of shape (8, 64), both with float32 data type. It produces an output tensor matC_out of shape (32768, 64) with float32 data type. The operation involves reconstructing a higher-resolution 3D volume from a lower-resolution input using learned filters. Input spatial dimensions are 32×32×32, and output spatial dimensions are 64×64×64 after applying a 2×2×2 kernel with stride 2 in all dimensions. Constraints include checking valid spatial indices and handling only even-index positions during the upsampling process.\n\nInput:\nmatA: float32, shape = (32768, 8)\nmatB: float32, shape = (8, 64)\n\nOutput:\nmatC_out: float32, shape = (32768, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 304, "task_name": "RMSNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRMSNorm\n\nTask Description:\nTask: RMSNorm. Compute RMS normalization for a matrix. Input matA is a float32 tensor of shape (32768, 1024), and matB is a float32 tensor of shape (1024, 1024). Only the first 1024 elements of matB (treated as a vector) are used as gamma weights. Output matC_out is a float32 tensor of shape (32768, 1024). For each row in matA, compute the root mean square (RMS) with an epsilon of 1e-6 added inside the square root. Then normalize each element in the row by dividing it by its row's RMS and multiply by the corresponding gamma weight from matB. Ensure computations respect matrix boundaries and the inner dimension of 1024.\n\nInput:\nmatA: float32, shape = (32768, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (32768, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 272, "task_name": "BatchNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBatchNorm\n\nTask Description:\nTask name: BatchNorm. Implement a CUDA kernel for batch normalization. The kernel takes two input tensors: matA of shape (8192, 2048) and matB of shape (2048, 2048), both of float32 type. The output tensor matC_out has shape (8192, 2048) and float32 type. For each column of matA, compute the mean and variance of all elements in that column. Then, for each element in the column, normalize it by subtracting the mean and dividing by the square root of the variance plus 1e-5. Then, scale the normalized value by a gamma parameter and add a beta parameter. The gamma and beta parameters for column index j are stored in matB: gamma at the j-th element of the first 2048 elements (flattened) of matB, and beta at the j-th element of the next 2048 elements (flattened) of matB. The kernel must be implemented to handle the given tensor dimensions.\n\nInput:\nmatA: float32, shape = (8192, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (8192, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 301, "task_name": "RMSNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRMSNorm\n\nTask Description:\nTask: RMSNorm. Compute the RMS normalization for each row of the input matrix matA. The RMS for a row is computed as the square root of the average of the squares of the elements in that row, plus a small epsilon (1e-6). Then, each element in the row is divided by the RMS and multiplied by a corresponding element from the gamma vector. The gamma vector is taken from the first row of the input matrix matB. The input matA is a float32 matrix of shape (8192, 1024). The input matB is a float32 matrix of shape (1024, 1024), but only the first row is used as gamma. The output is a float32 matrix of shape (8192, 1024). The kernel must respect the matrix dimensions and avoid out-of-bound accesses.\n\nInput:\nmatA: float32, shape = (8192, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (8192, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 270, "task_name": "Avg_Pooling_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nAvg_Pooling_3D\n\nTask Description:\nTask: Avg_Pooling_3D. Given an input tensor matA of shape (524288, 8) and dtype float32, which represents a reshaped 5D tensor of dimensions (1, 4, 128, 128, 64), perform a 3D average pooling operation with a kernel size of 3x3x3, stride of 2x2x2, and padding of 1x1x1. The second input tensor matB of shape (8, 1) and dtype float32 is provided but not used. The output tensor matC_out should be of shape (524288, 1) and dtype float32, which is the flattened result of the pooled tensor of shape (1, 4, 64, 64, 32). The pooling must account for the padding by including zero-padded values only when the window extends beyond the input boundaries, and the average is computed by summing the valid elements in the window and dividing by 27 (the total number of elements in the kernel).\n\nInput:\nmatA: float32, shape = (524288, 8)\nmatB: float32, shape = (8, 1)\n\nOutput:\nmatC_out: float32, shape = (524288, 1)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 307, "task_name": "SGD", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSGD\n\nTask Description:\nTask: SGD. Compute an output matrix where each element is calculated as the corresponding element from the first input matrix minus 0.01 times the corresponding element from the second input matrix. The inputs are two float32 matrices of shape 2048x2048. The output is a float32 matrix of the same shape. Each output element must be computed independently from its corresponding input elements at the same position.\n\nInput:\nmatA: float32, shape = (2048, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (2048, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 297, "task_name": "PReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPReLU\n\nTask Description:\nImplement the PReLU kernel on GPU. Given two input matrices: matA (16384x1024, float32) and matB (1024x1024, float32), compute an output matrix matC_out (16384x1024, float32) where each element is calculated as: if the element in matA is non-negative, output the same element; if negative, multiply the element by the scalar value stored in the first position of matB (matB[0]). The kernel must process all elements independently and preserve input shapes.\n\nInput:\nmatA: float32, shape = (16384, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (16384, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 283, "task_name": "Conv_Transposed_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_3D\n\nTask Description:\nImplement a CUDA kernel for 3D transposed convolution. The input matrix 'matA' (float32, 221184x8) represents a 5D tensor reshaped to 2D with dimensions (2, 8, 48, 48, 48). The weight matrix 'matB' (float32, 8x64) represents a 5D kernel reshaped to 2D with dimensions (8, 8, 2, 2, 2). The output 'matC_out' (float32, 221184x64) must be a 5D tensor reshaped to 2D with dimensions (2, 8, 96, 96, 96). For each output element at position (n, oc, od, oh, ow), compute the sum over input channels (ic) and kernel indices (kd, kh, kw) of the product between input element (n, ic, (od-kd)/2, (oh-kh)/2, (ow-kw)/2) and weight element (ic, oc, kd, kh, kw), only if (od-kd), (oh-kh), and (ow-kw) are even and the computed input spatial indices are within [0,47]. Skip contributions where input indices are invalid.\n\nInput:\nmatA: float32, shape = (221184, 8)\nmatB: float32, shape = (8, 64)\n\nOutput:\nmatC_out: float32, shape = (221184, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 305, "task_name": "RMSNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRMSNorm\n\nTask Description:\nTask: RMSNorm. The kernel computes RMS normalization on an input matrix. Given a float32 input matrix matA of shape (32768, 2048) and a float32 weight matrix matB of shape (2048, 2048), produce an output matrix matC_out of shape (32768, 2048). For each row in matA, compute the root mean square (RMS) of all elements in that row, add a small epsilon (1e-6) to avoid division by zero, and normalize each element in the row by dividing it by this RMS value. Then, multiply each normalized element by a corresponding weight from the first row of matB. Specifically, for element (i, j) in the output, the weight is taken from matB[0, j]. The kernel must handle all rows independently and respect the fixed dimensions of the matrices.\n\nInput:\nmatA: float32, shape = (32768, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (32768, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 302, "task_name": "RMSNorm", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRMSNorm\n\nTask Description:\nImplement the RMSNorm kernel. The kernel takes two input matrices: matA of shape (16384, 1024) and matB of shape (1024, 1024), both of float32 data type. The kernel must produce an output matrix matC_out of shape (16384, 1024) and float32 data type. For each row in matA, compute the root mean square (RMS) over the 1024 elements in that row, adding 1e-6 to the mean of squares to avoid division by zero. The RMS value for a row is the square root of (the sum of squares of the row elements divided by 1024 plus 1e-6). Then, for each element in the row, normalize the element by dividing it by the row's RMS and multiply by the corresponding element from the gamma vector. The gamma vector is the first row of matB (i.e., the first 1024 elements of matB). Each row of matA must be processed independently.\n\nInput:\nmatA: float32, shape = (16384, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (16384, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 308, "task_name": "SGD", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSGD\n\nTask Description:\nTask name: SGD. The kernel should compute an element-wise operation between two input matrices of size 4096x4096 and type float32. The operation for each element (i, j) is: output[i, j] = input1[i, j] - 0.001 * input2[i, j]. The output matrix must be of the same dimensions and type as the inputs. The kernel must respect the matrix dimensions and compute every element independently.\n\nInput:\nmatA: float32, shape = (4096, 4096)\nmatB: float32, shape = (4096, 4096)\n\nOutput:\nmatC_out: float32, shape = (4096, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 291, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nTask name: GELU. Compute the Gaussian Error Linear Unit (GELU) activation function for each element of the input matrix matA. The input matrix matA has dimensions 8192 x 1024 and data type float32. The input matrix matB has dimensions 1024 x 1024 and data type float32 but is not used in the computation. The output matrix matC_out must have the same dimensions (8192 x 1024) and data type float32. The GELU function is defined by the formula: y = 0.5 * x * (1 + tanh(0.7978845608 * (x + 0.044715 * x^3))). The kernel must be implemented to handle the given matrix dimensions.\n\nInput:\nmatA: float32, shape = (8192, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (8192, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 279, "task_name": "Conv_Transposed_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_2D\n\nTask Description:\nImplement a CUDA kernel for a transposed 2D convolution (Conv_Transposed_2D). The input tensor matA (float32, shape [131072, 32]) represents a 4D tensor of shape [1, 32, 512, 256]. The weight tensor matB (float32, shape [32, 128]) contains a 4D weight tensor of shape [32, 32, 2, 2] in its first 4096 elements. The output tensor matC_out (float32, shape [16777216]) is the flattened result of the transposed convolution, which has the 4D shape [1, 32, 1024, 512]. The convolution uses a 2x2 kernel, stride 2, and no padding. For each output element at [0, oc, oh, ow], the kernel must sum over input channels (ic) and kernel positions (kh, kw) the product of the input element at [0, ic, ih, iw] and the weight at [ic, oc, kh, kw], where ih = (oh - kh) / 2 (only if (oh - kh) is even and ih is in [0, 511]) and iw = (ow - kw) / 2 (only if (ow - kw) is even and iw is in [0, 255]).\n\nInput:\nmatA: float32, shape = (131072, 32)\nmatB: float32, shape = (32, 128)\n\nOutput:\nmatC_out: float32, shape = (16777216,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 316, "task_name": "Softmin", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmin\n\nTask Description:\nImplement the Softmin kernel for CUDA. Given two input matrices matA and matB, both of shape 2048x2048 with float32 data type, compute an output matrix matC_out of the same shape and type. The kernel should compute the Softmin function for each row of matA, where each element matC_out[i][j] equals the softmin of matA[i] at position j. Specifically, for each row i, compute the maximum value of -matA[i][k] across k, then calculate exponentials of (-matA[i][k] - max_value), sum them, and set matC_out[i][j] as the exponential of (-matA[i][j] - max_value) divided by the sum. The kernel must handle 2048x2048 matrices efficiently and ensure numerical stability.\n\nInput:\nmatA: float32, shape = (2048, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (2048, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 300, "task_name": "PReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPReLU\n\nTask Description:\nTask: PReLU. Given an input matrix matA of shape (32768, 2048) of float32 values and another matrix matB of shape (2048, 2048) of float32 values, compute an output matrix matC_out of shape (32768, 2048). For each element in matA, if the element is greater than or equal to zero, the corresponding element in matC_out is the same as the element in matA. If the element is negative, the corresponding element in matC_out is the product of the element in matA and the first element of matB (i.e., matB[0][0]). The computation must be performed element-wise and independently for each element.\n\nInput:\nmatA: float32, shape = (32768, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (32768, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 309, "task_name": "SGD", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSGD\n\nTask Description:\nImplement the SGD kernel for updating parameters. The kernel takes two matrices, matA and matB, both of dimensions 2304x2304 and data type float32. The output matrix matC_out, also 2304x2304 and float32, is computed by performing an element-wise operation: each element at position (i, j) in matC_out is equal to the element from matA at (i, j) minus 0.05 times the element from matB at (i, j). The kernel must handle the fixed matrix size and perform the operation independently for each element.\n\nInput:\nmatA: float32, shape = (2304, 2304)\nmatB: float32, shape = (2304, 2304)\n\nOutput:\nmatC_out: float32, shape = (2304, 2304)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 332, "task_name": "Multi-Head_Self-Attention", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMulti-Head_Self-Attention\n\nTask Description:\nMulti-Head Self-Attention: Implement a CUDA kernel that computes multi-head self-attention using input tensors Q, K, and V, each of shape (128, 64) and float32 data type. The output tensor must be of shape (128, 64) and float32 data type. The computation must split the input tensors into 8 heads, compute scaled dot-product attention scores for each head, apply row-wise softmax normalization, and combine the results. Constraints include: fixed head count (8), head dimension (8), and attention score scaling by 1/sqrt(8).\n\nInput:\nQ: float32, shape = (128, 64)\nK: float32, shape = (128, 64)\nV: float32, shape = (128, 64)\n\nOutput:\nout: float32, shape = (128, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 324, "task_name": "Softsign", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftsign\n\nTask Description:\nTask: Softsign. The kernel must compute the softsign function for each element of the input matrix matA. The softsign function is defined as f(x) = x / (1 + absolute_value(x)). The input matrix matA has dimensions 32768 rows by 1024 columns and is of data type float32. The input matrix matB has dimensions 1024 by 1024 and float32 type but is not used in the computation. The output matrix matC_out must have the same dimensions as matA (32768, 1024) and float32 type. The kernel must handle the entire matrix and compute each element independently.\n\nInput:\nmatA: float32, shape = (32768, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (32768, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 328, "task_name": "Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSwish\n\nTask Description:\nTask: Swish. Compute the element-wise Swish activation function on a 16384x2048 input matrix matA of float32 values. The Swish function is defined as x * sigmoid(x), where sigmoid(x) = 1 / (1 + exp(-x)). The output matrix matC_out must have the same shape (16384x2048) and float32 datatype as matA. Note that matB is provided but not used in the computation.\n\nInput:\nmatA: float32, shape = (16384, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (16384, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 314, "task_name": "Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmax\n\nTask Description:\nImplement a CUDA kernel named Softmax that computes the softmax activation for each row of input matrix matA. The input matA is a float32 tensor of shape (4096, 1024), and matB is a float32 tensor of shape (1024, 1024) that should be ignored. The output matC_out is a float32 tensor of shape (4096, 1024) where each element (i, j) equals exp(matA[i,j] - max_i) / sum_{k}(exp(matA[i,k] - max_i)), with max_i being the maximum value in row i. The kernel must process rows independently and avoid using matB.\n\nInput:\nmatA: float32, shape = (4096, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (4096, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 336, "task_name": "Cosine_Similarity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCosine_Similarity\n\nTask Description:\nTask: Cosine_Similarity. Compute cosine similarity between corresponding rows of two matrices. Inputs: matA and matB, both float32 matrices of shape (1024, 1024). Output: matC_cuda, a float32 vector of shape (1024,). For each row i, compute dot product between matA[i] and matB[i], then divide by the product of their L2 norms. Add 1e-8 to denominator to prevent division by zero. Each row must be processed independently.\n\nInput:\nmatA: float32, shape = (1024, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 311, "task_name": "Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmax\n\nTask Description:\nTask Name: Softmax. Implement a CUDA kernel that computes the softmax function for each row of the input matrix matA. For each row, find the maximum value, subtract it from each element to avoid numerical instability, compute exponentials, sum them, and then divide each exponential by the sum to normalize. The kernel takes two input matrices: matA and matB, both 2048x2048 float32 matrices. Note that matB is unused in the computation. The output is a single matrix matC_out of size 2048x2048 float32, where each row represents a valid probability distribution (sums to 1).\n\nInput:\nmatA: float32, shape = (2048, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (2048, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 319, "task_name": "Softmin", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmin\n\nTask Description:\nTask: Softmin. Compute the softmin of each row in the input matrix matA (shape: 4096x1024, dtype: float32), ignoring the second input matB (shape: 1024x1024, dtype: float32). The output matrix matC_out (shape: 4096x1024, dtype: float32) must be computed per row where each element (i,j) equals exp(-matA[i,j]) divided by the sum of exp(-matA[i,k]) for all k in the same row. For numerical stability, first compute the maximum value of -matA[i,:] and subtract it from each -matA[i,j] before exponentiation. Each row must be processed independently.\n\nInput:\nmatA: float32, shape = (4096, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (4096, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 312, "task_name": "Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmax\n\nTask Description:\nTask: Softmax. Compute the softmax of each row in input matrix A, which is a 1024x4096 float32 matrix. The kernel receives an unused second input matrix B (4096x4096 float32). The output is a 1024x4096 float32 matrix C. For each row, find the maximum value first, then compute exponentials of each element minus this max, sum these exponentials, and divide each exponential by the sum to produce the softmax output. Rows must be processed independently.\n\nInput:\nmatA: float32, shape = (1024, 4096)\nmatB: float32, shape = (4096, 4096)\n\nOutput:\nmatC_out: float32, shape = (1024, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 337, "task_name": "Cosine_Similarity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCosine_Similarity\n\nTask Description:\nImplement the Cosine_Similarity kernel for GPU using CUDA. The kernel computes cosine similarity row-wise between two input matrices: matA and matB, both of shape (4096, 1024) and data type float32. The output is a vector matC_cuda with shape (4096,) and float32 data type, where each element represents the cosine similarity of corresponding rows from matA and matB. The kernel must handle large matrices efficiently, avoid division by zero by adding a small epsilon (1e-8) during normalization, and compute each row independently.\n\nInput:\nmatA: float32, shape = (4096, 1024)\nmatB: float32, shape = (4096, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 321, "task_name": "Softsign", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftsign\n\nTask Description:\nTask name: Softsign. Compute the Softsign function element-wise on the input matrix matA. The Softsign function for an element x is defined as x / (1.0 + |x|). Input matrices are matA (float32, 8192x1024) and matB (float32, 1024x1024), though only matA is used in the computation. Output matrix matC_out (float32, 8192x1024) must have the same shape as matA. The kernel must process each element independently and only compute for valid indices within the matrix dimensions.\n\nInput:\nmatA: float32, shape = (8192, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (8192, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 298, "task_name": "PReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPReLU\n\nTask Description:\nTask: PReLU. Apply the PReLU activation function element-wise to each element of input matrix matA (shape: 16384x2048, dtype: float32). The activation function is defined as: for each element x, output = x if x >= 0, and output = alpha * x if x < 0. The parameter alpha is a scalar taken from the first element (at index [0,0]) of input matrix matB (shape: 2048x2048, dtype: float32). The output matrix matC_out must have the same shape and data type as matA. Constraints: The kernel must use the same alpha (from matB[0][0]) for every element. The computation must be done independently for each element.\n\nInput:\nmatA: float32, shape = (16384, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (16384, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 313, "task_name": "Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmax\n\nTask Description:\nTask: Softmax. Compute the row-wise softmax of input matrix matA (shape: 512x8192, dtype: float32), ignoring input matrix matB. For each row, find the maximum value, subtract it from all elements, exponentiate each value, then divide by the sum of exponentiated values in that row. Output matrix matC_out (shape: 512x8192, dtype: float32) must have normalized values per row where each row sums to 1. Kernel must process rows independently and ensure numerical stability.\n\nInput:\nmatA: float32, shape = (512, 8192)\nmatB: float32, shape = (8192, 8192)\n\nOutput:\nmatC_out: float32, shape = (512, 8192)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 323, "task_name": "Softsign", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftsign\n\nTask Description:\nSoftsign: Compute the element-wise Softsign function for a 16384x2048 input matrix of float32 values. The Softsign operation is defined as x / (1.0 + |x|) for each element x in the input matrix. The output matrix must have identical dimensions (16384x2048) and data type (float32) as the input. The kernel must process each element independently without altering adjacent elements.\n\nInput:\nmatA: float32, shape = (16384, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (16384, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 343, "task_name": "Elementwise_Addition", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Addition\n\nTask Description:\nImplement an elementwise addition kernel for CUDA. The kernel should take two input tensors, addA and addB, both of data type float32 and shape (16384, 2048). The output tensor addC_cuda must have the same shape and data type. Each element in the output tensor should be the sum of the corresponding elements in the input tensors. The kernel must efficiently handle the entire tensor size of 16384 x 2048 elements.\n\nInput:\naddA: float32, shape = (16384, 2048)\naddB: float32, shape = (16384, 2048)\n\nOutput:\naddC_cuda: float32, shape = (16384, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 338, "task_name": "Cosine_Similarity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCosine_Similarity\n\nTask Description:\nImplement a CUDA kernel called Cosine_Similarity that computes the cosine similarity between corresponding rows of two input matrices. The inputs are matA and matB, both float32 matrices of shape (16384, 1024). The output should be matC_cuda, a float32 vector of length 16384, where each element represents the cosine similarity between the corresponding rows of matA and matB. The computation must avoid division by zero by adding a small epsilon (1e-8) to the denominator. Each row should be processed independently, and the kernel must handle all 16384 rows.\n\nInput:\nmatA: float32, shape = (16384, 1024)\nmatB: float32, shape = (16384, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 329, "task_name": "Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSwish\n\nTask Description:\nTask: Swish. Compute the Swish activation function element-wise on the input matrix matA, ignoring the input matrix matB. The Swish function is defined as: for each element x in matA, compute x * sigmoid(x), where sigmoid(x) = 1 / (1 + exp(-x)). Inputs: matA is a float32 tensor of shape (32768, 1024), and matB is a float32 tensor of shape (1024, 1024) but unused. Output: matC_out is a float32 tensor of shape (32768, 1024) containing the Swish activation of each element in matA. Constraints: The computation must be element-wise and not use matB.\n\nInput:\nmatA: float32, shape = (32768, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (32768, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 339, "task_name": "Cosine_Similarity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCosine_Similarity\n\nTask Description:\nTask: Cosine_Similarity. Given two input matrices matA and matB, both of shape (65536, 1024) and data type float32, compute the cosine similarity for each corresponding row. The output is a vector of shape (65536,) and data type float32. For each row i, the cosine similarity is computed as the dot product of the i-th row of matA and the i-th row of matB, divided by the product of their L2 norms. To avoid division by zero, add a small epsilon (1e-8) to the denominator.\n\nInput:\nmatA: float32, shape = (65536, 1024)\nmatB: float32, shape = (65536, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 299, "task_name": "PReLU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPReLU\n\nTask Description:\nTask Name: PReLU\nDescription: Compute the element-wise Parametric Rectified Linear Unit (PReLU) activation function on a 32768x1024 input matrix of float32 values. The activation function uses a scalar parameter alpha, which is the first element of a second input matrix of shape 1024x1024 and dtype float32. The output is a float32 vector of 33554432 elements, which is the flattened result of the activation applied to each element of the input matrix. For each element, if the element is non-negative, the output is the element itself; if negative, the output is the product of alpha and the element. The kernel must process each element independently and use the provided alpha from the first element of the second input matrix.\n\nInput:\nmatA: float32, shape = (32768, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (33554432,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 344, "task_name": "Elementwise_Addition", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Addition\n\nTask Description:\nImplement an Elementwise_Addition CUDA kernel. The kernel takes two input tensors, addA and addB, both of data type float32 and shape (65536, 2048). It must produce an output tensor addC_cuda of the same shape and data type. The kernel should add corresponding elements from the two input tensors, meaning each element in the output is the sum of the corresponding elements in addA and addB. The kernel must be designed to handle the entire array of 65536 * 2048 elements.\n\nInput:\naddA: float32, shape = (65536, 2048)\naddB: float32, shape = (65536, 2048)\n\nOutput:\naddC_cuda: float32, shape = (65536, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 292, "task_name": "GELU", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGELU\n\nTask Description:\nImplement the GELU activation function as a CUDA kernel. The kernel must accept two input tensors: 'matA' of shape (16384, 1024) and 'matB' of shape (1024, 1024), both of float32 data type. Note that only 'matA' is used in the computation. The output tensor 'matC_out' must be of shape (16384, 1024) and float32 data type. For each element in 'matA', compute the GELU activation using the formula: y = 0.5 * x * (1.0 + tanh(0.7978845608 * (x + 0.044715 * x * x * x))). The kernel must be applied element-wise and must handle the provided tensor dimensions.\n\nInput:\nmatA: float32, shape = (16384, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (16384, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 340, "task_name": "Cosine_Similarity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nCosine_Similarity\n\nTask Description:\nImplement a CUDA kernel named Cosine_Similarity that computes the cosine similarity between corresponding rows of two input matrices. The input matrices matA and matB have dimensions (262144, 1024) and data type float32. The output vector matC_cuda should have dimensions (262144,) and data type float32. For each row index, compute the cosine similarity as the dot product of the row vectors divided by the product of their L2 norms, with a small epsilon added to the denominator to prevent division by zero.\n\nInput:\nmatA: float32, shape = (262144, 1024)\nmatB: float32, shape = (262144, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 280, "task_name": "Conv_Transposed_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_2D\n\nTask Description:\nTask: Conv_Transposed_2D. Implement a CUDA kernel for 2D transposed convolution. Inputs: matA of shape (147456, 64) and dtype float32, representing a tensor reshaped to (1, 64, 384, 384); matB of shape (64, 256) and dtype float32, representing weights reshaped to (64, 64, 2, 2). Output: matC_out of shape (147456, 256) and dtype float32, equivalent to reshaped (1, 64, 768, 768). The kernel must compute each output element by accumulating over input channels and a 2x2 kernel: for each output channel, height, and width, sum contributions from valid input positions where input height and width are derived as (output height - kernel height offset) divided by 2, and similarly for width, ensuring the result is an integer within [0, 383]. Only positions where the subtraction is even and non-negative are considered.\n\nInput:\nmatA: float32, shape = (147456, 64)\nmatB: float32, shape = (64, 256)\n\nOutput:\nmatC_out: float32, shape = (147456, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 330, "task_name": "Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSwish\n\nTask Description:\nThe task is named Swish. The kernel must compute the Swish activation function for each element of the first input matrix matA. The input consists of two matrices: matA of shape (32768, 2048) and matB of shape (2048, 2048), both of data type float32. The kernel should ignore the second input matrix matB. The output is a matrix matC_out of shape (32768, 2048) and float32. The Swish function is defined as: f(x) = x * sigmoid(x) = x / (1 + exp(-x)). The kernel must be applied element-wise, meaning each element in the output is computed solely from the corresponding element in matA.\n\nInput:\nmatA: float32, shape = (32768, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (32768, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 327, "task_name": "Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSwish\n\nTask Description:\nTask: Swish. Compute the Swish activation function for each element of the input matrix matA (16384x1024 of float32). The Swish function is defined as: output = x * sigmoid(x), where x is the input element and sigmoid(x) = 1 / (1 + exp(-x)). The input matrix matB (1024x1024 of float32) is provided but not used in the computation. The output matrix matC_out (16384x1024 of float32) must contain the Swish activation result for each corresponding element in matA. The kernel must process all elements independently and handle the exact matrix dimensions provided.\n\nInput:\nmatA: float32, shape = (16384, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (16384, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 315, "task_name": "Softmax", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmax\n\nTask Description:\nCompute the Softmax kernel. The kernel takes two input matrices: matA of shape (8192, 1024) and matB of shape (1024, 1024), both of float32 data type. The kernel must ignore matB and only use matA. The output matrix matC_out must be of shape (8192, 1024) and float32. For each row in matA, compute the maximum value in that row. Then, for each element in the row, subtract the row maximum and compute the exponential. The softmax value for each element is the exponential of the element minus the row maximum, divided by the sum of the exponentials of all elements in the row (after subtracting the row maximum). Each thread should compute one output element. The kernel must avoid numerical overflow by using the row maximum.\n\nInput:\nmatA: float32, shape = (8192, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (8192, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 348, "task_name": "Elementwise_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Multiplication\n\nTask Description:\nTask: Elementwise_Multiplication. Compute an output tensor by multiplying two input tensors element-wise. The inputs are two tensors named mulA and mulB, both of shape (16384, 2048) and data type float32. The output tensor mulC_cuda must have the same shape (16384, 2048) and data type float32. Each element in the output is the product of the corresponding elements in mulA and mulB. The kernel must handle exactly 16384 * 2048 elements.\n\nInput:\nmulA: float32, shape = (16384, 2048)\nmulB: float32, shape = (16384, 2048)\n\nOutput:\nmulC_cuda: float32, shape = (16384, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 342, "task_name": "Elementwise_Addition", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Addition\n\nTask Description:\nImplement an elementwise addition kernel for GPU computation. The task involves adding two input tensors, addA and addB, each of type float32 and shape (4096, 2048), to produce an output tensor addC_cuda with identical shape and data type. The kernel must compute the sum of corresponding elements from both input tensors, ensuring that every output element C[i][j] equals A[i][j] + B[i][j]. The input tensors are guaranteed to have matching dimensions.\n\nInput:\naddA: float32, shape = (4096, 2048)\naddB: float32, shape = (4096, 2048)\n\nOutput:\naddC_cuda: float32, shape = (4096, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 325, "task_name": "Softsign", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftsign\n\nTask Description:\nTask: Softsign. Compute the Softsign function element-wise on the input matrix matA. The Softsign function for an element x is defined as x / (1.0 + absolute_value(x)). Input matA is a float32 tensor of shape (32768, 2048), and matB is a float32 tensor of shape (2048, 2048) which is not used in this computation. Output matC_out must be a float32 tensor of shape (32768, 2048) containing the Softsign result for each element of matA. The kernel must process every element of matA independently and ignore matB.\n\nInput:\nmatA: float32, shape = (32768, 2048)\nmatB: float32, shape = (2048, 2048)\n\nOutput:\nmatC_out: float32, shape = (32768, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 341, "task_name": "Elementwise_Addition", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Addition\n\nTask Description:\nImplement a CUDA kernel for the Elementwise_Addition task. The kernel must perform element-wise addition between two input tensors: addA and addB, both of dtype float32 and shape (1024, 2048). The output tensor addC_cuda must have the same shape and dtype as the inputs. For each element at position (i, j), the output should be the sum of the corresponding elements from addA and addB. The kernel must handle exactly 1024 × 2048 elements.\n\nInput:\naddA: float32, shape = (1024, 2048)\naddB: float32, shape = (1024, 2048)\n\nOutput:\naddC_cuda: float32, shape = (1024, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 355, "task_name": "Matrix_Vector_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Vector_Multiplication\n\nTask Description:\nMatrix_Vector_Multiplication task: Multiply a matrix by a vector to produce a vector. Input matrix A has shape (262144, 4096) and data type float32. Input vector v has shape (4096,) and data type float32. Output vector y must have shape (262144,) and data type float32. Each element y[i] is computed as the dot product of the i-th row of A with vector v. The kernel must handle matrices with 262144 rows and 4096 columns, where each output element depends solely on its corresponding row in A and the entire vector v.\n\nInput:\nmvA_18: float32, shape = (262144, 4096)\nmvv_18: float32, shape = (4096,)\n\nOutput:\nmvy_cuda_18: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 276, "task_name": "Conv_Transposed_2D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_2D\n\nTask Description:\nImplement a CUDA kernel for a 2D transposed convolution (deconvolution) operation. The input consists of two matrices: 'matA' of shape (65536, 64) and 'matB' of shape (64, 256), both with float32 data type. The kernel must compute an output matrix 'matC_out' of shape (65536, 256) with float32 data type. The operation reverses a convolution with a 2x2 kernel and stride 2: for each output spatial position (oh, ow) and output channel, accumulate contributions from input channels and kernel positions. Specifically, map each output element to input positions via ih = (oh - kh)/2 and iw = (ow - kw)/2 only when (oh - kh) and (ow - kw) are even, and ensure ih ∈ [0, 255] and iw ∈ [0, 255]. Skip invalid positions during accumulation.\n\nInput:\nmatA: float32, shape = (65536, 64)\nmatB: float32, shape = (64, 256)\n\nOutput:\nmatC_out: float32, shape = (65536, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 354, "task_name": "Matrix_Vector_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Vector_Multiplication\n\nTask Description:\nImplement a CUDA kernel for matrix-vector multiplication. The input matrix 'mvA_16' has dimensions 65536x4096 and data type float32. The input vector 'mvv_16' has length 4096 and data type float32. The output vector 'mvy_cuda_16' must have length 65536 and data type float32. Each element of the output vector should be computed as the dot product of the corresponding row of the matrix with the input vector. The kernel must correctly handle the specified dimensions and ensure each output element corresponds to exactly one row of the input matrix.\n\nInput:\nmvA_16: float32, shape = (65536, 4096)\nmvv_16: float32, shape = (4096,)\n\nOutput:\nmvy_cuda_16: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 352, "task_name": "Matrix_Vector_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Vector_Multiplication\n\nTask Description:\nImplement the Matrix_Vector_Multiplication kernel on GPU using CUDA. The kernel must multiply a 4096x4096 matrix (mvA_12) with a 4096-element vector (mvv_12), both float32 tensors, to produce a 4096-element output vector (mvy_cuda_12) of float32 values. Each element of the output vector is computed as the dot product between the corresponding row of the matrix and the input vector. The kernel must handle all 4096 rows and avoid out-of-bound memory accesses.\n\nInput:\nmvA_12: float32, shape = (4096, 4096)\nmvv_12: float32, shape = (4096,)\n\nOutput:\nmvy_cuda_12: float32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 359, "task_name": "Pairwise_Euclidean_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPairwise_Euclidean_Distance\n\nTask Description:\nImplement a CUDA kernel named Pairwise_Euclidean_Distance that computes the Euclidean distance between corresponding rows of two input matrices. Inputs are two 2D tensors: matA and matB, each with shape (65536, 1024) and float32 data type. For each row index i, compute the Euclidean distance between matA[i] and matB[i], defined as the square root of the sum of squared differences for all elements in that row. The output is a 1D tensor named matC_cuda with shape (65536,) and float32 data type, where each element at index i contains the Euclidean distance between row i of matA and row i of matB. The kernel must compute one distance per row independently.\n\nInput:\nmatA: float32, shape = (65536, 1024)\nmatB: float32, shape = (65536, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 318, "task_name": "Softmin", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmin\n\nTask Description:\nThe task is named 'Softmin'. The kernel must compute the softmin of each row in the input matrix matA (shape: 512 rows by 8192 columns, dtype: float32), producing an output matrix matC_out as a flattened 1D array of 4194304 float32 elements. The second input matrix matB (shape: 8192x8192, dtype: float32) must be ignored. The computation involves: for each row, finding the maximum value among the negatives of its elements, then computing exponentials of each negative element adjusted by this maximum, summing these exponentials, and finally calculating each output element as the ratio of its exponential to the sum. Each row must be processed independently.\n\nInput:\nmatA: float32, shape = (512, 8192)\nmatB: float32, shape = (8192, 8192)\n\nOutput:\nmatC_out: float32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 356, "task_name": "Pairwise_Euclidean_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPairwise_Euclidean_Distance\n\nTask Description:\nCompute the pairwise Euclidean distance between corresponding rows of two matrices. The input matrices matA and matB are both float32 tensors with shape (1024, 1024). The output matC_cuda must be a float32 tensor of shape (1024,) where each element at index i represents the Euclidean distance between row i of matA and row i of matB. The computation must calculate the L2 norm for each row independently, summing squared differences element-wise and taking the square root.\n\nInput:\nmatA: float32, shape = (1024, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 357, "task_name": "Pairwise_Euclidean_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPairwise_Euclidean_Distance\n\nTask Description:\nImplement a CUDA kernel named 'Pairwise_Euclidean_Distance' that computes the Euclidean distance between corresponding rows of two input matrices. The inputs are two float32 matrices (matA and matB) each with shape (4096, 1024). The output should be a float32 vector (matC_cuda) of length 4096, where each element represents the Euclidean distance between the corresponding rows of matA and matB. The kernel must process each row independently and compute the distance as the square root of the sum of squared element-wise differences between the two rows.\n\nInput:\nmatA: float32, shape = (4096, 1024)\nmatB: float32, shape = (4096, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 317, "task_name": "Softmin", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmin\n\nTask Description:\nImplement the Softmin kernel. The kernel should compute the softmin of each row in input matrix matA (shape: 1024x4096, float32). The softmin is defined as the softmax of the negative values. For each row, first find the maximum value of the negatives (equivalent to the minimum of the original row) for numerical stability. Then compute exponentials of each negative element minus this maximum, sum these exponentials across the row, and output each exponential divided by the sum. Input matrix matB (4096x4096, float32) should be ignored. Output matrix matC_out must match matA's shape (1024x4096, float32).\n\nInput:\nmatA: float32, shape = (1024, 4096)\nmatB: float32, shape = (4096, 4096)\n\nOutput:\nmatC_out: float32, shape = (1024, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 347, "task_name": "Elementwise_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Multiplication\n\nTask Description:\nImplement an Elementwise_Multiplication CUDA kernel. The kernel must multiply corresponding elements from two input tensors: mulA and mulB. Both inputs are float32 tensors with shape (4096, 2048). The output tensor mulC_cuda must be a float32 tensor of shape (8388608,) containing element-wise products. The kernel must handle all 8,388,608 elements independently without altering input data.\n\nInput:\nmulA: float32, shape = (4096, 2048)\nmulB: float32, shape = (4096, 2048)\n\nOutput:\nmulC_cuda: float32, shape = (8388608,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 358, "task_name": "Pairwise_Euclidean_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPairwise_Euclidean_Distance\n\nTask Description:\nCompute the pairwise Euclidean distance between corresponding rows of two matrices. The inputs are two matrices, matA and matB, each with 16384 rows and 1024 columns, of 32-bit floating point numbers. The output is a vector matC_cuda of length 16384, where each element is the Euclidean distance between the corresponding rows of matA and matB. The computation must be performed independently for each row and must handle the exact dimensions.\n\nInput:\nmatA: float32, shape = (16384, 1024)\nmatB: float32, shape = (16384, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 350, "task_name": "Elementwise_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Multiplication\n\nTask Description:\nTask: Elementwise_Multiplication. Perform element-wise multiplication of two input tensors. Inputs are two tensors named 'mulA' and 'mulB', both of data type float32 and shape (262144, 2048). Output is a tensor named 'mulC_cuda' with the same shape and data type. Each element in the output tensor must be the product of the corresponding elements in the input tensors. The kernel must handle the entire tensor and respect the shape and data type constraints.\n\nInput:\nmulA: float32, shape = (262144, 2048)\nmulB: float32, shape = (262144, 2048)\n\nOutput:\nmulC_cuda: float32, shape = (262144, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 326, "task_name": "Swish", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSwish\n\nTask Description:\nTask: Swish. Given two input matrices: matA of size 8192x1024 and matB of size 1024x1024, both of float32 data type. Compute the Swish activation function element-wise on matA, ignoring matB. The Swish function is defined as: for each element x in matA, output x * sigmoid(x), where sigmoid(x) = 1/(1+exp(-x)). The output matrix matC_out must have the same shape as matA (8192x1024) and float32 data type. The kernel must be implemented in CUDA for a GPU and handle the entire matrix.\n\nInput:\nmatA: float32, shape = (8192, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (8192, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 345, "task_name": "Elementwise_Addition", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Addition\n\nTask Description:\nTask: Elementwise_Addition. Compute the elementwise addition of two input tensors. Inputs are tensors addA and addB, both with data type float32 and shape (262144, 2048). The output tensor addC_cuda must have identical shape and data type. Each element in the output must be the sum of corresponding elements from addA and addB. The kernel must process all elements without altering shapes or data types.\n\nInput:\naddA: float32, shape = (262144, 2048)\naddB: float32, shape = (262144, 2048)\n\nOutput:\naddC_cuda: float32, shape = (262144, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 333, "task_name": "Multi-Head_Self-Attention", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMulti-Head_Self-Attention\n\nTask Description:\nMulti-Head Self-Attention: Given three input tensors Q, K, and V, each with shape (256, 128) and float32 data type, compute the multi-head self-attention output. The model dimension (128) must be divided into 8 heads, each with 16 features. For each token (256 tokens) and each head, compute attention scores by taking the dot product between the token's query vector and all tokens' key vectors for that head, scaled by the inverse square root of the head dimension (16). Apply softmax to normalize the scores, then compute a weighted sum of all tokens' value vectors using these weights. Concatenate the outputs from all heads to form the final output tensor of shape (256, 128). The kernel must respect the fixed head count (8) and head dimension (16).\n\nInput:\nQ: float32, shape = (256, 128)\nK: float32, shape = (256, 128)\nV: float32, shape = (256, 128)\n\nOutput:\nout: float32, shape = (256, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 353, "task_name": "Matrix_Vector_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Vector_Multiplication\n\nTask Description:\nImplement a CUDA kernel for matrix-vector multiplication. The input matrix has dimensions 16384x4096 with float32 data type, and the input vector has length 4096 with float32 data type. The output should be a vector of length 16384 with float32 data type, where each element is the dot product of a corresponding row from the matrix and the input vector. The kernel must accurately compute results within a tolerance of 1e-3 compared to reference implementations.\n\nInput:\nmvA_14: float32, shape = (16384, 4096)\nmvv_14: float32, shape = (4096,)\n\nOutput:\nmvy_cuda_14: float32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 320, "task_name": "Softmin", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftmin\n\nTask Description:\nCompute the Softmin kernel. The inputs are two matrices: matA of size 8192x1024 and matB of size 1024x1024, both of float32 data type. The output is a matrix matC_out of size 8192x1024 and float32 data type. The kernel must compute the softmin of each row of matA independently, ignoring matB. The softmin of a row is computed by taking the negative of each element in the row, then applying the softmax function to the resulting vector. To ensure numerical stability, the kernel must subtract the maximum value of the negative row elements from each exponent. The output for each row must be a valid probability distribution (non-negative and summing to 1).\n\nInput:\nmatA: float32, shape = (8192, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (8192, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 334, "task_name": "Multi-Head_Self-Attention", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMulti-Head_Self-Attention\n\nTask Description:\nMulti-Head_Self-Attention: Given three input tensors Q, K, and V, each with shape (384, 256) and float32 data type, compute the output tensor with the same shape and data type. The kernel must split the feature dimension into 16 heads, compute scaled dot-product attention scores for each head, apply softmax normalization, and compute a weighted sum of value vectors. The feature dimension (256) must be divisible by the number of heads (16), and attention scores must be scaled by 1/sqrt(16).\n\nInput:\nQ: float32, shape = (384, 256)\nK: float32, shape = (384, 256)\nV: float32, shape = (384, 256)\n\nOutput:\nout: float32, shape = (384, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 360, "task_name": "Pairwise_Euclidean_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPairwise_Euclidean_Distance\n\nTask Description:\nImplement a CUDA kernel named 'Pairwise_Euclidean_Distance' that computes Euclidean distances between corresponding rows of two matrices. The input matrices matA and matB are both float32 tensors with shape (262144, 1024). The output matC_cuda must be a float32 tensor of shape (262144,) where each element represents the Euclidean distance between the corresponding rows of matA and matB. The kernel must compute the distance as the square root of the sum of squared differences between elements in the same column position for each row pair.\n\nInput:\nmatA: float32, shape = (262144, 1024)\nmatB: float32, shape = (262144, 1024)\n\nOutput:\nmatC_cuda: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 322, "task_name": "Softsign", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nSoftsign\n\nTask Description:\nTask: Softsign. Implement a CUDA kernel that computes the Softsign activation function element-wise on the input matrix `matA`. The Softsign function is defined as: for each element x in `matA`, compute x / (1 + absolute value of x). The kernel should ignore the second input matrix `matB`. The input `matA` is a 2D tensor of 16384 rows and 1024 columns with float32 data type. The input `matB` is a 2D tensor of 1024 rows and 1024 columns with float32 data type. The output `matC_out` is a 2D tensor of 16384 rows and 1024 columns with float32 data type. The kernel must compute each output element independently and the output must have the same shape as `matA`.\n\nInput:\nmatA: float32, shape = (16384, 1024)\nmatB: float32, shape = (1024, 1024)\n\nOutput:\nmatC_out: float32, shape = (16384, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 335, "task_name": "Multi-Head_Self-Attention", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMulti-Head_Self-Attention\n\nTask Description:\nCompute multi-head self-attention. The inputs are three 512x512 float32 tensors: Q (queries), K (keys), and V (values). The output is a 512x512 float32 tensor. Split the inputs into 16 heads along the feature dimension (each head has 32 features). For each head, compute attention scores by taking the scaled dot product of queries and keys (scale factor: 1/sqrt(32)), apply softmax to the scores, and compute a weighted sum of values using these weights. Concatenate the outputs from all heads. The kernel must process 512 rows and 512 columns.\n\nInput:\nQ: float32, shape = (512, 512)\nK: float32, shape = (512, 512)\nV: float32, shape = (512, 512)\n\nOutput:\nout: float32, shape = (512, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 361, "task_name": "FIR_Filtering", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFIR_Filtering\n\nTask Description:\nImplement a CUDA kernel for FIR filtering. The input consists of a signal tensor with 65536 float32 elements and a filter coefficients tensor with 64 float32 elements. The output is a filtered signal tensor with 65536 float32 elements. For each output index i, compute the sum of the product between the filter coefficients and the corresponding input signal values starting from index i down to max(0, i-63). Boundary conditions must be handled such that indices below 0 are not accessed.\n\nInput:\nsignal_input: float32, shape = (65536,)\nfilter_coeffs: float32, shape = (64,)\n\nOutput:\nsignal_output: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 362, "task_name": "FIR_Filtering", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFIR_Filtering\n\nTask Description:\nImplement a Finite Impulse Response (FIR) filter kernel named FIR_Filtering. The kernel should process an input signal tensor of 131072 float32 values using filter coefficients of 64 float32 values. Each output element at position i must be computed as the weighted sum of the current and previous input elements using the filter coefficients, with the constraint that only valid indices (i-k ≥ 0) are considered. The output should be a tensor of 131072 float32 values.\n\nInput:\nsignal_input: float32, shape = (131072,)\nfilter_coeffs: float32, shape = (64,)\n\nOutput:\nsignal_output: float32, shape = (131072,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 371, "task_name": "Rowwise_Minimum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Minimum\n\nTask Description:\nTask: Rowwise_Minimum. Given a 2D input tensor 'matA' of shape (1024, 2048) with float32 data type, compute the minimum value for each row. The output should be a 1D tensor 'matC_out' of shape (1024,) with float32 data type, where each element represents the minimum value of the corresponding row in the input. The kernel must process each row independently and handle the fixed dimensions (1024 rows and 2048 columns).\n\nInput:\nmatA: float32, shape = (1024, 2048)\n\nOutput:\nmatC_out: float32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 373, "task_name": "Rowwise_Minimum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Minimum\n\nTask Description:\nImplement a CUDA kernel named Rowwise_Minimum that computes the minimum value for each row in a 2D input matrix. The input tensor 'matA' is a float32 matrix with dimensions 16384 rows by 2048 columns. The output tensor 'matC_out' must be a float32 vector with 16384 elements, where each element represents the minimum value of the corresponding row in the input matrix. The kernel must process each row independently and correctly handle all values within the matrix dimensions.\n\nInput:\nmatA: float32, shape = (16384, 2048)\n\nOutput:\nmatC_out: float32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 364, "task_name": "FIR_Filtering", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFIR_Filtering\n\nTask Description:\nTask: FIR_Filtering. Implement a Finite Impulse Response (FIR) filter on an input signal using given filter coefficients. The input signal is a 1D array of 524288 float32 values. The filter coefficients are a 1D array of 64 float32 values. The output is a 1D array of 524288 float32 values representing the filtered signal. For each output index i, compute the sum of products between filter coefficients and signal values from index i down to i-63, but only for valid indices (i.e., ignore negative indices). The kernel must handle boundary conditions where fewer than 64 coefficients are used for the initial output indices.\n\nInput:\nsignal_input: float32, shape = (524288,)\nfilter_coeffs: float32, shape = (64,)\n\nOutput:\nsignal_output: float32, shape = (524288,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 369, "task_name": "Rowwise_Maximum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Maximum\n\nTask Description:\nImplement a CUDA kernel named Rowwise_Maximum that computes the maximum value for each row in a 2D input matrix. The input matrix 'matA' has dimensions 65536 rows by 2048 columns with float32 data type. The output 'matC_out' must be a 1D tensor of 65536 float32 elements, where each element represents the maximum value of its corresponding input row. Each row must be processed independently, and the kernel should handle the full matrix dimensions.\n\nInput:\nmatA: float32, shape = (65536, 2048)\n\nOutput:\nmatC_out: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 365, "task_name": "FIR_Filtering", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFIR_Filtering\n\nTask Description:\nFIR_Filtering task: Implement a finite impulse response (FIR) filter kernel. The kernel should compute each output element as the weighted sum of the current and previous input samples using filter coefficients. Inputs are a 1D signal tensor with 1048576 float32 elements and a filter coefficient tensor with 64 float32 elements. Output is a 1D tensor with 1048576 float32 elements. For each output index i, compute the sum of filter_coeffs[j] * signal_input[i-j] for j from 0 to min(i,63), ensuring boundary handling when i < 63.\n\nInput:\nsignal_input: float32, shape = (1048576,)\nfilter_coeffs: float32, shape = (64,)\n\nOutput:\nsignal_output: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 376, "task_name": "Rowwise_Variance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Variance\n\nTask Description:\nImplement a CUDA kernel named Rowwise_Variance that computes the variance for each row of a 1024×2048 input matrix of 32-bit floats. The output should be a vector of 1024 elements where each element represents the population variance (unbiased=False) of its corresponding row. The kernel must correctly calculate the mean for each row first, then compute the average squared deviation from that mean.\n\nInput:\nmatA: float32, shape = (1024, 2048)\n\nOutput:\nmatC_out: float32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 368, "task_name": "Rowwise_Maximum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Maximum\n\nTask Description:\nCompute the maximum value for each row in a 2D matrix. The input matrix 'matA' has dimensions 16384 rows by 2048 columns and is of type float32. The output 'matC_out' is a vector of 16384 float32 values, each being the maximum of the corresponding row. The kernel must process each row independently and the results must be accurate within a tolerance of 1e-5.\n\nInput:\nmatA: float32, shape = (16384, 2048)\n\nOutput:\nmatC_out: float32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 363, "task_name": "FIR_Filtering", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFIR_Filtering\n\nTask Description:\nImplement a CUDA kernel for FIR_Filtering. The kernel applies a finite impulse response (FIR) filter to an input signal using given filter coefficients. The input signal is a 1D float32 tensor of size 262144, and filter coefficients are a 1D float32 tensor of size 64. The output should be a 1D float32 tensor of size 262144. For each output index i, compute the sum of filter_coeffs[k] multiplied by signal_input[i-k] for k ranging from 0 to 63, but only when i-k is non-negative (i.e., valid indices).\n\nInput:\nsignal_input: float32, shape = (262144,)\nfilter_coeffs: float32, shape = (64,)\n\nOutput:\nsignal_output: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 372, "task_name": "Rowwise_Minimum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Minimum\n\nTask Description:\nImplement a CUDA kernel named Rowwise_Minimum that computes the minimum value for each row of a 2D input tensor. The input tensor matA has dimensions 4096 rows by 2048 columns with float32 data type. The output tensor matC_out must be a 1D vector of 4096 float32 elements, where each element corresponds to the minimum value in the respective row of matA. The kernel must handle exactly 4096 rows and 2048 columns, and preserve numerical precision within a tolerance of 1e-5.\n\nInput:\nmatA: float32, shape = (4096, 2048)\n\nOutput:\nmatC_out: float32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 370, "task_name": "Rowwise_Maximum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Maximum\n\nTask Description:\nImplement the Rowwise_Maximum CUDA kernel. The kernel should compute the maximum value in each row of a 2D input tensor named 'matA' with dimensions 262144 rows by 2048 columns, where all elements are 32-bit floating-point numbers. The output should be a 1D tensor named 'matC_out' with 262144 elements, containing the maximum value of each corresponding row. The kernel must process each row independently and ensure numerical accuracy within acceptable tolerance.\n\nInput:\nmatA: float32, shape = (262144, 2048)\n\nOutput:\nmatC_out: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 379, "task_name": "Rowwise_Variance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Variance\n\nTask Description:\nImplement a CUDA kernel named 'Rowwise_Variance' that computes the variance for each row in a 2D input tensor. The input tensor 'matA' has dimensions (65536, 2048) and contains float32 values. The output tensor 'matC_out' must be a 1D float32 tensor of size (65536,), where each element represents the variance of its corresponding row. The variance calculation must first compute the row mean, then sum the squared differences from this mean, and finally divide by the row length (2048) without bias correction.\n\nInput:\nmatA: float32, shape = (65536, 2048)\n\nOutput:\nmatC_out: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 331, "task_name": "Multi-Head_Self-Attention", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMulti-Head_Self-Attention\n\nTask Description:\nImplement the Multi-Head Self-Attention operation. The inputs are three tensors: Q, K, and V, each of shape (64, 32) and data type float32. The output is a tensor of shape (64, 32) and data type float32. The operation must split the 32-dimensional model into 4 heads, each of 8 dimensions. For each head, compute the scaled dot-product attention: for each row in Q (corresponding to a token), compute attention scores by taking the dot product of the row's query vector (from Q) and all key vectors (from K) in the same head, then scale each score by 1 divided by the square root of 8. Apply softmax over the attention scores for each row to get probabilities. Then, for each row, compute a weighted sum of the value vectors (from V) in the same head using these probabilities. Concatenate the outputs from all 4 heads to form the final output. The kernel must respect the constraint that the model dimension (32) is divisible by the number of heads (4).\n\nInput:\nQ: float32, shape = (64, 32)\nK: float32, shape = (64, 32)\nV: float32, shape = (64, 32)\n\nOutput:\nout: float32, shape = (64, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 378, "task_name": "Rowwise_Variance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Variance\n\nTask Description:\nImplement a CUDA kernel called Rowwise_Variance that computes the population variance for each row of a 2D input matrix. The input matrix 'matA' is a float32 tensor with dimensions (16384, 2048). The output 'matC_out' must be a float32 vector of length 16384, where each element represents the variance of the corresponding row. The variance calculation must first compute the mean of each row, then compute the average of squared differences from that mean, using the entire row length as the denominator (unbiased=False).\n\nInput:\nmatA: float32, shape = (16384, 2048)\n\nOutput:\nmatC_out: float32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 351, "task_name": "Matrix_Vector_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Vector_Multiplication\n\nTask Description:\nTask: Matrix_Vector_Multiplication. Compute the product of a matrix and a vector. The input matrix has dimensions 1024 rows by 4096 columns and data type float32. The input vector has 4096 elements and data type float32. The output must be a vector of 1024 elements and data type float32. Each element of the output vector is the dot product of the corresponding row of the matrix and the input vector.\n\nInput:\nmvA_10: float32, shape = (1024, 4096)\nmvv_10: float32, shape = (4096,)\n\nOutput:\nmvy_cuda_10: float32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 367, "task_name": "Rowwise_Maximum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Maximum\n\nTask Description:\nTask: Rowwise_Maximum. Compute the maximum value for each row in a 2D input matrix. The input is a float32 tensor of shape (4096, 2048). The output should be a float32 vector of length 4096, where each element represents the maximum value in the corresponding row of the input matrix. The kernel must process each row independently and handle all elements within a row to determine the maximum value.\n\nInput:\nmatA: float32, shape = (4096, 2048)\n\nOutput:\nmatC_out: float32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 366, "task_name": "Rowwise_Maximum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Maximum\n\nTask Description:\nTask: Rowwise_Maximum. Given a 2D input matrix 'matA' of shape (1024, 2048) with float32 elements, compute the maximum value for each row. The output 'matC_out' must be a vector of shape (1024,) with float32 elements, where each element represents the maximum value of the corresponding row in the input matrix. The kernel must process each row independently and handle the fixed dimensions correctly.\n\nInput:\nmatA: float32, shape = (1024, 2048)\n\nOutput:\nmatC_out: float32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 380, "task_name": "Rowwise_Variance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Variance\n\nTask Description:\nImplement a CUDA kernel named 'Rowwise_Variance' that computes the variance for each row of a 2D input matrix. The input is a float32 tensor with shape (262144, 2048). For each row, calculate the mean of its 2048 elements, then compute the average of squared differences from this mean (using biased variance calculation, i.e., dividing by N=2048). The output must be a float32 tensor with shape (262144,) containing one variance value per row. Ensure the kernel processes rows independently.\n\nInput:\nmatA: float32, shape = (262144, 2048)\n\nOutput:\nmatC_out: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 377, "task_name": "Rowwise_Variance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Variance\n\nTask Description:\nImplement the Rowwise_Variance CUDA kernel that computes the variance for each row of a 2D input matrix. The input is a float32 tensor named matA with shape (4096, 2048), representing a matrix of 4096 rows and 2048 columns. The output should be a float32 tensor named matC_out with shape (4096,), where each element corresponds to the variance of its respective row. The variance must be calculated using the biased estimator (dividing by N instead of N-1). Each row must be processed independently, and the kernel should respect the fixed input dimensions.\n\nInput:\nmatA: float32, shape = (4096, 2048)\n\nOutput:\nmatC_out: float32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 349, "task_name": "Elementwise_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Multiplication\n\nTask Description:\nImplement an Elementwise_Multiplication CUDA kernel. The kernel takes two input tensors, mulA and mulB, each of shape (65536, 2048) and data type float32. The output tensor, mulC_cuda, must have the same shape (65536, 2048) and data type float32. The kernel must compute the element-wise product: each element in mulC_cuda is the product of the corresponding elements in mulA and mulB. The kernel must handle the entire tensor and respect the memory layout.\n\nInput:\nmulA: float32, shape = (65536, 2048)\nmulB: float32, shape = (65536, 2048)\n\nOutput:\nmulC_cuda: float32, shape = (65536, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 387, "task_name": "Mean_Square_Error", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMean_Square_Error\n\nTask Description:\nTask: Mean_Square_Error. Compute the mean squared error between two input vectors. Inputs are two float32 vectors of shape (4096,): predictions and targets. Output is a single float32 scalar representing the average of squared element-wise differences. The kernel must calculate the sum of squared differences across all elements and divide by the total number of elements (4096).\n\nInput:\nmse_preds: float32, shape = (4096,)\nmse_targets: float32, shape = (4096,)\n\nOutput:\nmse_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 346, "task_name": "Elementwise_Multiplication", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nElementwise_Multiplication\n\nTask Description:\nImplement an elementwise multiplication kernel named Elementwise_Multiplication. The kernel must multiply corresponding elements from two input tensors: mulA and mulB, both float32 arrays of shape (1024, 2048). The output tensor mulC_cuda must be a flattened float32 array of length 2097152 containing the elementwise products. The kernel must handle all elements in the input tensors and maintain strict numerical correspondence between input positions and output values.\n\nInput:\nmulA: float32, shape = (1024, 2048)\nmulB: float32, shape = (1024, 2048)\n\nOutput:\nmulC_cuda: float32, shape = (2097152,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 386, "task_name": "Mean_Square_Error", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMean_Square_Error\n\nTask Description:\nTask: Mean_Square_Error. Compute the mean squared error between two input vectors. The inputs are two vectors, each with 1024 elements of type float32, representing predictions and targets. The output is a single scalar of type float32, which is the average of the squared differences between corresponding elements. The vectors must be of the same length, and the computation must sum all squared differences before dividing by the number of elements.\n\nInput:\nmse_preds: float32, shape = (1024,)\nmse_targets: float32, shape = (1024,)\n\nOutput:\nmse_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 390, "task_name": "Mean_Square_Error", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMean_Square_Error\n\nTask Description:\nTask: Mean_Square_Error. Compute the mean squared error between two input vectors. The inputs are two float32 tensors, each of shape (262144,). The output is a single float32 scalar (shape (1,)) representing the mean of the squared differences between corresponding elements in the two vectors. The kernel must compute the squared difference for each element, sum these squared differences, and then divide the total sum by the number of elements to obtain the mean.\n\nInput:\nmse_preds: float32, shape = (262144,)\nmse_targets: float32, shape = (262144,)\n\nOutput:\nmse_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 389, "task_name": "Mean_Square_Error", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMean_Square_Error\n\nTask Description:\nImplement a CUDA kernel for Mean Square Error calculation. The kernel should compute the mean of the squared differences between two input vectors. The inputs are two float32 tensors named 'mse_preds' and 'mse_targets', each with shape (65536,). The output is a single float32 scalar in a tensor named 'mse_out' with shape (1,). The kernel must calculate the squared difference for each element pair, accumulate these squared differences using atomic addition for thread safety, and finally compute the mean by dividing the total sum by the number of elements (65536).\n\nInput:\nmse_preds: float32, shape = (65536,)\nmse_targets: float32, shape = (65536,)\n\nOutput:\nmse_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 400, "task_name": "Chamfer_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nChamfer_Distance\n\nTask Description:\nImplement the Chamfer_Distance kernel to compute the average Chamfer distance between two sets of 2D points across multiple batches. The inputs are two float32 tensors: input_points_a with shape (8192, 128, 2) and input_points_b with shape (8192, 256, 2). The output is a single float32 value representing the average Chamfer distance across all batches. For each batch, compute two sums: (1) the minimum squared Euclidean distance from each point in set A to any point in set B, averaged over set A points; (2) the minimum squared Euclidean distance from each point in set B to any point in set A, averaged over set B points. The Chamfer distance per batch is the sum of these two averages. Accumulate this value across all batches and divide by the batch count to produce the final output.\n\nInput:\ninput_points_a: float32, shape = (8192, 128, 2)\ninput_points_b: float32, shape = (8192, 256, 2)\n\nOutput:\noutput_chamfer_distance: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 397, "task_name": "Chamfer_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nChamfer_Distance\n\nTask Description:\nImplement the Chamfer_Distance kernel. The task involves computing the Chamfer distance between two sets of 2D points across multiple batches. Input A is a float32 tensor of shape (1024, 128, 2) representing 1024 batches with 128 points each. Input B is a float32 tensor of shape (1024, 256, 2) with 256 points per batch. For each batch, calculate two components: first, for every point in set A, find its nearest point in set B using squared Euclidean distance and sum these minima; second, repeat the process for every point in set B to find its nearest in set A. Average the first sum by the number of points in A and the second by points in B. Sum these averages to get the Chamfer distance per batch. Finally, compute the mean Chamfer distance across all batches and output it as a single float32 value in a tensor of shape (1,).\n\nInput:\ninput_points_a: float32, shape = (1024, 128, 2)\ninput_points_b: float32, shape = (1024, 256, 2)\n\nOutput:\noutput_chamfer_distance: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 374, "task_name": "Rowwise_Minimum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Minimum\n\nTask Description:\nCompute the row-wise minimum of a 2D matrix. The input is a tensor named 'matA' with 65536 rows and 2048 columns of type float32. The output is a tensor named 'matC_out' with 65536 elements of type float32, where each element is the minimum value of the corresponding row in the input matrix. The kernel must process each row independently to find the minimum value in that row.\n\nInput:\nmatA: float32, shape = (65536, 2048)\n\nOutput:\nmatC_out: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 381, "task_name": "Color_Conversion", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nColor_Conversion\n\nTask Description:\nColor_Conversion task: Convert an RGB image to grayscale using the standard luminance formula. Input is a 256x256x3 float32 tensor representing RGB values. Output must be a 256x256 float32 tensor containing grayscale values. The conversion must use fixed weights: 0.299 for red, 0.587 for green, and 0.114 for blue. Each output pixel must be computed independently from its corresponding RGB input pixel.\n\nInput:\nrgb_input: float32, shape = (256, 256, 3)\n\nOutput:\ngray_output: float32, shape = (256, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 388, "task_name": "Mean_Square_Error", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMean_Square_Error\n\nTask Description:\nTask: Mean_Square_Error. Compute the mean square error between two input vectors of 16384 float32 elements. The inputs are two vectors: mse_preds and mse_targets, both of shape (16384,) and dtype float32. The output is a scalar tensor of shape (1,) and dtype float32, which is the mean of the squared differences between corresponding elements in the input vectors. The kernel must handle vectors of exactly 16384 elements and compute the mean correctly by dividing the total sum of squared differences by 16384.\n\nInput:\nmse_preds: float32, shape = (16384,)\nmse_targets: float32, shape = (16384,)\n\nOutput:\nmse_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 284, "task_name": "Conv_Transposed_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nConv_Transposed_3D\n\nTask Description:\nTask: Conv_Transposed_3D. Perform a 3D transposed convolution operation. Input matA is a float32 tensor of shape (55296, 16), representing a 5D tensor with dimensions (1, 16, 24, 48, 48). Input matB is a float32 tensor of shape (16, 128), representing a 5D weight tensor with dimensions (16, 16, 2, 2, 2). Output matC_out must be a float32 tensor of shape (55296, 128), corresponding to a 5D output tensor of shape (1, 16, 48, 96, 96). The kernel must compute each output element by summing over input channels and kernel positions (depth, height, width) with stride 2 in all spatial dimensions. Input spatial indices must be derived from output indices by reversing the transposed convolution operation, ensuring indices are integers (via even difference checks) and within valid spatial bounds (0-23 for depth, 0-47 for height/width).\n\nInput:\nmatA: float32, shape = (55296, 16)\nmatB: float32, shape = (16, 128)\n\nOutput:\nmatC_out: float32, shape = (55296, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 384, "task_name": "Color_Conversion", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nColor_Conversion\n\nTask Description:\nTask: Color_Conversion. Convert an RGB image to grayscale using a specific luminance formula. The input is a 2048x2048 image with 3 color channels (red, green, blue) as float32 values. The output should be a single-channel 2048x2048 grayscale image flattened into a 4194304-element float32 tensor. The conversion must use the weights: red channel multiplied by 0.299, green by 0.587, and blue by 0.114. Ensure the kernel handles all pixels within the image boundaries.\n\nInput:\nrgb_input: float32, shape = (2048, 2048, 3)\n\nOutput:\ngray_output: float32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 375, "task_name": "Rowwise_Minimum", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nRowwise_Minimum\n\nTask Description:\nTask: Rowwise_Minimum. Compute the minimum value for each row of a matrix. The input is a tensor named 'matA' with shape (262144, 2048) and data type float32. The output is a tensor named 'matC_out' with shape (262144,) and data type float32. Each element of the output must be the minimum value of the corresponding row in the input matrix. The computation for each row is independent.\n\nInput:\nmatA: float32, shape = (262144, 2048)\n\nOutput:\nmatC_out: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 383, "task_name": "Color_Conversion", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nColor_Conversion\n\nTask Description:\nTask: Color_Conversion. Convert an input RGB image to a grayscale image. The input is a 3D tensor of shape (1024, 1024, 3) with float32 values representing red, green, and blue channels. The output is a 2D tensor of shape (1024, 1024) with float32 values. Each output pixel must be computed using the formula: grayscale = 0.299 * red + 0.587 * green + 0.114 * blue. The kernel must process each pixel independently.\n\nInput:\nrgb_input: float32, shape = (1024, 1024, 3)\n\nOutput:\ngray_output: float32, shape = (1024, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 391, "task_name": "Ball_Query", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBall_Query\n\nTask Description:\nTask: Ball_Query. The kernel should compute indices of points within a spherical shell around center points. For each center in a batch, find up to 16 points from the input point cloud where the squared Euclidean distance to the center is between 1.0 and 100.0 (inclusive), or exactly zero. Inputs: 'input_xyz' (float32, shape [16, 4096, 3]) and 'input_center' (float32, shape [16, 64, 3]). Output: 'output_ball_query_idx' (int32, shape [16, 64, 16]) storing point indices. If no points meet criteria, output indices default to zero. Each center must be processed independently.\n\nInput:\ninput_xyz: float32, shape = (16, 4096, 3)\ninput_center: float32, shape = (16, 64, 3)\n\nOutput:\noutput_ball_query_idx: int32, shape = (16, 64, 16)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 385, "task_name": "Color_Conversion", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nColor_Conversion\n\nTask Description:\nTask: Color_Conversion. Convert an RGB image to grayscale using a standard luminance formula. The input is a 3D tensor of shape (4096, 4096, 3) with float32 values representing the RGB channels. The output must be a 2D tensor of shape (4096, 4096) with float32 values representing the grayscale result. The conversion must use fixed weights: red channel multiplied by 0.299, green by 0.587, and blue by 0.114.\n\nInput:\nrgb_input: float32, shape = (4096, 4096, 3)\n\nOutput:\ngray_output: float32, shape = (4096, 4096)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 412, "task_name": "Furthest_Point_Sample_with_Dist", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample_with_Dist\n\nTask Description:\nFurthest Point Sample with Dist: Implement a kernel that performs iterative furthest point sampling based on pairwise distances. The input is a 3D tensor of float32 values with shape (8, 256, 256), representing 8 batches where each batch contains a 256x256 matrix of distances between every point pair. The output should be a 2D tensor of int32 values with shape (8, 32), containing indices of 32 sampled points per batch. The kernel must start by selecting point 0 as the first sample in each batch. For subsequent samples, it must update a temporary distance array to track the minimum distance from each point to the current selected set and select the point with the maximum such distance. The kernel must handle batches independently and maintain the invariant that each new sample maximizes the minimum distance to existing samples.\n\nInput:\ninput_point_dists: float32, shape = (8, 256, 256)\n\nOutput:\noutput_sampled_indices: int32, shape = (8, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 382, "task_name": "Color_Conversion", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nColor_Conversion\n\nTask Description:\nTask: Color_Conversion. Convert an RGB image to a grayscale image. The input is a 3D tensor of shape (512, 512, 3) and data type float32, representing an RGB image. The output is a 1D tensor of shape (262144,) and data type float32, which is the flattened grayscale image. The conversion must use the standard luminance formula: 0.299 * red + 0.587 * green + 0.114 * blue for each pixel. Each output pixel is computed independently from the corresponding input pixel.\n\nInput:\nrgb_input: float32, shape = (512, 512, 3)\n\nOutput:\ngray_output: float32, shape = (262144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 413, "task_name": "Furthest_Point_Sample_with_Dist", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample_with_Dist\n\nTask Description:\nImplement the Furthest_Point_Sample_with_Dist kernel. Given a 3D float32 tensor of shape (8, 512, 512) representing distance matrices between points for 8 batches, sample 32 points per batch where each new point is the furthest from all previously selected points. The output should be an int32 tensor of shape (8, 32) containing the indices of sampled points. The algorithm must start with index 0 as the first sample and iteratively select points that maximize the minimum distance to existing samples.\n\nInput:\ninput_point_dists: float32, shape = (8, 512, 512)\n\nOutput:\noutput_sampled_indices: int32, shape = (8, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 404, "task_name": "Dynamic_Voxelize", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDynamic_Voxelize\n\nTask Description:\nImplement a CUDA kernel named Dynamic_Voxelize that assigns each input point to a voxel in a predefined 3D grid. The input is a tensor with shape (8192, 5) of float32 values, where only the first three elements (x, y, z) are used. The grid spans from (0.0, 0.0, 0.0) to (5.0, 5.0, 5.0) with voxel size 1.0 in all dimensions. For each point, compute integer voxel coordinates by subtracting the grid minimum and dividing by the voxel size. If the point lies outside the grid boundaries in any dimension, output (-1, -1, -1). Otherwise, output the voxel coordinates as (z, y, x). The output tensor must be of shape (8192, 3) with int32 data type.\n\nInput:\ninput_points: float32, shape = (8192, 5)\n\nOutput:\noutput_dynamic_voxelize_coordinates: int32, shape = (8192, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 407, "task_name": "Furthest_Point_Sample", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample\n\nTask Description:\nTask name: Furthest_Point_Sample. Perform furthest point sampling on batches of 3D points to select a subset of points maximally distant from each other. The input is a float32 tensor of shape (8, 256, 3), representing 8 batches of 256 points with 3D coordinates. The output is an int32 tensor of shape (8, 32) containing indices of the selected points in each batch. Constraints: Each batch must be processed independently, starting with index 0 as the first sample. For each subsequent sample, update the minimum distance of all points to the current sample set and select the point with the maximum updated distance. Exactly 32 points must be sampled per batch.\n\nInput:\ninput_points: float32, shape = (8, 256, 3)\n\nOutput:\noutput_sampled_indices: int32, shape = (8, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 416, "task_name": "Group_Points", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGroup_Points\n\nTask Description:\nImplement the Group_Points kernel. Given an input_features tensor of shape (16, 4, 1024) containing float32 values and an input_indices tensor of shape (16, 32, 32) containing int32 indices, produce an output_grouped_features tensor of shape (16, 4, 32, 32) with float32 values. For each batch (16), channel (4), point (32), and sample (32), select the feature value from input_features using the index specified in input_indices. The indices must be within valid range [0, 1023] for the last dimension of input_features.\n\nInput:\ninput_features: float32, shape = (16, 4, 1024)\ninput_indices: int32, shape = (16, 32, 32)\n\nOutput:\noutput_grouped_features: float32, shape = (16, 4, 32, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 408, "task_name": "Furthest_Point_Sample", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample\n\nTask Description:\nImplement the Furthest_Point_Sample kernel. The kernel processes a batch of 8 point clouds, each containing 512 points in 3D space (x, y, z). For each point cloud, the kernel should select 32 points that are maximally distant from each other. The algorithm starts by selecting the first point (index 0) and then iteratively selects the point with the maximum minimum distance to the already selected set. The input is a float32 tensor of shape (8, 512, 3). The output is an int32 tensor of shape (8, 32) containing the indices of the selected points for each batch. The kernel must use a temporary distance array per batch, initialized to a large value, and update it at each step by taking the minimum between the current distance and the distance to the last selected point. The next selected point is the one with the maximum value in this temporary array.\n\nInput:\ninput_points: float32, shape = (8, 512, 3)\n\nOutput:\noutput_sampled_indices: int32, shape = (8, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 418, "task_name": "Group_Points", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGroup_Points\n\nTask Description:\nTask name: Group_Points. The kernel should group feature points using indices. Inputs include a feature tensor of shape (16, 4, 1024) with float32 data type and an indices tensor of shape (16, 32, 128) with int32 data type. The output is a grouped feature tensor of shape (16, 4, 32, 128) with float32 data type. For each batch, channel, point, and sample, the kernel must use the index value from the input indices tensor to select the corresponding feature value from the input feature tensor and place it in the output tensor. Indices must be within the valid range [0, 1023].\n\nInput:\ninput_features: float32, shape = (16, 4, 1024)\ninput_indices: int32, shape = (16, 32, 128)\n\nOutput:\noutput_grouped_features: float32, shape = (16, 4, 32, 128)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 427, "task_name": "Three_NN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nThree_NN\n\nTask Description:\nImplement the Three_NN kernel to compute the three nearest neighbors for each point in a set of unknown points from a set of known points. The input includes two 3D point clouds: 'input_unknown_points' of shape (16, 512, 3) and 'input_known_points' of shape (16, 128, 3), both with float32 data type. The output 'output_three_nn_idx' must be an int32 tensor of shape (16, 512, 3) storing the indices of the three closest known points for each unknown point. For each batch and unknown point, calculate squared Euclidean distances to all known points, maintain the three smallest distances, and track their indices. The kernel must process batches independently.\n\nInput:\ninput_unknown_points: float32, shape = (16, 512, 3)\ninput_known_points: float32, shape = (16, 128, 3)\n\nOutput:\noutput_three_nn_idx: int32, shape = (16, 512, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 395, "task_name": "Ball_Query", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBall_Query\n\nTask Description:\nBall_Query: Implement a CUDA kernel that, for each center point in a batch, finds up to 16 neighboring points within a spherical shell defined by inner radius 1.0 and outer radius 10.0. The input consists of two float32 tensors: input_xyz with shape (16, 4096, 3) representing point coordinates, and input_center with shape (16, 1024, 3) representing center points. The output is an int32 tensor with shape (16, 1024, 16) storing indices of qualifying points. For each center, if no points are found, output indices remain zero; if points are found, initialize all 16 indices to the first qualifying point and sequentially overwrite indices with subsequent points until 16 are recorded.\n\nInput:\ninput_xyz: float32, shape = (16, 4096, 3)\ninput_center: float32, shape = (16, 1024, 3)\n\nOutput:\noutput_ball_query_idx: int32, shape = (16, 1024, 16)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 392, "task_name": "Ball_Query", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBall_Query\n\nTask Description:\nBall_Query: Implement a CUDA kernel that processes batches of 3D points to find neighboring points within a spherical shell around center points. For each of 16 batches, the input includes 4096 points (float32, shape [16,4096,3]) and 128 center points (float32, shape [16,128,3]). The kernel must output a tensor (int32, shape [16,128,16]) containing indices of up to 16 neighboring points per center that satisfy two conditions: either exactly coinciding with the center or having Euclidean distance squared between 1.0 and 100.0. When points are found, initialize all output indices for that center with the first valid point's index, then overwrite sequentially with subsequent valid indices until 16 are collected. Maintain zeros for centers with no valid points.\n\nInput:\ninput_xyz: float32, shape = (16, 4096, 3)\ninput_center: float32, shape = (16, 128, 3)\n\nOutput:\noutput_ball_query_idx: int32, shape = (16, 128, 16)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 396, "task_name": "Chamfer_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nChamfer_Distance\n\nTask Description:\nTask: Chamfer_Distance. Compute the Chamfer distance between two sets of points for 512 batches. Each batch has two sets: set A with 128 points and set B with 256 points, each point having two coordinates (x, y). The kernel must compute, for each batch, two sums: (1) for each point in A, the minimum squared Euclidean distance to any point in B, summed and then divided by the number of points in A; (2) similarly for each point in B to A. The Chamfer distance for the batch is the sum of these two averages. The kernel accumulates the Chamfer distance for all batches and then divides the total by the batch size (512) to get the average Chamfer distance, which is output as a single scalar.\n\nInput:\ninput_points_a: float32, shape = (512, 128, 2)\ninput_points_b: float32, shape = (512, 256, 2)\n\nOutput:\noutput_chamfer_distance: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 414, "task_name": "Furthest_Point_Sample_with_Dist", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample_with_Dist\n\nTask Description:\nTask Name: Furthest_Point_Sample_with_Dist. For each of 8 batches, given a 1024x1024 matrix of pairwise distances between points (float32), select 32 points such that each new point maximizes the minimum distance to already selected points. Start with point index 0. Output the indices (int32) of selected points for each batch in a (8, 32) tensor. Maintain and update a distance array tracking each point's minimum distance to the current selection set.\n\nInput:\ninput_point_dists: float32, shape = (8, 1024, 1024)\n\nOutput:\noutput_sampled_indices: int32, shape = (8, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 393, "task_name": "Ball_Query", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBall_Query\n\nTask Description:\nTask: Ball_Query. Compute the indices of up to 16 neighboring points within a spherical shell around each center point. Inputs include a 3D point cloud tensor of shape (16, 4096, 3) and center points tensor of shape (16, 256, 3), both float32. Output is an int32 tensor of shape (16, 256, 16) storing indices of qualifying points. Constraints: Points must lie within the same batch; neighbors must satisfy distance squared between 1 and 100 units from the center (inclusive); exactly matching centers (distance 0) are included; outputs are initialized to zero; if any point is found, all output indices for that center start with the first found point and subsequent positions are filled with additional points in traversal order.\n\nInput:\ninput_xyz: float32, shape = (16, 4096, 3)\ninput_center: float32, shape = (16, 256, 3)\n\nOutput:\noutput_ball_query_idx: int32, shape = (16, 256, 16)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 415, "task_name": "Furthest_Point_Sample_with_Dist", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample_with_Dist\n\nTask Description:\nFurthest_Point_Sample_with_Dist: Implement a kernel that performs furthest point sampling using distance matrices. The input is a 3D tensor of shape (8, 2048, 2048) containing float32 values representing pairwise distances between points in each batch. The output should be a 2D tensor of shape (8, 32) containing int32 indices of sampled points. For each batch independently, initialize the first sampled point as index 0. For each subsequent sample, update minimum distances between all points and the current sampled set by comparing existing distances with distances from the latest sampled point. Select the point with the maximum updated distance as the next sample. The kernel must process batches in parallel and maintain intermediate distance arrays.\n\nInput:\ninput_point_dists: float32, shape = (8, 2048, 2048)\n\nOutput:\noutput_sampled_indices: int32, shape = (8, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 419, "task_name": "Group_Points", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGroup_Points\n\nTask Description:\nImplement a CUDA kernel named 'Group_Points'. The kernel should group point features based on provided indices. Input includes 'input_features' tensor of shape (16, 4, 1024) with float32 values and 'input_indices' tensor of shape (16, 32, 256) with int32 values. Output must be 'output_grouped_features' tensor of shape (16, 4, 32, 256) with float32 values. The kernel must gather features such that for each batch, channel, point, and sample, the feature value at the index specified by 'input_indices' is taken from 'input_features' and placed in the corresponding position in the output. Indices must be in the range [0, 1023] to be valid.\n\nInput:\ninput_features: float32, shape = (16, 4, 1024)\ninput_indices: int32, shape = (16, 32, 256)\n\nOutput:\noutput_grouped_features: float32, shape = (16, 4, 32, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 422, "task_name": "KNN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKNN\n\nTask Description:\nImplement a CUDA kernel for K-Nearest Neighbors (KNN). Given a batch of point clouds and center points, compute the indices of the K nearest points for each center. Inputs include a float32 tensor 'input_xyz' with shape (16, 2048, 3) representing point coordinates and a float32 tensor 'input_center' with shape (16, 128, 3) representing center coordinates. Output must be an int32 tensor 'output_knn_idx' with shape (16, 128, 32) containing indices of the 32 nearest neighbors for each center. The kernel must compute squared Euclidean distances, maintain a min-heap of the K smallest distances, sort the results by distance, and output sorted indices.\n\nInput:\ninput_xyz: float32, shape = (16, 2048, 3)\ninput_center: float32, shape = (16, 128, 3)\n\nOutput:\noutput_knn_idx: int32, shape = (16, 128, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 399, "task_name": "Chamfer_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nChamfer_Distance\n\nTask Description:\nCompute the Chamfer distance for a batch of point set pairs. The input consists of two tensors: input_points_a of shape (4096, 128, 2) and input_points_b of shape (4096, 256, 2), both of dtype float32. For each batch element, compute the Chamfer distance as the sum of two terms: the average of the minimum squared Euclidean distances from each point in set A to any point in set B, and the average of the minimum squared Euclidean distances from each point in set B to any point in set A. The kernel must accumulate the Chamfer distance for each batch element and output a single scalar that is the sum of these per-batch distances. The host will then average this sum by the batch size (4096). The output tensor output_chamfer_distance is a scalar of dtype float32.\n\nInput:\ninput_points_a: float32, shape = (4096, 128, 2)\ninput_points_b: float32, shape = (4096, 256, 2)\n\nOutput:\noutput_chamfer_distance: float32, shape = ()\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 424, "task_name": "KNN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKNN\n\nTask Description:\nImplement a CUDA kernel for K-Nearest Neighbors (KNN) computation. The kernel should process batches of 3D point clouds to find the 32 nearest neighbors for each center point. Inputs include a float32 tensor of shape (16, 2048, 3) representing 16 batches of 2048 points each with XYZ coordinates, and a float32 tensor of shape (16, 512, 3) representing center points. The output should be an int32 tensor of shape (16, 512, 32) storing indices of nearest neighbors in the input point cloud for each center. For each center point, compute squared Euclidean distances to all input points, maintain a max-heap of the 32 smallest distances, and output indices sorted by ascending distance.\n\nInput:\ninput_xyz: float32, shape = (16, 2048, 3)\ninput_center: float32, shape = (16, 512, 3)\n\nOutput:\noutput_knn_idx: int32, shape = (16, 512, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 421, "task_name": "KNN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKNN\n\nTask Description:\nImplement the KNN kernel to find the K nearest neighbors for each center point in a batch of point clouds. The input includes two tensors: 'input_xyz' (float32, shape [16, 2048, 3]) representing batch point coordinates, and 'input_center' (float32, shape [16, 64, 3]) representing center point coordinates. The output tensor 'output_knn_idx' (int32, shape [16, 64, 32]) must store indices of the 32 nearest points from 'input_xyz' for each center point. Use squared Euclidean distance as the metric, maintain a max-heap for efficient neighbor tracking, and ensure sorted results from closest to farthest.\n\nInput:\ninput_xyz: float32, shape = (16, 2048, 3)\ninput_center: float32, shape = (16, 64, 3)\n\nOutput:\noutput_knn_idx: int32, shape = (16, 64, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 430, "task_name": "Three_NN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nThree_NN\n\nTask Description:\nTask: Three_NN. Compute the indices of the three nearest neighbors for each unknown point from a set of known points. The input includes two tensors: input_unknown_points of shape (16, 4096, 3) and input_known_points of shape (16, 128, 3), both with float32 data type representing 3D coordinates. The output is a tensor named output_three_nn_idx of shape (16, 4096, 3) with int32 data type, storing the indices of the three closest known points for each unknown point. Constraints: Each batch is processed independently, and the kernel must compute Euclidean distances to find the nearest neighbors without modifying input data.\n\nInput:\ninput_unknown_points: float32, shape = (16, 4096, 3)\ninput_known_points: float32, shape = (16, 128, 3)\n\nOutput:\noutput_three_nn_idx: int32, shape = (16, 4096, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 398, "task_name": "Chamfer_Distance", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nChamfer_Distance\n\nTask Description:\nImplement a CUDA kernel named 'Chamfer_Distance' that computes the Chamfer distance between two sets of points for a batch of 2048 samples. Each sample has two point sets: set A with 128 points and set B with 256 points, each point being 2D (x, y). The inputs are two tensors: 'input_points_a' of shape (2048, 128, 2) and 'input_points_b' of shape (2048, 256, 2), both with dtype float32. The output is a scalar tensor 'output_chamfer_distance' of shape (1,) and dtype float32. For each sample, the kernel must compute two sums: (1) the sum of the minimum squared Euclidean distances from each point in set A to any point in set B, and (2) the sum of the minimum squared Euclidean distances from each point in set B to any point in set A. The per-sample loss is then computed as (sum_A / 128) + (sum_B / 256). The kernel must use atomicAdd to accumulate this per-sample loss into the output scalar. Note: the kernel does not average over the batch; the host will divide the accumulated output by 2048 to get the mean.\n\nInput:\ninput_points_a: float32, shape = (2048, 128, 2)\ninput_points_b: float32, shape = (2048, 256, 2)\n\nOutput:\noutput_chamfer_distance: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 429, "task_name": "Three_NN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nThree_NN\n\nTask Description:\nTask: Three_NN. For each batch of 16 sets containing 2048 unknown 3D points, find the indices of the three nearest neighbors from a corresponding set of 128 known 3D points per batch. Inputs include float32 tensors for unknown points (16x2048x3) and known points (16x128x3). Output must be an int32 tensor (16x2048x3) storing indices of the three closest known points for each unknown point, ordered by ascending distance. Kernel must compute squared Euclidean distances and maintain per-batch isolation.\n\nInput:\ninput_unknown_points: float32, shape = (16, 2048, 3)\ninput_known_points: float32, shape = (16, 128, 3)\n\nOutput:\noutput_three_nn_idx: int32, shape = (16, 2048, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 428, "task_name": "Three_NN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nThree_NN\n\nTask Description:\nThree_NN task: Implement a CUDA kernel that computes the indices of the three nearest neighbors for each unknown point within a batch. The input consists of two float32 tensors: 'input_unknown_points' with shape (16, 1024, 3) representing 16 batches of 1024 unknown 3D points, and 'input_known_points' with shape (16, 128, 3) representing 16 batches of 128 known 3D points. The output must be an int32 tensor 'output_three_nn_idx' with shape (16, 1024, 3) storing the indices (0-127) of the three closest known points for each unknown point. The kernel must compute squared Euclidean distances and maintain batch independence.\n\nInput:\ninput_unknown_points: float32, shape = (16, 1024, 3)\ninput_known_points: float32, shape = (16, 128, 3)\n\nOutput:\noutput_three_nn_idx: int32, shape = (16, 1024, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 403, "task_name": "Dynamic_Voxelize", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDynamic_Voxelize\n\nTask Description:\nTask: Dynamic_Voxelize. Given 4096 points with 5 features each (float32), compute voxel coordinates for each point in a fixed 3D grid. The grid spans from (0.0, 0.0, 0.0) to (5.0, 5.0, 5.0) with voxel size 1.0 in each dimension. For each point, use the first three features (x,y,z) to compute voxel indices by subtracting the minimum coordinate and dividing by voxel size, then taking the floor. If x is outside [0,5), set the first output coordinate to -1. If x is valid but y is outside [0,5), set the first two output coordinates to -1. If both x and y are valid but z is outside [0,5), set all three output coordinates to -1. For valid points, output voxel indices in (z,y,x) order. Output must be a 4096x3 int32 tensor.\n\nInput:\ninput_points: float32, shape = (4096, 5)\n\nOutput:\noutput_dynamic_voxelize_coordinates: int32, shape = (4096, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 417, "task_name": "Group_Points", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGroup_Points\n\nTask Description:\nTask: Group_Points. Given an input feature tensor of shape (16, 4, 1024) and an input indices tensor of shape (16, 32, 64), both batched over 16 examples, produce an output tensor of shape (16, 4, 32, 64). For each batch, channel, point, and sample, select the feature value from the input feature tensor at the index specified in the input indices tensor. The indices are guaranteed to be in the range [0, 1023], and the kernel must respect tensor bounds without memory access violations.\n\nInput:\ninput_features: float32, shape = (16, 4, 1024)\ninput_indices: int32, shape = (16, 32, 64)\n\nOutput:\noutput_grouped_features: float32, shape = (16, 4, 32, 64)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 440, "task_name": "Image_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Convolution\n\nTask Description:\nImplement the Image_Convolution kernel. Perform a depthwise convolution on a 3-channel input image with dimensions 4096x4096 using a 3x3 kernel. The input tensor 'img_input' is float32 with shape (4096, 4096, 3), and the kernel tensor 'conv_kernel' is float32 with shape (3, 3). Output tensor 'img_output' must match the input shape and dtype. Use stride=1, padding=1 (zero-padded), and apply the same kernel independently to each channel. Boundary pixels must be handled by skipping out-of-bounds kernel elements.\n\nInput:\nimg_input: float32, shape = (4096, 4096, 3)\nconv_kernel: float32, shape = (3, 3)\n\nOutput:\nimg_output: float32, shape = (4096, 4096, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 394, "task_name": "Ball_Query", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBall_Query\n\nTask Description:\nTask: Ball_Query. For each center point in a batch, find indices of up to 16 points that lie either exactly at the center (distance squared equals zero) or within a spherical shell defined by minimum radius 1.0 and maximum radius 10.0. Inputs include: input_xyz (float32, shape [16, 4096, 3]) representing point clouds, and input_center (float32, shape [16, 512, 3]) representing center points. Output is output_ball_query_idx (int32, shape [16, 512, 16]). Constraints: Each center point must independently search all 4096 points in its batch; output indices are initialized to zero and overwritten when points are found; if no points satisfy the condition, indices remain zero.\n\nInput:\ninput_xyz: float32, shape = (16, 4096, 3)\ninput_center: float32, shape = (16, 512, 3)\n\nOutput:\noutput_ball_query_idx: int32, shape = (16, 512, 16)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 438, "task_name": "Image_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Convolution\n\nTask Description:\nTask: Image_Convolution. Perform a depthwise 2D convolution on a 1024x1024x3 image (HWC layout) using a 3x3 kernel. The input tensor 'img_input' is of shape (1024, 1024, 3) and type float32. The kernel tensor 'conv_kernel' is of shape (3, 3) and type float32. The output tensor 'img_output' must be of the same shape (1024, 1024, 3) and type float32. The convolution must use a stride of 1 and zero-padding of 1 pixel on all sides. Each channel of the image is convolved independently with the same kernel. For each output pixel, compute the sum of the element-wise products between the kernel and the corresponding input window, skipping out-of-bound input pixels (treated as zero).\n\nInput:\nimg_input: float32, shape = (1024, 1024, 3)\nconv_kernel: float32, shape = (3, 3)\n\nOutput:\nimg_output: float32, shape = (1024, 1024, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 425, "task_name": "KNN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKNN\n\nTask Description:\nImplement a CUDA kernel for the KNN task. For each center point in 3D space, find the indices of the K nearest points from a separate set of points. The input consists of two tensors: 'input_xyz' with float32 values of shape (16, 2048, 3) representing 16 batches of 2048 points each, and 'input_center' with float32 values of shape (16, 1024, 3) representing center points. The output tensor 'output_knn_idx' must be int32 with shape (16, 1024, 32) storing the indices of the 32 nearest points for each center. The kernel must compute Euclidean distances and maintain a min-heap to track nearest neighbors.\n\nInput:\ninput_xyz: float32, shape = (16, 2048, 3)\ninput_center: float32, shape = (16, 1024, 3)\n\nOutput:\noutput_knn_idx: int32, shape = (16, 1024, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 420, "task_name": "Group_Points", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGroup_Points\n\nTask Description:\nTask: Group_Points. Given an input features tensor of shape (16, 4, 1024) with float32 data type and an input indices tensor of shape (16, 32, 512) with int32 data type, produce an output tensor of shape (16, 4, 32, 512) with float32 data type. The kernel must gather features from the input features tensor using the indices. For each batch, channel, point (from 32 points), and sample (from 512 samples), the kernel uses the index from the input indices tensor to select a feature from the input features tensor (from the 1024 points). The indices must be in the range [0, 1023].\n\nInput:\ninput_features: float32, shape = (16, 4, 1024)\ninput_indices: int32, shape = (16, 32, 512)\n\nOutput:\noutput_grouped_features: float32, shape = (16, 4, 32, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 434, "task_name": "Edge_Detection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nEdge_Detection\n\nTask Description:\nEdge Detection: Implement a CUDA kernel that performs edge detection on a 2048x2048 grayscale image. The input is a float32 tensor representing pixel intensities. Apply the Sobel operator using 3x3 filters (Gx and Gy) with zero-padding at boundaries. For each pixel, compute horizontal and vertical gradients through weighted neighborhood sums, then calculate the Euclidean norm of these gradients. Clamp the resulting magnitude to 255.0 if it exceeds this value. Output a float32 tensor of identical dimensions to the input.\n\nInput:\nimg_input: float32, shape = (2048, 2048)\n\nOutput:\nimg_edges: float32, shape = (2048, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 431, "task_name": "Edge_Detection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nEdge_Detection\n\nTask Description:\nEdge_Detection: Implement a kernel that detects edges in an input image using the Sobel operator. The input is a 256x256 float32 tensor representing the image. The output is a 256x256 float32 tensor containing the edge map. Compute horizontal and vertical gradients by applying 3x3 Sobel kernels with zero-padding for boundary pixels, then calculate the gradient magnitude as the Euclidean norm of the gradients and clamp it to the range [0, 255].\n\nInput:\nimg_input: float32, shape = (256, 256)\n\nOutput:\nimg_edges: float32, shape = (256, 256)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 437, "task_name": "Image_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Convolution\n\nTask Description:\nImplement a depthwise convolution kernel for image processing. The input is a 512x512x3 tensor (height, width, channels) of float32 values representing an image. Apply a shared 3x3 convolution kernel (float32) independently to each channel. Use stride=1 and zero-padding=1 to maintain output dimensions. For each output pixel, compute the weighted sum of surrounding pixels from the same channel using the kernel weights, skipping out-of-bound pixels. Output must be a 512x512x3 float32 tensor matching input dimensions.\n\nInput:\nimg_input: float32, shape = (512, 512, 3)\nconv_kernel: float32, shape = (3, 3)\n\nOutput:\nimg_output: float32, shape = (512, 512, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 452, "task_name": "Image_Scaling", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Scaling\n\nTask Description:\nImplement a CUDA kernel for bilinear image scaling. The input is a 512x512 RGB image with 3 channels stored as float32 in HWC layout. The output is a scaled 768x768 RGB image with 3 channels in float32. The scaling must use bilinear interpolation with coordinate mapping that matches align_corners=false behavior. Coordinates must be clamped to image boundaries during sampling. The kernel should handle each output pixel independently.\n\nInput:\nimg_input: float32, shape = (512, 512, 3)\n\nOutput:\nimg_scaled: float32, shape = (768, 768, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 442, "task_name": "Matrix_Transpose", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Transpose\n\nTask Description:\nMatrix_Transpose task requires implementing a CUDA kernel that transposes a 2048x2048 matrix of float32 values. The input tensor 'matrix_input' with shape (2048, 2048) must be transformed such that each element at position (i, j) in the input appears at position (j, i) in the output tensor 'matrix_out'. The kernel must preserve all values exactly during transposition without altering data types or values.\n\nInput:\nmatrix_input: float32, shape = (2048, 2048)\n\nOutput:\nmatrix_out: float32, shape = (2048, 2048)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 451, "task_name": "Image_Scaling", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Scaling\n\nTask Description:\nImplement an image scaling kernel named 'Image_Scaling'. The kernel should bilinearly interpolate an input image from 256x256 resolution to 384x384 resolution. Input is a 256x256x3 tensor of float32 values in HWC format (height, width, channels). Output must be a 384x384x3 float32 tensor in the same format. Use the coordinate mapping: input_x = (output_x + 0.5) * (input_width / output_width) - 0.5 and similarly for y. Clamp sampling coordinates to image boundaries and weight contributions from four nearest neighbors using fractional differences. Ensure identical results to PyTorch's bilinear interpolation with align_corners=False.\n\nInput:\nimg_input: float32, shape = (256, 256, 3)\n\nOutput:\nimg_scaled: float32, shape = (384, 384, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 405, "task_name": "Dynamic_Voxelize", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDynamic_Voxelize\n\nTask Description:\nTask: Dynamic_Voxelize. Description: Assign each 3D point to a voxel in a fixed grid. The grid spans coordinates (0.0, 0.0, 0.0) to (5.0, 5.0, 5.0) with 1.0-unit voxel sizes. Input is a float32 tensor of shape (16384, 5), where only the first three features (x, y, z) are used. For each point, compute voxel indices by subtracting min coordinates and dividing by voxel size. If x-coordinate is out-of-bounds, output (-1, 0, 0); if y is out-of-bounds, output (-1, -1, 0); if z is out-of-bounds, output (-1, -1, -1); otherwise output (z_index, y_index, x_index). Output must be an int32 tensor of shape (16384, 3).\n\nInput:\ninput_points: float32, shape = (16384, 5)\n\nOutput:\noutput_dynamic_voxelize_coordinates: int32, shape = (16384, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 449, "task_name": "Image_Rotate", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Rotate\n\nTask Description:\nImplement a CUDA kernel that rotates an input image by 90 degrees clockwise. The input tensor 'img_input' has shape (2048, 2048, 3) and dtype float32, representing an image with height 2048, width 2048, and 3 color channels. The output tensor 'img_rotated' must have the same shape and dtype. Each pixel at position (h, w) in the input should be mapped to position (w, H-1-h) in the output, where H is the image height (2048). The kernel must preserve channel data and handle all pixels without altering the original data.\n\nInput:\nimg_input: float32, shape = (2048, 2048, 3)\n\nOutput:\nimg_rotated: float32, shape = (2048, 2048, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 426, "task_name": "Three_NN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nThree_NN\n\nTask Description:\nTask: Three_NN. For each of 16 batches, for each of 256 unknown points (each with 3D coordinates), find the indices of the three nearest points from a set of 128 known points in the same batch. The distance is computed as squared Euclidean distance. The output must be a tensor of shape (16, 256, 3) with int32 indices, where each index corresponds to a point in the known set (ranging from 0 to 127) and is ordered by ascending distance (closest first).\n\nInput:\ninput_unknown_points: float32, shape = (16, 256, 3)\ninput_known_points: float32, shape = (16, 128, 3)\n\nOutput:\noutput_three_nn_idx: int32, shape = (16, 256, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 411, "task_name": "Furthest_Point_Sample_with_Dist", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample_with_Dist\n\nTask Description:\nTask: Furthest_Point_Sample_with_Dist. Given an input tensor of shape (8, 128, 128) with dtype float32 representing pairwise distances between points across 8 batches (each with 128 points), output a tensor of shape (8, 32) with dtype float32 containing the indices of 32 sampled points per batch. The kernel must implement iterative furthest point sampling: start with point index 0, then repeatedly select the point with the maximum minimum distance to the already-sampled set. For each new selection, update minimum distances by comparing existing values against distances from the latest sampled point. Batches must be processed independently.\n\nInput:\ninput_point_dists: float32, shape = (8, 128, 128)\n\nOutput:\noutput_sampled_indices: float32, shape = (8, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 448, "task_name": "Image_Rotate", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Rotate\n\nTask Description:\nTask: Image_Rotate. Rotate a 1024x1024 RGB image (3 channels) by 90 degrees clockwise. Input tensor 'img_input' is a float32 array of shape (1024, 1024, 3). Output tensor 'img_rotated' must be a float32 array of the same shape. The kernel must map each input pixel at (h, w) to the output pixel at (w, 1023 - h) for each color channel, without exceeding the image boundaries.\n\nInput:\nimg_input: float32, shape = (1024, 1024, 3)\n\nOutput:\nimg_rotated: float32, shape = (1024, 1024, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 433, "task_name": "Edge_Detection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nEdge_Detection\n\nTask Description:\nTask: Edge Detection. The kernel must compute edge detection on a 1024x1024 input image. The input tensor, named img_input, is a float32 array of shape (1024, 1024) representing a grayscale image. The output tensor, named img_edges, is a float32 array of the same shape (1024, 1024). The kernel must apply the Sobel operator to compute the gradient at each pixel. Specifically, for each pixel, compute the horizontal gradient (Gx) and vertical gradient (Gy) by convolving the 3x3 Sobel filters (Gx: [[-1,0,1],[-2,0,2],[-1,0,1]] and Gy: [[1,2,1],[0,0,0],[-1,-2,-1]]) with the 3x3 neighborhood of the pixel, using zero-padding for boundary pixels. The magnitude of the gradient is then calculated as the square root of (Gx^2 + Gy^2) and clamped to a maximum value of 255.0. The output for each pixel is this clamped magnitude.\n\nInput:\nimg_input: float32, shape = (1024, 1024)\n\nOutput:\nimg_edges: float32, shape = (1024, 1024)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 410, "task_name": "Furthest_Point_Sample", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample\n\nTask Description:\nTask name: Furthest_Point_Sample. Implement a CUDA kernel for furthest point sampling. The input is a tensor 'input_points' of shape (8, 2048, 3) and dtype float32, containing 8 batches of 2048 points in 3D space. The kernel must select 32 points per batch by iteratively choosing the point with the maximum minimum distance to the current set of selected points, starting with index 0 for each batch. The output is a tensor 'output_sampled_indices' of shape (1,) and dtype float32. The kernel must run on the GPU and process batches independently.\n\nInput:\ninput_points: float32, shape = (8, 2048, 3)\n\nOutput:\noutput_sampled_indices: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 432, "task_name": "Edge_Detection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nEdge_Detection\n\nTask Description:\nTask: Edge_Detection. Compute edge detection on a 512x512 grayscale input image using the Sobel operator. The Sobel operator involves applying two 3x3 filters: one for horizontal gradients (Gx) and one for vertical gradients (Gy). For each pixel, compute the gradient magnitudes by convolving the surrounding 3x3 pixel neighborhood with these filters. Boundary pixels should be handled via zero-padding. The output magnitude is the Euclidean norm of the horizontal and vertical gradients, clamped to a maximum value of 255. The input is a float32 tensor of shape (512, 512), and the output is a float32 tensor of the same shape.\n\nInput:\nimg_input: float32, shape = (512, 512)\n\nOutput:\nimg_edges: float32, shape = (512, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 447, "task_name": "Image_Rotate", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Rotate\n\nTask Description:\nTask: Image_Rotate. The kernel must rotate a 3-channel input image 90 degrees clockwise. The input is a float32 tensor of shape (512, 512, 3), representing an image with height 512, width 512, and 3 channels. The output is a float32 tensor of the same shape, containing the rotated image. Constraints: Each pixel from input position (h, w) must be mapped to output position (w, 511 - h), and the kernel must avoid out-of-bounds access by checking indices. Invariants: All pixel values must be copied exactly without modification, preserving the data.\n\nInput:\nimg_input: float32, shape = (512, 512, 3)\n\nOutput:\nimg_rotated: float32, shape = (512, 512, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 446, "task_name": "Image_Rotate", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Rotate\n\nTask Description:\nImplement a CUDA kernel for the Image_Rotate task that rotates an input image 90 degrees clockwise. The input is a 256×256×3 float32 tensor representing an image with height 256, width 256, and 3 color channels. The output should be a 256×256×3 float32 tensor with the rotated image. Each input pixel at (h, w) must map to output position (w, 255-h) while preserving channel values. The kernel must handle boundary checks to avoid out-of-bounds memory access.\n\nInput:\nimg_input: float32, shape = (256, 256, 3)\n\nOutput:\nimg_rotated: float32, shape = (256, 256, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 423, "task_name": "KNN", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nKNN\n\nTask Description:\nTask name: KNN. Given a batch of 16 sets of points (each set has 2048 points in 3D space) and a batch of 16 sets of center points (each set has 256 points in 3D space), compute for each center point the indices of the 32 nearest points from the corresponding set of points. The distance metric is squared Euclidean distance. The output must be a tensor of shape (16, 256, 32) of type int32, where for each center point, the 32 indices are ordered from the closest point (smallest distance) to the 32nd closest. The kernel must use a max-heap to efficiently track the nearest neighbors during the iteration and then sort the heap to produce the indices in ascending order of distance.\n\nInput:\ninput_xyz: float32, shape = (16, 2048, 3)\ninput_center: float32, shape = (16, 256, 3)\n\nOutput:\noutput_knn_idx: int32, shape = (16, 256, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 436, "task_name": "Image_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Convolution\n\nTask Description:\nImplement the Image_Convolution task. The kernel should perform a depthwise convolution on an input image tensor of shape (256, 256, 3) with data type float32, using a 3x3 convolution kernel of data type float32. The output tensor must have the same shape (256, 256, 3) and dtype float32. The convolution must be applied with a stride of 1 and padding of 1 (zero-padding), resulting in output dimensions matching the input. The same kernel is used for each channel independently, and the computation must respect the depthwise constraint where channels are processed separately without cross-channel interactions.\n\nInput:\nimg_input: float32, shape = (256, 256, 3)\nconv_kernel: float32, shape = (3, 3)\n\nOutput:\nimg_output: float32, shape = (256, 256, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 409, "task_name": "Furthest_Point_Sample", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample\n\nTask Description:\nImplement the Furthest_Point_Sample kernel. The input is a float32 tensor named 'input_points' with shape (8, 1024, 3), representing 8 batches of 1024 points in 3D space. The output is a float32 tensor named 'output_sampled_indices' with shape (1,). The kernel must perform furthest point sampling to select 32 points per batch. The algorithm starts by selecting the point at index 0 in each batch. Then, for each subsequent sample, it selects the point that has the maximum minimum distance to the set of already selected points. The kernel must maintain a temporary distance array for each point to track the minimum distance to the selected set. The output is a single float32 value, which is the result of the sampling process.\n\nInput:\ninput_points: float32, shape = (8, 1024, 3)\n\nOutput:\noutput_sampled_indices: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 453, "task_name": "Image_Scaling", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Scaling\n\nTask Description:\nImplement an image scaling kernel named 'Image_Scaling'. The kernel should take an input tensor 'img_input' of shape (1024, 1024, 3) and data type float32, representing an image in HWC layout. It should produce an output tensor 'img_scaled' of shape (1536, 1536, 3) and data type float32. The kernel must use bilinear interpolation to scale the image from 1024x1024 to 1536x1536. The interpolation method must align with the 'align_corners=False' setting in PyTorch. For each output pixel and each channel, the kernel must compute the corresponding input coordinates, then take the four nearest input pixels (clamped to the image boundaries) and compute a weighted average based on the fractional distances to the neighboring pixels.\n\nInput:\nimg_input: float32, shape = (1024, 1024, 3)\n\nOutput:\nimg_scaled: float32, shape = (1536, 1536, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 450, "task_name": "Image_Rotate", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Rotate\n\nTask Description:\nImplement the Image_Rotate kernel to rotate an input image by 90 degrees clockwise. The input is a float32 tensor of shape (4096, 4096, 3) representing an RGB image. The output should be a float32 tensor of the same shape. Each input pixel at position (h, w) must map to output position (w, H-1-h) for all color channels, where H is the image height (4096). The kernel must preserve channel order and data type while handling all pixels.\n\nInput:\nimg_input: float32, shape = (4096, 4096, 3)\n\nOutput:\nimg_rotated: float32, shape = (4096, 4096, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 454, "task_name": "Image_Scaling", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Scaling\n\nTask Description:\nTask: Image_Scaling. Scale an input image tensor of shape (2048, 2048, 3) with float32 values to an output tensor of shape (3072, 3072, 3) using bilinear interpolation. The kernel must map each output pixel to a continuous coordinate in the input space using the formula: input_x = (output_x + 0.5) * (input_width / output_width) - 0.5 and input_y = (output_y + 0.5) * (input_height / output_height) - 0.5. For each output pixel, compute weights from four nearest input pixels (clamped to image boundaries) and calculate a weighted average across all three color channels independently.\n\nInput:\nimg_input: float32, shape = (2048, 2048, 3)\n\nOutput:\nimg_scaled: float32, shape = (3072, 3072, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 467, "task_name": "DB4_Wavelet_Transform", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDB4_Wavelet_Transform\n\nTask Description:\nImplement a GPU kernel for the DB4_Wavelet_Transform task. The kernel should compute a Daubechies 4 wavelet transform on an input signal of 2048 float32 values. Input includes the signal array and two filter coefficient arrays (4 float32 values each for low-pass and high-pass filters). The output must be a transformed signal of 2048 float32 values, where the first half contains approximation coefficients and the second half contains detail coefficients. The kernel must handle boundary conditions by clamping indices to valid positions when accessing input elements. Each thread should process non-overlapping pairs of output coefficients using a sliding window of four consecutive input samples.\n\nInput:\nin: float32, shape = (2048,)\nlo: float32, shape = (4,)\nhi: float32, shape = (4,)\n\nOutput:\nout: float32, shape = (2048,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 455, "task_name": "Image_Scaling", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Scaling\n\nTask Description:\nTask: Image_Scaling. Scale an image from 4096x4096x3 to 6144x6144x3 using bilinear interpolation. The input is a float32 tensor of shape (4096, 4096, 3) representing an RGB image in HWC format. The output must be a float32 tensor of shape (6144, 6144, 3). The kernel must implement coordinate mapping equivalent to PyTorch's align_corners=False setting: for each output pixel, compute input coordinates as (ow + 0.5) * (inW/outW) - 0.5 and (oh + 0.5) * (inH/outH) - 0.5. Sampling coordinates must be clamped to input boundaries, and each output pixel must be a weighted average of four nearest input pixels based on fractional coordinates.\n\nInput:\nimg_input: float32, shape = (4096, 4096, 3)\n\nOutput:\nimg_scaled: float32, shape = (6144, 6144, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 406, "task_name": "Furthest_Point_Sample", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFurthest_Point_Sample\n\nTask Description:\nTask name: Furthest_Point_Sample. The kernel should perform furthest point sampling on a batch of 8 point clouds, each containing 128 points in 3D space. The input is a float32 tensor of shape (8, 128, 3). The algorithm must start by selecting index 0 as the first sample for each cloud. For each subsequent sample (31 more per cloud), it should compute the squared Euclidean distance from every point to the last selected sample, update the minimum distance to any selected sample, and select the point with the maximum updated distance. The output is a float32 tensor of shape (1,).\n\nInput:\ninput_points: float32, shape = (8, 128, 3)\n\nOutput:\noutput_sampled_indices: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 445, "task_name": "Matrix_Transpose", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Transpose\n\nTask Description:\nImplement a CUDA kernel for matrix transposition. The kernel should transpose a 16384x16384 input matrix of float32 values to an output matrix of the same shape and data type. For every element in the input matrix at position (i, j), it must appear at position (j, i) in the output matrix. Ensure all elements are correctly transposed without data loss or out-of-bound access.\n\nInput:\nmatrix_input: float32, shape = (16384, 16384)\n\nOutput:\nmatrix_out: float32, shape = (16384, 16384)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 461, "task_name": "Graph_Conv", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGraph_Conv\n\nTask Description:\nImplement a Graph Convolution kernel for a graph with 64 nodes and 256 edges. The kernel should compute output node features by aggregating messages from neighbor nodes. Inputs include: node features (64x8 float32), edge connections (2x256 int32), edge weights (256 float32), weight matrix (8x16 float32), and bias vector (16 float32). The output is a 64x16 float32 tensor. For each node, aggregate incoming edges by weighting source node features, apply linear transformation using the weight matrix, add bias, and store results.\n\nInput:\nx: float32, shape = (64, 8)\nedge_index: int32, shape = (2, 256)\nedge_weight: float32, shape = (256,)\nw: float32, shape = (8, 16)\nb: float32, shape = (16,)\n\nOutput:\ny_out: float32, shape = (64, 16)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 468, "task_name": "DB4_Wavelet_Transform", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDB4_Wavelet_Transform\n\nTask Description:\nDB4_Wavelet_Transform: Compute a Daubechies 4 wavelet transform on a 4096-element input signal. The kernel must process the input in overlapping segments of 4 elements, starting at every even index. For each segment, compute two coefficients: an approximation coefficient using the low-pass filter coefficients and a detail coefficient using the high-pass filter coefficients. If segment indices exceed the signal boundaries, clamp them to the nearest valid position. Store the 2048 approximation coefficients in the first half of the output and the 2048 detail coefficients in the second half. Inputs include a float32 signal (4096), low-pass coefficients (float32, 4), and high-pass coefficients (float32, 4). Output is a float32 array (4096).\n\nInput:\nin: float32, shape = (4096,)\nlo: float32, shape = (4,)\nhi: float32, shape = (4,)\n\nOutput:\nout: float32, shape = (4096,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 435, "task_name": "Edge_Detection", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nEdge_Detection\n\nTask Description:\nImplement an edge detection kernel for a 4096x4096 grayscale image. The input is a 2D tensor of float32 values representing pixel intensities. Apply the Sobel operator using two predefined 3x3 filters: one for horizontal gradients (Gx: [[-1,0,1], [-2,0,2], [-1,0,1]]) and one for vertical gradients (Gy: [[1,2,1], [0,0,0], [-1,-2,-1]]). For each pixel, compute the convolution with both filters over its 3x3 neighborhood, using zero-padding for boundary pixels. Calculate the gradient magnitude as the square root of the sum of squares of the horizontal and vertical gradients. Clamp the output magnitude to 255 if it exceeds this value. The output is a 1D tensor of float32 values with length 16777216 (4096x4096 flattened).\n\nInput:\nimg_input: float32, shape = (4096, 4096)\n\nOutput:\nimg_edges: float32, shape = (16777216,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 469, "task_name": "DB4_Wavelet_Transform", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDB4_Wavelet_Transform\n\nTask Description:\nTask: DB4_Wavelet_Transform. Perform a Daubechies 4 wavelet transform on an input signal. Input includes a 8192-element float32 signal array, a 4-element float32 low-pass filter coefficient array, and a 4-element float32 high-pass filter coefficient array. Output must be a 8192-element float32 array. The first half of the output contains approximation coefficients calculated by applying the low-pass filter to overlapping 4-sample blocks. The second half contains detail coefficients calculated by applying the high-pass filter to the same blocks. Input indices must be clamped to the signal boundaries when exceeding array limits. The kernel must process the entire signal using half as many threads as input samples.\n\nInput:\nin: float32, shape = (8192,)\nlo: float32, shape = (4,)\nhi: float32, shape = (4,)\n\nOutput:\nout: float32, shape = (8192,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 444, "task_name": "Matrix_Transpose", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Transpose\n\nTask Description:\nTask: Matrix_Transpose. Perform a matrix transpose operation on an input matrix of size 8192 rows by 8192 columns, with data type float32. The input is provided as a two-dimensional tensor with shape (8192, 8192). The output should be a one-dimensional tensor with shape (67108864,) and dtype float32, representing the flattened transposed matrix where each element originally at row i and column j in the input is placed at the position corresponding to row j and column i in the logical transposed matrix. Ensure the kernel handles all elements within the matrix bounds and produces an output that matches the reference transpose exactly.\n\nInput:\nmatrix_input: float32, shape = (8192, 8192)\n\nOutput:\nmatrix_out: float32, shape = (67108864,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 466, "task_name": "DB4_Wavelet_Transform", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDB4_Wavelet_Transform\n\nTask Description:\nTask: DB4_Wavelet_Transform. Compute a 1D Daubechies 4 wavelet transform on an input signal. The input is a tensor 'in' with 1024 float32 values. Use two filter coefficient tensors: 'lo' (4 float32 values for low-pass filtering) and 'hi' (4 float32 values for high-pass filtering). Output a tensor 'out' with 1024 float32 values. The first half of the output contains approximation coefficients computed by applying the low-pass filter to overlapping 4-sample windows of the input. The second half contains detail coefficients computed by applying the high-pass filter to the same windows. Input indices beyond the signal length must be clamped to the last valid index.\n\nInput:\nin: float32, shape = (1024,)\nlo: float32, shape = (4,)\nhi: float32, shape = (4,)\n\nOutput:\nout: float32, shape = (1024,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 470, "task_name": "DB4_Wavelet_Transform", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDB4_Wavelet_Transform\n\nTask Description:\nImplement the DB4_Wavelet_Transform kernel. The kernel must compute a Daubechies 4 wavelet transform on a 16384-element input signal. Inputs include: (1) a float32 tensor 'in' of shape (16384,) containing the input signal, (2) a float32 tensor 'lo' of shape (4,) with low-pass filter coefficients, and (3) a float32 tensor 'hi' of shape (4,) with high-pass filter coefficients. The output is a float32 tensor 'out' of shape (16384,) where the first half contains approximation coefficients and the second half contains detail coefficients. For each block of four consecutive input samples starting at even indices, compute one approximation and one detail coefficient. When indices exceed the input bounds, clamp them to the last valid index. The kernel must process all blocks in parallel.\n\nInput:\nin: float32, shape = (16384,)\nlo: float32, shape = (4,)\nhi: float32, shape = (4,)\n\nOutput:\nout: float32, shape = (16384,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 472, "task_name": "PSNR", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPSNR\n\nTask Description:\nCompute the Peak Signal-to-Noise Ratio (PSNR) between two images. The inputs are two float32 tensors named 'inputImage' and 'compressedInputImage', each with 262144 elements. The output is a single float32 value named 'psnr_out'. The kernel must calculate the sum of squared differences between corresponding elements of the input tensors, then compute the mean squared error by dividing this sum by the number of elements, and finally derive the PSNR using the formula 10 * log10(1 / mean squared error). The inputs must be of the same size and shape, and the computation requires a reduction step to sum the squared differences across all elements.\n\nInput:\ninputImage: float32, shape = (262144,)\ncompressedInputImage: float32, shape = (262144,)\n\nOutput:\npsnr_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 462, "task_name": "Graph_Conv", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGraph_Conv\n\nTask Description:\nImplement the Graph_Conv kernel. The kernel computes a graph convolution operation with 128 nodes and 1024 edges. Input tensor 'x' contains node features as float32 values in a (128, 16) shape. 'edge_index' provides directed edge connections as int32 indices in (2, 1024) shape where row 0 contains source nodes and row 1 contains destination nodes. 'edge_weight' contains float32 weights for each edge in (1024,) shape. Weight matrix 'w' is a float32 tensor of shape (16, 16), and bias 'b' is a float32 tensor of shape (16,). The output tensor 'y_out' must be a flattened float32 vector of 2048 elements (128 nodes × 16 features). The kernel must compute each output feature by: starting with the bias term, then for each incoming edge to a node, multiply the edge weight by the dot product between the source node's features and the corresponding weight matrix column.\n\nInput:\nx: float32, shape = (128, 16)\nedge_index: int32, shape = (2, 1024)\nedge_weight: float32, shape = (1024,)\nw: float32, shape = (16, 16)\nb: float32, shape = (16,)\n\nOutput:\ny_out: float32, shape = (2048,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 478, "task_name": "Black_Scholes", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBlack_Scholes\n\nTask Description:\nImplement the Black_Scholes kernel to compute call and put option prices using the Black-Scholes model. Inputs include three float32 vectors of length 4194304: stock_price, option_strike, and option_years. Outputs must be two float32 vectors of length 4194304: call_out for call option prices and put_out for put option prices. The kernel must use a polynomial approximation for the cumulative normal distribution function with fixed constants (A1-A5 and RSQRT2PI), process two options per thread for increased ILP, and use fixed risk-free rate (0.02) and volatility (0.30) parameters. Input size must be even.\n\nInput:\nstock_price: float32, shape = (4194304,)\noption_strike: float32, shape = (4194304,)\noption_years: float32, shape = (4194304,)\n\nOutput:\ncall_out: float32, shape = (4194304,)\nput_out: float32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 441, "task_name": "Matrix_Transpose", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Transpose\n\nTask Description:\nTask name: Matrix_Transpose. The kernel should compute the transpose of a 1024x1024 matrix. The input tensor 'matrix_input' has shape (1024, 1024) and data type float32, representing the input matrix. The output tensor 'matrix_out' must be a flattened array of the transposed matrix, with shape (1048576,) and data type float32. The kernel must map each element at position (i, j) in the input matrix to position (j, i) in the transposed matrix, which is then flattened into a 1D array in row-major order.\n\nInput:\nmatrix_input: float32, shape = (1024, 1024)\n\nOutput:\nmatrix_out: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 475, "task_name": "PSNR", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPSNR\n\nTask Description:\nImplement a CUDA kernel to compute the Peak Signal-to-Noise Ratio (PSNR) between two images. The inputs are two 1D float32 tensors named 'inputImage' and 'compressedInputImage', each with 16777216 elements representing flattened image pixel values. The output is a single float32 scalar named 'psnr_out'. The kernel must calculate the sum of squared differences between corresponding elements, compute the mean squared error, and derive the PSNR using the formula 10 * log10(1 / MSE). Ensure the implementation handles the large dataset efficiently through parallel reduction.\n\nInput:\ninputImage: float32, shape = (16777216,)\ncompressedInputImage: float32, shape = (16777216,)\n\nOutput:\npsnr_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 476, "task_name": "Black_Scholes", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBlack_Scholes\n\nTask Description:\nImplement the Black_Scholes kernel for pricing European options. The kernel must compute call and put option prices using the Black-Scholes model. Inputs include three float32 vectors of length 1048576: stock_price (current stock price), option_strike (strike price), and option_years (time to expiration in years). Outputs are two float32 vectors of length 1048576: call_out (call option prices) and put_out (put option prices). The risk-free rate is fixed at 0.02 and volatility at 0.30. The kernel must use a polynomial approximation for the cumulative normal distribution function and process two options per thread for efficiency. The total number of options must be even.\n\nInput:\nstock_price: float32, shape = (1048576,)\noption_strike: float32, shape = (1048576,)\noption_years: float32, shape = (1048576,)\n\nOutput:\ncall_out: float32, shape = (1048576,)\nput_out: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 482, "task_name": "DWT_Haar", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDWT_Haar\n\nTask Description:\nImplement the DWT_Haar kernel to compute the 1D Haar wavelet transform on a signal. The input is a 1024-element float32 signal tensor. The kernel must produce two outputs: a 1024-element float32 details_out tensor containing wavelet detail coefficients at multiple levels, and a single float32 approximation_out value representing the final approximation coefficient. The transform must be computed recursively over log2(1024)=10 levels, processing element pairs at each level. For each pair (x, y), compute detail as (x - y) * 1/√2 and approximation as (x + y) * 1/√2. The output must store first-level details in the second half of details_out, with subsequent levels stored in progressively lower indices. The kernel must avoid shared memory bank conflicts using appropriate padding.\n\nInput:\nsignal: float32, shape = (1024,)\n\nOutput:\ndetails_out: float32, shape = (1024,)\napprox_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 477, "task_name": "Black_Scholes", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBlack_Scholes\n\nTask Description:\nTask: Black_Scholes. Implement a CUDA kernel to compute call and put option prices using the Black-Scholes model for a batch of 2097152 options. Inputs include three float32 tensors of shape (2097152,): stock_price, option_strike, and option_years. Outputs are two float32 tensors of the same shape: call_out and put_out. The kernel must use a fixed risk-free rate of 0.02 and volatility rate of 0.30. It must employ a polynomial approximation for the cumulative normal distribution function and process two options per thread for efficiency.\n\nInput:\nstock_price: float32, shape = (2097152,)\noption_strike: float32, shape = (2097152,)\noption_years: float32, shape = (2097152,)\n\nOutput:\ncall_out: float32, shape = (2097152,)\nput_out: float32, shape = (2097152,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 471, "task_name": "PSNR", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPSNR\n\nTask Description:\nCompute the Peak Signal-to-Noise Ratio (PSNR) between two images. The inputs are two one-dimensional float32 arrays, each with 65536 elements, representing the original and compressed images. The output is a single float32 scalar value. The kernel must calculate the mean squared error (MSE) by summing the squared differences between corresponding elements of the input arrays, then compute PSNR as 10 * log10(1 / MSE). The computation must handle all 65536 elements correctly.\n\nInput:\ninputImage: float32, shape = (65536,)\ncompressedInputImage: float32, shape = (65536,)\n\nOutput:\npsnr_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 402, "task_name": "Dynamic_Voxelize", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDynamic_Voxelize\n\nTask Description:\nTask name: Dynamic_Voxelize. Compute the voxel coordinates for each point in a 3D grid. The input is a tensor of shape (2048, 5) with dtype float32, representing points with 5 features. The output is a tensor of shape (6144,) with dtype int32, storing 3 integers per point for the voxel coordinates. Use fixed voxel sizes of 1.0 in each dimension and coordinate ranges from min [0.0, 0.0, 0.0] to max [5.0, 5.0, 5.0], resulting in a grid size of 5 in each dimension. For each point, if the computed x-coordinate is out of bounds (less than 0 or >=5), set all three output coordinates to -1. If y is out of bounds after a valid x, set to -1. Similarly for z. If all coordinates are valid, store them as (z, y, x) for the point. Ensure the output matches the shape and data type.\n\nInput:\ninput_points: float32, shape = (2048, 5)\n\nOutput:\noutput_dynamic_voxelize_coordinates: int32, shape = (6144,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 480, "task_name": "Black_Scholes", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBlack_Scholes\n\nTask Description:\nTask name: Black_Scholes. The kernel computes call and put option prices using the Black-Scholes model. Inputs are three float32 arrays of size 16777216: stock_price (current stock price), option_strike (strike price), and option_years (time to maturity). Outputs are two float32 arrays of size 16777216: call_out (call option price) and put_out (put option price). Constraints: Fixed risk-free rate of 0.02 and volatility of 0.30 must be used. The cumulative normal distribution function is approximated with a polynomial, and outputs must match a reference within a tolerance of 1e-3.\n\nInput:\nstock_price: float32, shape = (16777216,)\noption_strike: float32, shape = (16777216,)\noption_years: float32, shape = (16777216,)\n\nOutput:\ncall_out: float32, shape = (16777216,)\nput_out: float32, shape = (16777216,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 456, "task_name": "Stereo_Disparity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nStereo_Disparity\n\nTask Description:\nTask: Stereo_Disparity. Given two input images, left and right, each of size 256x256 with uint32 data type, compute a disparity map output of size 65536 (flattened 256x256) with uint32 data type. For each pixel in the left image, compute the disparity by comparing it to the right image across a fixed range of candidate disparities (0 to 16). For each candidate disparity, calculate the sum of absolute differences (SAD) over a 17x17 window centered at the pixel. The candidate disparity that minimizes the SAD cost is selected, and 8 is added to it to form the output value for that pixel. The kernel must handle image borders by clamping out-of-bound accesses.\n\nInput:\nleft: uint32, shape = (256, 256)\nright: uint32, shape = (256, 256)\n\nOutput:\noutput: uint32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 439, "task_name": "Image_Convolution", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nImage_Convolution\n\nTask Description:\nImplement the Image_Convolution kernel. Perform a depthwise convolution on a 3-channel input image using a 3x3 kernel. The input tensor is a 2048x2048x3 float32 array representing image data in height-width-channel layout. The kernel tensor is a 3x3 float32 array. Apply the same kernel to each channel independently with stride 1 and zero-padding of 1 pixel. The output must be a 2048x2048x3 float32 tensor with identical dimensions to the input. Each output pixel is computed by taking the weighted sum of its 3x3 neighborhood in the corresponding input channel, skipping out-of-bound pixels.\n\nInput:\nimg_input: float32, shape = (2048, 2048, 3)\nconv_kernel: float32, shape = (3, 3)\n\nOutput:\nimg_output: float32, shape = (2048, 2048, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 483, "task_name": "DWT_Haar", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDWT_Haar\n\nTask Description:\nImplement the DWT_Haar kernel to compute a 1D Discrete Wavelet Transform using the Haar wavelet. The input is a 2048-element float32 signal tensor. The outputs are: a 2048-element float32 details_out tensor storing wavelet detail coefficients, and a single-element float32 approx_out tensor for the final approximation coefficient. Constraints: The input length must be a power of two (2048=2^11), requiring 11 recursive decomposition levels. Each level computes pairwise differences (details) and averages (approximations) scaled by 1/√2, with results stored in specific output positions. Bank conflicts must be avoided in shared memory using padding.\n\nInput:\nsignal: float32, shape = (2048,)\n\nOutput:\ndetails_out: float32, shape = (2048,)\napprox_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 473, "task_name": "PSNR", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPSNR\n\nTask Description:\nCompute the Peak Signal-to-Noise Ratio (PSNR) between two input images. The inputs are two one-dimensional tensors of 1048576 float32 values each, representing the original and compressed images. The output is a single float32 value. The PSNR is computed by first calculating the mean squared error (MSE) between the two images and then applying the formula: PSNR = 10 * log10(1 / MSE). The kernel must compute the sum of squared differences (SSE) for every corresponding element in the two arrays, then divide by the total number of elements to get the MSE, and finally compute the PSNR.\n\nInput:\ninputImage: float32, shape = (1048576,)\ncompressedInputImage: float32, shape = (1048576,)\n\nOutput:\npsnr_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 460, "task_name": "Stereo_Disparity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nStereo_Disparity\n\nTask Description:\nTask: Stereo_Disparity. Compute a disparity map from two input images. The inputs are two 4096x4096 images (left and right) of type uint32. The output is a flattened disparity map of 16777216 elements (4096x4096) of type uint32. For each pixel in the left image, search over a fixed disparity range (0-16) to find the best-matching pixel in the right image by computing the sum of absolute differences of 4 color channels over a 17x17 window. The best disparity (minimum cost) must be found and incremented by 8 in the output. Thread synchronization and shared memory must be used for intermediate cost calculations.\n\nInput:\nleft: uint32, shape = (4096, 4096)\nright: uint32, shape = (4096, 4096)\n\nOutput:\noutput: uint32, shape = (16777216,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 479, "task_name": "Black_Scholes", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nBlack_Scholes\n\nTask Description:\nTask Name: Black_Scholes\nCompute call and put option prices for 8,388,608 elements using the Black-Scholes model. Inputs include three one-dimensional float32 arrays: stock_price (current stock price), option_strike (strike price), and option_years (time to expiration in years). Outputs include two one-dimensional float32 arrays: call_out (call option price) and put_out (put option price).\n\nThe Black-Scholes formulas are defined as:\ncall = S * N(d1) - X * exp(-r*T) * N(d2)\nput = X * exp(-r*T) * N(-d2) - S * N(-d1)\nwhere:\nd1 = (ln(S/X) + (r + 0.5*v^2)*T) / (v*sqrt(T))\nd2 = d1 - v*sqrt(T)\n\nFixed parameters: risk-free rate r=0.02, volatility v=0.30. The cumulative normal distribution function N must be approximated using a polynomial. Results must be computed independently for each element.\n\nInput:\nstock_price: float32, shape = (8388608,)\noption_strike: float32, shape = (8388608,)\noption_years: float32, shape = (8388608,)\n\nOutput:\ncall_out: float32, shape = (8388608,)\nput_out: float32, shape = (8388608,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 459, "task_name": "Stereo_Disparity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nStereo_Disparity\n\nTask Description:\nTask: Stereo_Disparity. Compute a disparity map between two stereo images. The inputs are two 2048x2048 uint32 images representing left and right views. The output is a flattened 4194304-element uint32 array representing per-pixel disparity values. For each pixel position, compare 17x17 windows between left and right images across a disparity search range. Compute byte-wise sum of absolute differences (SAD) for uint32 values (each treated as four bytes), aggregate costs horizontally and vertically within the window, and select the disparity with minimal cost. The final disparity value must be the minimal cost disparity plus 8. The kernel must handle border clamping and use a fixed window radius of 8.\n\nInput:\nleft: uint32, shape = (2048, 2048)\nright: uint32, shape = (2048, 2048)\n\nOutput:\noutput: uint32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 491, "task_name": "Lennard_Jones", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLennard_Jones\n\nTask Description:\nLennard_Jones task: Compute the Lennard-Jones forces for each atom in a system of 1024 atoms. The input is a float32 tensor 'pos' with shape (1024, 4), where each row represents an atom's position (x, y, z) and an unused fourth component. The output is a float32 tensor 'force_out' with shape (1024, 3), representing the force vector (x, y, z) on each atom. For each atom i, calculate the force by summing contributions from all other atoms j where the squared distance between them is less than a cutoff (cutsq = 100.0). Skip self-interactions (i=j). The force calculation uses the Lennard-Jones potential formula: 24 * epsilon * r6inv * (2 * sigma6 * r6inv - 1) * r2inv, where r2inv = 1/rsq, r6inv = r2inv^3, and rsq is the squared distance between atoms. Constants epsilon = 1.0 and sigma6 = 1.0 are fixed.\n\nInput:\npos: float32, shape = (1024, 4)\n\nOutput:\nforce_out: float32, shape = (1024, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 496, "task_name": "Reverse_Array", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReverse_Array\n\nTask Description:\nImplement a CUDA kernel named 'Reverse_Array' that reverses the order of elements in a one-dimensional array. The input is a single tensor called 'reverse_input' of dtype float32 with shape (1048576,). The output must be a tensor called 'reverse_out' of dtype float32 with the same shape (1048576,). The kernel must produce an output where each element at position i is the element from the input at position (n-1-i), where n is the array length. The kernel must efficiently handle the entire array length without data loss or corruption.\n\nInput:\nreverse_input: float32, shape = (1048576,)\n\nOutput:\nreverse_out: float32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 443, "task_name": "Matrix_Transpose", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nMatrix_Transpose\n\nTask Description:\nTask: Matrix Transpose. Given an input matrix of 4096 rows and 4096 columns (each element is a 32-bit floating point number), compute its transpose. The input is provided as a 1D array of 16777216 elements in row-major order, meaning the element at row i and column j is at index i * 4096 + j. The output must be a 1D array of 16777216 elements representing the transposed matrix stored in row-major order, so that the element originally at (i, j) in the input is placed at (j, i) in the output matrix (which is stored at index j * 4096 + i). The kernel must only process indices within the valid range [0, 4095] for both row and column indices.\n\nInput:\nmatrix_input: float32, shape = (4096, 4096)\n\nOutput:\nmatrix_out: float32, shape = (16777216,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 474, "task_name": "PSNR", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nPSNR\n\nTask Description:\nCompute the Peak Signal-to-Noise Ratio (PSNR) between two images. The inputs are two 1D float32 tensors of shape (4194304,) representing the original and compressed images. The output is a single float32 value in a tensor of shape (1,). PSNR is calculated by first computing the sum of squared differences between corresponding pixels, then deriving the mean squared error (MSE) by dividing the sum by the total number of pixels. Finally, compute PSNR as 10 times the base-10 logarithm of (1 divided by MSE). The peak signal value is 1.0.\n\nInput:\ninputImage: float32, shape = (4194304,)\ncompressedInputImage: float32, shape = (4194304,)\n\nOutput:\npsnr_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 458, "task_name": "Stereo_Disparity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nStereo_Disparity\n\nTask Description:\nImplement a CUDA kernel for stereo disparity calculation. The kernel takes two input images, left and right, each of size 1024x1024 with uint32 data type. For each pixel in the left image, compute the optimal horizontal disparity by comparing it with shifted pixels in the right image. The disparity search range is from minDisparity (0) to maxDisparity (16). For each candidate disparity, compute the sum of absolute differences (SAD) over a 17x17 window centered on the pixel. The output should be a flattened array of 1048576 uint32 values where each element is the optimal disparity plus 8. Use shared memory for intermediate calculations and ensure thread synchronization.\n\nInput:\nleft: uint32, shape = (1024, 1024)\nright: uint32, shape = (1024, 1024)\n\nOutput:\noutput: uint32, shape = (1048576,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 465, "task_name": "Graph_Conv", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGraph_Conv\n\nTask Description:\nTask: Graph_Conv. Compute a graph convolution operation where each node's features are updated by aggregating weighted features from its neighbors. Inputs include: node features tensor 'x' (1024 nodes × 64 features, float32), edge connection tensor 'edge_index' (2 × 65536 edges, int32, with row0 as source nodes and row1 as destination nodes), edge weights tensor 'edge_weight' (65536 weights, float32), weight matrix 'w' (64 input features × 64 output features, float32), and bias vector 'b' (64 elements, float32). Output is a flattened tensor 'y_out' (65536 elements, float32) representing updated node features (1024 × 64). Constraints: For each node and output feature, accumulate bias term plus weighted sum of dot products between neighbor features and weight matrix columns. Must handle all 65536 edges and 1024 nodes.\n\nInput:\nx: float32, shape = (1024, 64)\nedge_index: int32, shape = (2, 65536)\nedge_weight: float32, shape = (65536,)\nw: float32, shape = (64, 64)\nb: float32, shape = (64,)\n\nOutput:\ny_out: float32, shape = (65536,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 457, "task_name": "Stereo_Disparity", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nStereo_Disparity\n\nTask Description:\nStereo Disparity: Compute a disparity map between two stereo images. Inputs include a 512x512 left image and a 512x512 right image, both as uint32 tensors. The output is a 512x512 uint32 disparity map. For each pixel, search horizontally across a predefined disparity range (0 to 16) to find the best match between images. Matching is evaluated using sum of absolute differences (SAD) over a 17x17 window (radius 8). The kernel must use shared memory for intermediate calculations and synchronize threads. The final disparity value is the best match position plus 8.\n\nInput:\nleft: uint32, shape = (512, 512)\nright: uint32, shape = (512, 512)\n\nOutput:\noutput: uint32, shape = (512, 512)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 488, "task_name": "FDTD_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFDTD_3D\n\nTask Description:\nTask: FDTD_3D. Implement a CUDA kernel for a 3D finite-difference time-domain stencil operation. The input is a 3D tensor named 'fdtd_input' of shape (264, 264, 264) with float32 data type, which includes a padding of 4 in each dimension. The stencil coefficients are provided as a 1D tensor named 'fdtd_stencil' of shape (5,) with float32 data type. The kernel must compute the output for each point in the inner grid of dimensions (256,256,256) and write to the corresponding inner region of the output tensor 'fdtd_output', which has the same shape (264,264,264) and float32 data type. The computation for each inner point (x,y,z) is: output[z][y][x] = stencil[0] * input[z][y][x] + sum_{i=1}^{4} stencil[i] * (input[z-i][y][x] + input[z+i][y][x] + input[z][y-i][x] + input[z][y+i][x] + input[z][y][x-i] + input[z][y][x+i]). The kernel must not write to the padding region. The kernel should use shared memory to optimize data reuse for the xy-plane and iterate over the z-dimension.\n\nInput:\nfdtd_input: float32, shape = (264, 264, 264)\nfdtd_stencil: float32, shape = (5,)\n\nOutput:\nfdtd_output: float32, shape = (264, 264, 264)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 485, "task_name": "DWT_Haar", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDWT_Haar\n\nTask Description:\nImplement a CUDA kernel for the Haar wavelet transform (DWT_Haar). The input is a signal tensor of 8192 float32 values. The kernel must recursively decompose the signal into detail and approximation coefficients. Each thread block processes 1024 elements using 512 threads. The computation involves pairwise operations: detail coefficients are calculated as (x - y) * 1/sqrt(2) and approximation coefficients as (x + y) * 1/sqrt(2). Detail coefficients from all decomposition levels must be stored in the 8192-length output tensor, with level-1 details placed in the second half. The final approximation coefficient per block must be stored in an 8-length output tensor. Constraints: fixed input size (8192), 8 blocks, and 512 threads per block.\n\nInput:\nsignal: float32, shape = (8192,)\n\nOutput:\ndetails_out: float32, shape = (8192,)\napprox_out: float32, shape = (8,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 494, "task_name": "Lennard_Jones", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLennard_Jones\n\nTask Description:\nCompute the Lennard-Jones kernel for atomic force calculation. Given an input tensor 'pos' of shape (8192, 4) with float32 data type representing atom positions (x,y,z,w), calculate the net force vector on each atom. The output tensor 'force_out' must have shape (8192, 3) with float32 data type. For each atom i, sum forces from all other atoms j where squared distance is below cutoff. Force calculation uses the Lennard-Jones potential formula: 24 * epsilon * r6inv * (2 * sigma6 * r6inv - 1) * r2inv, where r6inv = (1/rsq)^3 and rsq is squared distance between atoms. Exclude self-interactions (i=j) and interactions beyond cutoff distance.\n\nInput:\npos: float32, shape = (8192, 4)\n\nOutput:\nforce_out: float32, shape = (8192, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 492, "task_name": "Lennard_Jones", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLennard_Jones\n\nTask Description:\nImplement a CUDA kernel named 'Lennard_Jones' that calculates atomic forces using the Lennard-Jones potential. The input is a 2048×4 float32 tensor 'pos' where each row contains atomic coordinates (x,y,z) and an unused fourth value. The output must be a 2048×3 float32 tensor 'force_out' containing force vectors (fx,fy,fz) for each atom. The kernel must exclude self-interactions (i≠j), only compute interactions when squared distance is below 100.0, and use fixed parameters epsilon=1.0 and sigma6=1.0. Forces are accumulated through pairwise interactions where force contribution is calculated as 24ε·r6inv·(2σ6·r6inv - 1)·r2inv·(dx,dy,dz).\n\nInput:\npos: float32, shape = (2048, 4)\n\nOutput:\nforce_out: float32, shape = (2048, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 464, "task_name": "Graph_Conv", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGraph_Conv\n\nTask Description:\nTask: Graph Convolution. Compute the output feature matrix for each node by aggregating weighted messages from its neighbors. Inputs include: x (node features, shape (512, 64), dtype float32), edge_index (edge list with source and destination indices, shape (2, 16384), dtype int32), edge_weight (edge weights, shape (16384,), dtype float32), w (weight matrix, shape (64, 32), dtype float32), and b (bias vector, shape (32,), dtype float32). Output: y_out (output features, shape (512, 32), dtype float32). Constraints: The graph has 512 nodes and 16384 edges; features transition from 64 to 32 dimensions; each node's output is computed by summing contributions only from edges where it is the destination node, starting with the bias term.\n\nInput:\nx: float32, shape = (512, 64)\nedge_index: int32, shape = (2, 16384)\nedge_weight: float32, shape = (16384,)\nw: float32, shape = (64, 32)\nb: float32, shape = (32,)\n\nOutput:\ny_out: float32, shape = (512, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 489, "task_name": "FDTD_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFDTD_3D\n\nTask Description:\nImplement the FDTD_3D kernel to perform a 3D finite-difference time-domain computation. The kernel takes two inputs: a 520×520×520 float32 tensor representing a padded 3D grid and a 5-element float32 tensor containing stencil coefficients. It outputs a 520×520×520 float32 tensor. The computation must apply a star-shaped stencil with a radius of 4 to each point in the inner 512×512×512 region of the input grid, while leaving the outer padding unchanged. The stencil uses the center point and neighbors along all three axes at offsets ±1 to ±4. Only the inner region must be updated; the padding should remain unmodified.\n\nInput:\nfdtd_input: float32, shape = (520, 520, 520)\nfdtd_stencil: float32, shape = (5,)\n\nOutput:\nfdtd_output: float32, shape = (520, 520, 520)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 484, "task_name": "DWT_Haar", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDWT_Haar\n\nTask Description:\nImplement the DWT_Haar kernel for a 1D signal of length 4096. The kernel must compute multi-level Haar wavelet decomposition using multiple GPU blocks. Each block processes 1024 consecutive elements. For the first decomposition level, compute detail coefficients as (x[2i] - x[2i+1]) * √2/2 and approximation coefficients as (x[2i] + x[2i+1]) * √2/2. Recursively apply the same decomposition to approximation coefficients within each block until reaching a single final approximation value per block. Output must include: (1) A 4096-length details_out tensor storing all detail coefficients (first-level details in indices 2048-4095, subsequent levels in lower indices), and (2) A 4-length approx_out tensor containing the final approximation coefficient from each block.\n\nInput:\nsignal: float32, shape = (4096,)\n\nOutput:\ndetails_out: float32, shape = (4096,)\napprox_out: float32, shape = (4,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 497, "task_name": "Reverse_Array", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReverse_Array\n\nTask Description:\nImplement the Reverse_Array kernel to reverse the order of elements in a 1D array. The input is a tensor named 'reverse_input' with shape (4194304,) and dtype float32. The output should be a tensor named 'reverse_out' with identical shape and dtype, containing elements in reversed order. The kernel must only swap elements symmetrically around the midpoint without modifying indices beyond the first half of the array.\n\nInput:\nreverse_input: float32, shape = (4194304,)\n\nOutput:\nreverse_out: float32, shape = (4194304,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 495, "task_name": "Lennard_Jones", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLennard_Jones\n\nTask Description:\nTask: Lennard_Jones. Compute the total force on each atom in a system of 16384 atoms using the Lennard-Jones potential. The input is a tensor 'pos' of shape (16384, 4) and dtype float32, where the first three elements of each row are x, y, z coordinates and the fourth element is unused. The output is a tensor 'force_out' of shape (16384, 3) and dtype float32, representing the force vector (x, y, z components) on each atom. Constraints: Force is computed only for atom pairs with squared distance less than 100.0. The force calculation uses parameters epsilon=1.0 and sigma6=1.0 (sigma^6). Each atom's force is the vector sum of contributions from all other atoms within the cutoff distance.\n\nInput:\npos: float32, shape = (16384, 4)\n\nOutput:\nforce_out: float32, shape = (16384, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 481, "task_name": "DWT_Haar", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDWT_Haar\n\nTask Description:\nImplement a kernel for the Haar wavelet transform on a 1D signal. The input is a float32 tensor of shape (512,). The output consists of two tensors: a details_out tensor of float32 values with shape (512,), and an approx_out tensor of float32 values with shape (1,). The kernel must compute multi-level decomposition: at each level, pairs of elements are transformed into approximation and detail coefficients using the scaling factor 1/sqrt(2). The first-level details are stored in the second half of the output array. Subsequent levels recursively process the approximation coefficients from the previous level, storing details in decreasing segments of the output array. The final approximation coefficient must be stored in approx_out.\n\nInput:\nsignal: float32, shape = (512,)\n\nOutput:\ndetails_out: float32, shape = (512,)\napprox_out: float32, shape = (1,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 493, "task_name": "Lennard_Jones", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nLennard_Jones\n\nTask Description:\nImplement the Lennard-Jones force computation kernel. The kernel should calculate the 3D force vectors for 4096 atoms based on pairwise interactions using the Lennard-Jones potential. Input is a float32 tensor of shape (4096, 4) where each row contains an atom's x, y, z coordinates and an unused fourth value. Output should be a float32 tensor of shape (4096, 3) containing the force vectors in x, y, z directions. Constraints: avoid self-interactions (i != j), only consider atom pairs within squared cutoff distance (cutsq = 100), and use Lennard-Jones parameters epsilon=1.0 and sigma6=1.0.\n\nInput:\npos: float32, shape = (4096, 4)\n\nOutput:\nforce_out: float32, shape = (4096, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 498, "task_name": "Reverse_Array", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReverse_Array\n\nTask Description:\nTask: Reverse_Array. Reverse the order of elements in a one-dimensional array of 16777216 floating-point numbers (float32). The kernel must swap each element at index i with the element at index N-1-i for all i in the range [0, N/2), where N is the array length. Since N is even, the entire array is reversed without leaving any middle element. The output must be an array of the same shape and data type as the input.\n\nInput:\nreverse_input: float32, shape = (16777216,)\n\nOutput:\nreverse_out: float32, shape = (16777216,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 499, "task_name": "Reverse_Array", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReverse_Array\n\nTask Description:\nTask: Reverse_Array. Reverse the order of elements in a one-dimensional float32 array of size 67108864. The input array 'reverse_input' contains 67108864 float32 values. The output array 'reverse_out' must have the same shape and data type, where each element at position i is the element from the input array at position (67108863 - i). The kernel must correctly reverse the entire array, ensuring the output exactly matches the reversed input without modifying the original input array.\n\nInput:\nreverse_input: float32, shape = (67108864,)\n\nOutput:\nreverse_out: float32, shape = (67108864,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 486, "task_name": "FDTD_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFDTD_3D\n\nTask Description:\nTask name: FDTD_3D. This kernel performs a 3D finite-difference time-domain stencil computation on a padded input grid. The input is a 72×72×72 float32 tensor with a 4-element halo on all sides, where the inner 64×64×64 region contains valid data. A 5-element float32 stencil array provides coefficients. The output is a 72×72×72 float32 tensor where only the inner 64×64×64 region is modified. For each inner grid point, compute a weighted sum of its center value and all neighbors within Euclidean distance 4 along the X/Y/Z axes, using stencil coefficients where index 0 corresponds to the center and indices 1-4 correspond to radial offsets 1-4 respectively. The halo regions must remain unchanged.\n\nInput:\nfdtd_input: float32, shape = (72, 72, 72)\nfdtd_stencil: float32, shape = (5,)\n\nOutput:\nfdtd_output: float32, shape = (72, 72, 72)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 487, "task_name": "FDTD_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFDTD_3D\n\nTask Description:\nTask: FDTD_3D. Apply a 3D finite-difference stencil with a radius of 4 to update the inner region of a padded volume. The input is a 136x136x136 float32 tensor representing the volume with padding, and a 5-element float32 tensor for stencil coefficients. The kernel must update only the inner 128x128x128 region by computing a weighted sum of each point and its neighbors in all three dimensions at distances 1 to 4. The output is a 136x136x136 float32 tensor where padding regions remain unchanged.\n\nInput:\nfdtd_input: float32, shape = (136, 136, 136)\nfdtd_stencil: float32, shape = (5,)\n\nOutput:\nfdtd_output: float32, shape = (136, 136, 136)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e38d8bed", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 401, "task_name": "Dynamic_Voxelize", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nDynamic_Voxelize\n\nTask Description:\nTask: Dynamic_Voxelize. Given an input tensor of 1024 points with 5 features each (dtype float32), where the first three features represent 3D coordinates (x, y, z), compute voxel indices for each point in a fixed grid. The grid has boundaries [0.0, 5.0] in all dimensions and a voxel size of 1.0 in each axis. For each point, calculate integer voxel coordinates (z, y, x) by normalizing and discretizing its 3D position. If a point's x-coordinate falls outside the grid, output (-1, 0, 0); if y is outside, output (-1, -1, 0); if z is outside, output (-1, -1, -1). The output must be a tensor of shape (1024, 3) with dtype int32.\n\nInput:\ninput_points: float32, shape = (1024, 5)\n\nOutput:\noutput_dynamic_voxelize_coordinates: int32, shape = (1024, 3)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "31212aef", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 490, "task_name": "FDTD_3D", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nFDTD_3D\n\nTask Description:\nTask: FDTD_3D. The kernel must compute a 3D finite-difference time-domain (FDTD) stencil operation with a radius of 4. Inputs include a float32 tensor of shape (1032, 1032, 1032) representing a padded 3D grid and a float32 tensor of shape (5,) containing stencil coefficients. The output is a float32 tensor of shape (1032, 1032, 1032). The computation applies the stencil to each point in the inner grid (1024x1024x1024) by summing weighted contributions from the center point and its neighbors at distances 1 to 4 along the x, y, and z axes. The padded regions in the output remain unchanged.\n\nInput:\nfdtd_input: float32, shape = (1032, 1032, 1032)\nfdtd_stencil: float32, shape = (5,)\n\nOutput:\nfdtd_output: float32, shape = (1032, 1032, 1032)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "e230d861", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 500, "task_name": "Reverse_Array", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nReverse_Array\n\nTask Description:\nTask: Reverse_Array. Reverse the input array of 268435456 float32 values by swapping each element at index i with the element at index (n-1-i) for all i from 0 to n/2 - 1, where n is the array length. The input is a one-dimensional tensor of float32 values, and the output must be a one-dimensional tensor of the same shape and type containing the reversed elements. The kernel must only perform swaps in the first half of the array to avoid redundant operations and ensure correctness for even lengths.\n\nInput:\nreverse_input: float32, shape = (268435456,)\n\nOutput:\nreverse_out: float32, shape = (268435456,)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "c70ca1e1", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
{"id": 463, "task_name": "Graph_Conv", "prompt": "\n### TASK SPECIFICATION\n\nTask Name:\nGraph_Conv\n\nTask Description:\nTask name: Graph_Conv. Implement a graph convolution kernel that updates node features based on incoming edges. Inputs: x (float32, 256 nodes × 32 features), edge_index (int32, 2×4096 edges with [source, destination] pairs), edge_weight (float32, 4096 edge weights), w (float32, 32×32 weight matrix), b (float32, 32-element bias vector). Output: y_out (float32, 256×32 updated node features). For each node i and output feature o: initialize with bias b[o]; then for each incoming edge where destination = i, compute dot product between source node's features and w's o-th column, multiply by edge weight, and accumulate. Must handle exactly 256 nodes, 4096 edges, and 32 features.\n\nInput:\nx: float32, shape = (256, 32)\nedge_index: int32, shape = (2, 4096)\nedge_weight: float32, shape = (4096,)\nw: float32, shape = (32, 32)\nb: float32, shape = (32,)\n\nOutput:\ny_out: float32, shape = (256, 32)\n\nGPU:\nNVIDIA GeForce RTX 4090\n\nCUDA program:\n", "run_id": "9f8714fa", "response1": null, "code1": null, "response2": null, "code2": null, "response3": null, "code3": null}
